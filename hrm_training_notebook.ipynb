{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Reasoning Model (HRM) Training Notebook\n",
    "\n",
    "This notebook serves as the entry point for training your HRM model on Kaggle/Colab using their computational resources.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload your entire codebase to Google Drive or Kaggle\n",
    "2. Mount Google Drive (if using Colab)\n",
    "3. Set the correct project path\n",
    "4. Install dependencies\n",
    "5. Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (Colab only)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully!\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab, skipping Drive mount\")\n",
    "\n",
    "# Set your project path\n",
    "# For Colab with Drive: PROJECT_PATH = '/content/drive/MyDrive/AlgoTrading'\n",
    "# For Kaggle: PROJECT_PATH = '/kaggle/input/your-dataset/AlgoTrading'\n",
    "PROJECT_PATH = '/content/drive/MyDrive/AlgoTrading'  # CHANGE THIS TO YOUR ACTUAL PATH\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to Python path\n",
    "sys.path.insert(0, PROJECT_PATH)\n",
    "sys.path.insert(0, os.path.join(PROJECT_PATH, 'src'))\n",
    "\n",
    "print(f\"Project path added to sys.path: {PROJECT_PATH}\")\n",
    "print(f\"Python path: {sys.path[:5]}...\")  # Show first 5 paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r \"{PROJECT_PATH}/requirements.txt\" --quiet\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify imports work correctly\n",
    "try:\n",
    "    # Core HRM components\n",
    "    from src.models.hierarchical_reasoning_model import HierarchicalReasoningModel\n",
    "    from src.backtesting.environment import TradingEnv\n",
    "    from src.utils.data_loader import DataLoader\n",
    "    from src.training.trainer import Trainer\n",
    "    from src.training.universal_trainer import UniversalTrainer\n",
    "    \n",
    "    # Configuration\n",
    "    from src.utils.config_loader import ConfigLoader\n",
    "    from src.utils.instrument_loader import load_instruments\n",
    "    \n",
    "    # Utilities\n",
    "    from src.utils.iteration_manager import IterationManager\n",
    "    from src.utils.research_logger import ResearchLogger\n",
    "    \n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "    print(\"‚úÖ HRM model components loaded\")\n",
    "    print(\"‚úÖ Training utilities loaded\")\n",
    "    print(\"‚úÖ Configuration systems loaded\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please check your project path and file structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify configuration loading\n",
    "try:\n",
    "    config_loader = ConfigLoader()\n",
    "    config = config_loader.get_config()\n",
    "    \n",
    "    # Check key configuration sections\n",
    "    print(\"‚úÖ Configuration loaded successfully!\")\n",
    "    print(f\"Model type: {config.get('model', {}).get('model_type', 'Not specified')}\")\n",
    "    print(f\"HRM episodes: {config.get('training_sequence', {}).get('stage_1_hrm', {}).get('episodes', 'Not specified')}\")\n",
    "    print(f\"Environment episode length: {config.get('environment', {}).get('episode_length', 'Not specified')}\")\n",
    "    \n",
    "    # Check instruments\n",
    "    instruments = load_instruments(os.path.join(PROJECT_PATH, 'config', 'instruments.yaml'))\n",
    "    print(f\"‚úÖ Loaded {len(instruments)} instruments\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data availability\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = os.path.join(PROJECT_PATH, 'data', 'final')\n",
    "\n",
    "if os.path.exists(DATA_DIR):\n",
    "    try:\n",
    "        # List available data files\n",
    "        csv_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv')]\n",
    "        print(f\"‚úÖ Found {len(csv_files)} CSV files in data directory\")\n",
    "        \n",
    "        if csv_files:\n",
    "            print(\"Sample files:\")\n",
    "            for i, file in enumerate(csv_files[:5]):\n",
    "                print(f\"  {i+1}. {file}\")\n",
    "            \n",
    "            # Try to load one file to verify format\n",
    "            sample_file = os.path.join(DATA_DIR, csv_files[0])\n",
    "            try:\n",
    "                df = pd.read_csv(sample_file, index_col=0, nrows=5)\n",
    "                print(f\"\\n‚úÖ Sample data from {csv_files[0]}:\")\n",
    "                print(f\"Shape: {df.shape}\")\n",
    "                print(f\"Columns: {list(df.columns)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not read sample file: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error scanning data directory: {e}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Data directory not found: {DATA_DIR}\")\n",
    "    print(\"Please ensure your data is uploaded to the correct location\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Data Processing: Convert Raw Data to Processed Features\nprint(\"üîÑ Starting data processing pipeline...\")\n\n# Use the actual naming convention from the existing system\nRAW_DATA_DIR = os.path.join(PROJECT_PATH, 'data', 'raw')\nFINAL_DATA_DIR = os.path.join(PROJECT_PATH, 'data', 'final')\n\n# Check for raw data files\nraw_files = []\nif os.path.exists(RAW_DATA_DIR):\n    raw_files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith('.csv')]\n    print(f\"üìÅ Found {len(raw_files)} raw data files\")\nelse:\n    print(f\"‚ö†Ô∏è  Raw data directory not found: {RAW_DATA_DIR}\")\n\n# Check for final processed files (actual naming: features_*.csv)\nfinal_files = []\nif os.path.exists(FINAL_DATA_DIR):\n    final_files = [f for f in os.listdir(FINAL_DATA_DIR) if f.startswith('features_') and f.endswith('.csv')]\n    print(f\"üìÅ Found {len(final_files)} processed feature files\")\nelse:\n    print(f\"üìÅ Final data directory will be created: {FINAL_DATA_DIR}\")\n\n# Determine if we need to run data processing\nneed_processing = len(final_files) == 0 or len(raw_files) > len(final_files)\n\nif need_processing and len(raw_files) > 0:\n    print(\"üöÄ Running feature generation pipeline...\")\n    \n    try:\n        # Import and use the actual feature generator used in the system\n        from src.data_processing.feature_generator import DynamicFileProcessor\n        \n        # Initialize the same processor class used in pipeline.py\n        processor = DynamicFileProcessor()\n        \n        print(\"‚öôÔ∏è  Running feature generation with technical indicators...\")\n        print(\"üîß Processing: OHLCV ‚Üí Technical Indicators ‚Üí Market Structure ‚Üí Features\")\n        \n        # Run the same feature processing logic\n        results = processor.process_all_files()\n        \n        if results:\n            print(f\"‚úÖ Feature generation completed successfully!\")\n            \n            # Count and verify final files\n            final_files_created = [f for f in os.listdir(FINAL_DATA_DIR) if f.startswith('features_') and f.endswith('.csv')]\n            print(f\"üìä Created {len(final_files_created)} feature files\")\n            \n            # Calculate total rows processed\n            total_rows = 0\n            for feature_file in final_files_created:\n                try:\n                    df = pd.read_csv(os.path.join(FINAL_DATA_DIR, feature_file))\n                    total_rows += len(df)\n                except Exception as e:\n                    print(f\"‚ö†Ô∏è  Could not count rows in {feature_file}: {e}\")\n            \n            print(f\"üìà Total rows processed: {total_rows:,}\")\n            print(f\"üìÅ Final data location: {FINAL_DATA_DIR}\")\n            \n            # Show sample of created files\n            print(\"‚úÖ Created feature files:\")\n            for i, file in enumerate(final_files_created[:10]):\n                print(f\"   üìÑ {file}\")\n            if len(final_files_created) > 10:\n                print(f\"   ... and {len(final_files_created) - 10} more files\")\n                \n        else:\n            print(f\"‚ùå Feature generation failed: No files processed\")\n            \n    except Exception as e:\n        print(f\"‚ùå Data processing pipeline failed: {e}\")\n        print(\"‚ö†Ô∏è  You may need to manually run the data processing pipeline\")\n        print(\"üí° Try running: python src/data_processing/pipeline.py\")\n        import traceback\n        traceback.print_exc()\n        \nelif len(final_files) > 0:\n    print(\"‚úÖ Processed feature files already exist, skipping data processing\")\n    print(f\"üìÅ Using existing {len(final_files)} feature files in {FINAL_DATA_DIR}\")\n    \n    # Show sample of existing files\n    for i, file in enumerate(final_files[:5]):\n        print(f\"   üìÑ {file}\")\n    if len(final_files) > 5:\n        print(f\"   ... and {len(final_files) - 5} more files\")\n        \nelse:\n    print(\"‚ö†Ô∏è  No raw data files found for processing\")\n    print(\"üí° Please ensure raw historical data is uploaded to the data/raw directory\")\n    print(\"üí° Expected format: CSV files with OHLCV data\")\n\n# Read and display sample of processed data\nfinal_files_available = [f for f in os.listdir(FINAL_DATA_DIR) if f.startswith('features_') and f.endswith('.csv')] if os.path.exists(FINAL_DATA_DIR) else []\n\nif final_files_available:\n    print(\"\\n\" + \"=\"*60)\n    print(\"üìä SAMPLE OF PROCESSED FEATURE DATA\")\n    print(\"=\"*60)\n    \n    # Read first available processed file\n    sample_file = final_files_available[0]\n    sample_path = os.path.join(FINAL_DATA_DIR, sample_file)\n    \n    try:\n        df = pd.read_csv(sample_path)\n        print(f\"üìÑ Sample from: {sample_file}\")\n        print(f\"üìà Shape: {df.shape} (rows, columns)\")\n        print(f\"üóìÔ∏è  Date range: {df['datetime_readable'].iloc[0]} to {df['datetime_readable'].iloc[-1]}\")\n        \n        print(\"\\nüîß Feature Categories:\")\n        feature_cols = df.columns.tolist()\n        print(f\"   üìä Price Data: {[col for col in feature_cols if col in ['open', 'high', 'low', 'close']]}\")\n        print(f\"   üìà Moving Averages: {[col for col in feature_cols if 'sma' in col or 'ema' in col][:8]}...\")\n        print(f\"   üéØ Oscillators: {[col for col in feature_cols if any(x in col for x in ['rsi', 'stoch', 'macd', 'cci'])]}\")\n        print(f\"   üìè Volatility: {[col for col in feature_cols if any(x in col for x in ['atr', 'bb_', 'volatility'])]}\")\n        print(f\"   üìê Market Structure: {[col for col in feature_cols if any(x in col for x in ['trend_', 'momentum', 'roc'])]}\")\n        \n        print(f\"\\nüìã First 3 rows:\")\n        print(df.head(3).to_string(max_cols=15, max_colwidth=10))\n        \n        print(f\"\\n‚úÖ Data contains {len(feature_cols)} technical features ready for HRM training\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Could not read sample data: {e}\")\n\nprint(\"\\nüîÑ Data processing check completed\")\nprint(f\"üìÇ Training will use data from: {FINAL_DATA_DIR}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging for training\n",
    "import logging\n",
    "\n",
    "# Configure clean, minimal logging for training\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Reduce verbosity for specific modules during training\n",
    "logging.getLogger('src.backtesting.environment').setLevel(logging.WARNING)\n",
    "logging.getLogger('src.utils.data_loader').setLevel(logging.WARNING)\n",
    "logging.getLogger('src.data_processing.feature_generator').setLevel(logging.WARNING)\n",
    "logging.getLogger('src.utils.data_feeding_strategy').setLevel(logging.WARNING)\n",
    "logging.getLogger('src.utils.config_loader').setLevel(logging.WARNING)\n",
    "logging.getLogger('src.training.trainer').setLevel(logging.WARNING)\n",
    "logging.getLogger('src.training.universal_trainer').setLevel(logging.WARNING)\n",
    "\n",
    "print(\"‚úÖ Logging configured for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Function\n",
    "def run_hrm_training(episodes=None, testing_mode=False, symbols=None):\n",
    "    \"\"\"\n",
    "    Run HRM training with the specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        episodes (int): Number of episodes to train (overrides config if specified)\n",
    "        testing_mode (bool): Enable testing mode with reduced episodes\n",
    "        symbols (list): Specific symbols to train on (None for all available)\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting HRM Training\")\n",
    "    \n",
    "    # Enable detailed logging for direct training runs\n",
    "    os.environ['DETAILED_BACKTEST_LOGGING'] = 'true'\n",
    "    \n",
    "    # Load configuration\n",
    "    config_loader = ConfigLoader()\n",
    "    config = config_loader.get_config()\n",
    "    \n",
    "    # Determine data directory\n",
    "    data_processing_config = config.get('data_processing', {})\n",
    "    if testing_mode:\n",
    "        data_dir = os.path.join(data_processing_config.get('test_folder', 'data/test'), 'final')\n",
    "    else:\n",
    "        data_dir = os.path.join(PROJECT_PATH, 'data/final')\n",
    "    \n",
    "    print(f\"üìÇ Using data directory: {data_dir}\")\n",
    "    \n",
    "    # Initialize data loader\n",
    "    data_loader = DataLoader(final_data_dir=data_dir, use_parquet=True)\n",
    "    \n",
    "    # Determine episodes\n",
    "    if episodes is not None:\n",
    "        print(f\"üìä Using specified episodes: {episodes}\")\n",
    "    elif testing_mode:\n",
    "        if 'testing_overrides' in config and 'training_sequence' in config['testing_overrides']:\n",
    "            episodes = config['testing_overrides']['training_sequence']['stage_1_hrm']['episodes']\n",
    "        else:\n",
    "            episodes = 5\n",
    "        print(f\"üß™ Testing mode: {episodes} episodes\")\n",
    "    else:\n",
    "        if 'training_sequence' in config and 'stage_1_hrm' in config['training_sequence']:\n",
    "            episodes = config['training_sequence']['stage_1_hrm']['episodes']\n",
    "        else:\n",
    "            episodes = 100\n",
    "        print(f\"üìä Production mode: {episodes} episodes\")\n",
    "    \n",
    "    # Get symbols\n",
    "    if symbols is not None:\n",
    "        print(f\"üéØ Using specified symbols: {symbols}\")\n",
    "    elif testing_mode:\n",
    "        symbols = [\"RELIANCE_1\", \"Bank_Nifty_5\"]\n",
    "        print(f\"üß™ Testing mode symbols: {symbols}\")\n",
    "    else:\n",
    "        # Get all available symbols\n",
    "        symbols = []\n",
    "        if os.path.exists(data_dir):\n",
    "            for filename in os.listdir(data_dir):\n",
    "                if filename.endswith('.csv') and filename.startswith('features_'):\n",
    "                    symbol = filename.replace('features_', '').replace('.csv', '')\n",
    "                    symbols.append(symbol)\n",
    "        symbols = sorted(list(set(symbols)))\n",
    "        print(f\"üîç Found {len(symbols)} symbols: {symbols[:10]}{'...' if len(symbols) > 10 else ''}\")\n",
    "    \n",
    "    if not symbols:\n",
    "        print(\"‚ùå No symbols found. Please check your data directory.\")\n",
    "        return\n",
    "    \n",
    "    # Set up iteration management\n",
    "    try:\n",
    "        iteration_manager = IterationManager(config)\n",
    "        \n",
    "        # Setup iteration directory\n",
    "        iteration_dir = iteration_manager.setup_iteration(data_loader, symbols)\n",
    "        \n",
    "        # Initialize research logger\n",
    "        research_logger = ResearchLogger(config, iteration_dir)\n",
    "        \n",
    "        print(f\"üî¨ Research iteration: {iteration_manager.current_iteration}\")\n",
    "        print(f\"üìÅ Iteration directory: {iteration_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Iteration management setup failed: {e}\")\n",
    "        iteration_manager = None\n",
    "        research_logger = None\n",
    "    \n",
    "    # Run universal HRM training\n",
    "    try:\n",
    "        print(\"üéØ Starting Universal HRM Training with Symbol Rotation\")\n",
    "        \n",
    "        # Get configuration sections\n",
    "        env_config = config.get('environment', {})\n",
    "        model_config = config.get('model', {})\n",
    "        \n",
    "        # Create environment with first symbol to get dimensions\n",
    "        env = TradingEnv(\n",
    "            data_loader=data_loader,\n",
    "            symbol=symbols[0],\n",
    "            initial_capital=env_config.get('initial_capital', 100000.0),\n",
    "            lookback_window=env_config.get('lookback_window', 50),\n",
    "            episode_length=env_config.get('episode_length', 500),\n",
    "            reward_function=env_config.get('reward_function', \"trading_focused\"),\n",
    "            use_streaming=env_config.get('use_streaming', False),\n",
    "            trailing_stop_percentage=env_config.get('trailing_stop_percentage', 0.02)\n",
    "        )\n",
    "        \n",
    "        # Call reset to initialize observation_space\n",
    "        env.reset()\n",
    "        \n",
    "        # Dynamically get dimensions from the environment\n",
    "        observation_dim = env.observation_space.shape[0]\n",
    "        print(f\"üîß Environment configured with Observation Dim: {observation_dim}\")\n",
    "        \n",
    "        # Update config with the true, environment-derived dimension before creating the model\n",
    "        config_copy = config.copy()\n",
    "        config_copy['model'] = config_copy.get('model', {})\n",
    "        config_copy['model']['observation_dim'] = observation_dim\n",
    "        \n",
    "        # Ensure hierarchical_reasoning_model.input_embedding.input_dim is also updated\n",
    "        hrm_config = config_copy.get('hierarchical_reasoning_model', {})\n",
    "        input_embedding_config = hrm_config.get('input_embedding', {})\n",
    "        input_embedding_config['input_dim'] = observation_dim\n",
    "        hrm_config['input_embedding'] = input_embedding_config\n",
    "        config_copy['hierarchical_reasoning_model'] = hrm_config\n",
    "        \n",
    "        # Create HRM agent\n",
    "        agent = HierarchicalReasoningModel(config_copy)\n",
    "        print(f\"ü§ñ HRM Model created with {sum(p.numel() for p in agent.parameters())} parameters\")\n",
    "        \n",
    "        # Create universal trainer that handles symbol rotation\n",
    "        trainer = UniversalTrainer(\n",
    "            agent, symbols, data_loader, \n",
    "            num_episodes=episodes, \n",
    "            log_interval=10, \n",
    "            config=config,\n",
    "            research_logger=research_logger\n",
    "        )\n",
    "        \n",
    "        print(f\"üéØ Training {episodes} episodes with symbol rotation\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save model\n",
    "        if not testing_mode:\n",
    "            model_path = model_config.get('model_path', 'models/universal_final_model.pth')\n",
    "            model_dir = os.path.dirname(os.path.join(PROJECT_PATH, model_path))\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            \n",
    "            if hasattr(agent, 'save_model'):\n",
    "                full_model_path = os.path.join(PROJECT_PATH, model_path)\n",
    "                agent.save_model(full_model_path)\n",
    "                print(f\"‚úÖ Universal model saved to {full_model_path}\")\n",
    "                \n",
    "                # Copy model to iteration directory\n",
    "                if iteration_manager:\n",
    "                    iteration_manager.save_model_artifacts(full_model_path)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Agent does not have save_model method. Cannot save model\")\n",
    "        else:\n",
    "            print(\"üß™ Testing mode - Model not saved\")\n",
    "        \n",
    "        print(\"‚úÖ Universal training complete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Test Run (Testing Mode)\n",
    "print(\"üß™ Running quick test...\")\n",
    "run_hrm_training(episodes=3, testing_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Training Run\n",
    "# Uncomment the line below to run full training\n",
    "# run_hrm_training(episodes=2000, testing_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Training Run with Specific Symbols\n",
    "# Uncomment the lines below to run training on specific symbols\n",
    "# symbols_to_train = [\"Nifty_5\", \"Bank_Nifty_15\", \"RELIANCE_1\"]\n",
    "# run_hrm_training(episodes=500, testing_mode=False, symbols=symbols_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions:\n",
    "\n",
    "1. **Setup**: \n",
    "   - Upload your entire AlgoTrading codebase to Google Drive or Kaggle\n",
    "   - Update `PROJECT_PATH` to point to your codebase location\n",
    "   - Run the setup cells\n",
    "\n",
    "2. **Quick Test**:\n",
    "   - Run the \"Quick Test Run\" cell to verify everything works\n",
    "   - This runs 3 episodes in testing mode\n",
    "\n",
    "3. **Full Training**:\n",
    "   - Uncomment and run the \"Full Training Run\" cell\n",
    "   - Adjust episode count as needed (recommended: 1500-3000 episodes)\n",
    "\n",
    "4. **Custom Training**:\n",
    "   - Uncomment and modify the \"Custom Training Run\" cell\n",
    "   - Specify symbols and episode count\n",
    "\n",
    "## Performance Tips:\n",
    "\n",
    "1. **Use GPU Runtime**: Enable GPU acceleration in Colab/Kaggle\n",
    "2. **Monitor Progress**: Check logs for training metrics\n",
    "3. **Save Models**: Models are automatically saved to the models/ directory\n",
    "4. **Resume Training**: Load saved models for continued training\n",
    "\n",
    "## Expected Results:\n",
    "\n",
    "With your 13M+ data points across 55 instruments and 10 timeframes:\n",
    "- 2000 episodes √ó 5000 steps = 10M training steps\n",
    "- Coverage: ~76% of your dataset (with early termination)\n",
    "- Model Size: FIXED at ~5.6M parameters\n",
    "- Training Time: 8-16 hours on GPU (much faster than local training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}