{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRM (Hierarchical Reasoning Model) Trading Training on Colab/Kaggle\n",
    "\n",
    "This notebook converts the HRM training pipeline to run on Google Colab or Kaggle notebooks with automatic TPU/GPU/CPU detection.\n",
    "\n",
    "## Overview\n",
    "- **HRM Architecture**: Brain-inspired hierarchical model with 27M parameters\n",
    "- **Training Approach**: Deep supervision with adaptive computation time\n",
    "- **Data Processing**: Automated pipeline from raw to training-ready data\n",
    "- **Hardware Support**: Automatic TPU/GPU/CPU detection with parallel training\n",
    "\n",
    "Based on the research paper: *Hierarchical Reasoning Model* by Guan Wang et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Clone Repository\n",
    "\n",
    "Clone the private GitHub repository using the service token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running in Colab: False | Kaggle: False | TPU: False\n",
      "‚è≠Ô∏è Local environment detected. Skipping repository cloning.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Detect runtime environment\n",
    "def is_colab():\n",
    "    return \"COLAB_GPU\" in os.environ\n",
    "\n",
    "def is_kaggle():\n",
    "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "\n",
    "def check_tpu_availability():\n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.get(\n",
    "            'http://metadata.google.internal/computeMetadata/v1/instance/attributes/accelerator-type',\n",
    "            headers={'Metadata-Flavor': 'Google'},\n",
    "            timeout=5\n",
    "        )\n",
    "        return 'tpu' in response.text.lower()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "in_cloud_env = is_colab() or is_kaggle()\n",
    "is_tpu_environment = check_tpu_availability() if in_cloud_env else False\n",
    "\n",
    "print(f\"üîç Running in Colab: {is_colab()} | Kaggle: {is_kaggle()} | TPU: {is_tpu_environment}\")\n",
    "\n",
    "# Repo details\n",
    "repo_url = \"https://{personal_access_token}@github.com/marshaltudu14/AlgoTrading.git\"\n",
    "repo_path = \"/content/AlgoTrading\"\n",
    "\n",
    "if in_cloud_env or is_tpu_environment:\n",
    "    # Clone repository only if running in cloud\n",
    "    if not os.path.exists(repo_path):\n",
    "        print(\"üì• Cloning AlgoTrading repository...\")\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"clone\", repo_url, repo_path],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Repository cloned successfully!\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to clone repository: {result.stderr}\")\n",
    "            raise Exception(\"Repository clone failed\")\n",
    "    else:\n",
    "        print(\"‚úÖ Repository already exists\")\n",
    "\n",
    "    # Change to repository directory\n",
    "    os.chdir(repo_path)\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "    print(f\"üìÅ Current directory: {os.getcwd()}\")\n",
    "    print(f\"üìã Repository contents:\")\n",
    "    for item in sorted(os.listdir('.')):\n",
    "        print(f\"  {item}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Local environment detected. Skipping repository cloning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "Install all required packages including TPU support for Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running in Colab: False | Kaggle: False | TPU: False\n",
      "‚è≠Ô∏è Local environment detected. Skipping installation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect runtime environment\n",
    "def is_colab():\n",
    "    return \"COLAB_GPU\" in os.environ\n",
    "\n",
    "def is_kaggle():\n",
    "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "\n",
    "def check_tpu_availability():\n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.get(\n",
    "            'http://metadata.google.internal/computeMetadata/v1/instance/attributes/accelerator-type',\n",
    "            headers={'Metadata-Flavor': 'Google'},\n",
    "            timeout=5\n",
    "        )\n",
    "        return 'tpu' in response.text.lower()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "in_cloud_env = is_colab() or is_kaggle()\n",
    "is_tpu_environment = check_tpu_availability() if in_cloud_env else False\n",
    "\n",
    "print(f\"üîç Running in Colab: {is_colab()} | Kaggle: {is_kaggle()} | TPU: {is_tpu_environment}\")\n",
    "\n",
    "# Install only if in cloud (Colab/Kaggle/TPU)\n",
    "if in_cloud_env or is_tpu_environment:\n",
    "    print(\"üì¶ Installing base requirements...\")\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "    if is_tpu_environment:\n",
    "        print(\"üöÄ Installing TPU support...\")\n",
    "        !pip install torch_xla cloud-tpu-client\n",
    "\n",
    "    print(\"üìä Installing visualization libraries...\")\n",
    "    !pip install ipywidgets plotly kaleido\n",
    "\n",
    "    print(\"‚úÖ All dependencies installed successfully!\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Local environment detected. Skipping installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Device Detection and Setup\n",
    "\n",
    "Automatically detect and configure the best available device (TPU > GPU > CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 15:37:40,274 - INFO - üíª Using CPU for training (TPU/GPU not available)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üî• DEVICE CONFIGURATION SUMMARY\n",
      "============================================================\n",
      "Selected Device: cpu\n",
      "Device Type: CPU\n",
      "CPU Cores: 4\n",
      "CPU Threads: 4\n",
      "============================================================\n",
      "\n",
      "üìè Recommended batch size: 16\n",
      "\n",
      "üéØ Training will use: cpu with batch size 16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import device manager after installation\n",
    "from src.utils.device_manager import get_device_manager\n",
    "\n",
    "# Initialize device manager\n",
    "device_manager = get_device_manager()\n",
    "device = device_manager.get_device()\n",
    "device_type = device_manager.get_device_type()\n",
    "\n",
    "# Print detailed device information\n",
    "device_manager.print_device_summary()\n",
    "\n",
    "# Get optimal batch size recommendation\n",
    "recommended_batch_size = device_manager.get_batch_size_recommendation(32)\n",
    "print(f\"üìè Recommended batch size: {recommended_batch_size}\")\n",
    "\n",
    "# Store device info for later use\n",
    "DEVICE = device\n",
    "DEVICE_TYPE = device_type\n",
    "BATCH_SIZE = recommended_batch_size\n",
    "\n",
    "print(f\"\\nüéØ Training will use: {DEVICE} with batch size {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Processing Pipeline\n",
    "\n",
    "Run the data processing pipeline to convert raw data into training-ready features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 15:37:41,097 - INFO - DataProcessingPipeline initialized\n",
      "2025-08-31 15:37:41,101 - INFO - ================================================================================\n",
      "2025-08-31 15:37:41,101 - INFO - STARTING COMPLETE DATA PROCESSING PIPELINE\n",
      "2025-08-31 15:37:41,106 - INFO - ================================================================================\n",
      "2025-08-31 15:37:41,106 - INFO - STEP 1: FEATURE GENERATION\n",
      "2025-08-31 15:37:41,106 - INFO - ----------------------------------------\n",
      "2025-08-31 15:37:41,106 - INFO - Running feature generator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing Data Processing Pipeline...\n",
      "================================================================================\n",
      "üìÇ Raw data path: data\\raw\n",
      "üìÇ Final data path: data\\final\n",
      "üìÑ Found 60 raw data files:\n",
      "  - Bankex_10.csv\n",
      "  - Bankex_120.csv\n",
      "  - Bankex_15.csv\n",
      "  - Bankex_180.csv\n",
      "  - Bankex_2.csv\n",
      "  ... and 55 more files\n",
      "\n",
      "üöÄ Running Feature Generation Pipeline...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas_ta\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n",
      "2025-08-31 15:37:41,744 - INFO - Found 60 CSV files to process\n",
      "2025-08-31 15:37:41,770 - INFO - Dropped 'volume' column from Bankex_10.csv\n",
      "2025-08-31 15:37:41,786 - INFO - Loaded 10127 rows from Bankex_10.csv\n",
      "2025-08-31 15:37:41,786 - INFO - Processing in-memory DataFrame with 10127 rows...\n",
      "2025-08-31 15:37:45,426 - INFO - ‚úÖ Enhanced datetime processing: epoch as feature, readable as index\n",
      "2025-08-31 15:37:55,218 - INFO - Removed 199 rows with NaN values. Final dataset: 9928 rows\n",
      "2025-08-31 15:37:55,221 - INFO - Generated 48 features for DataFrame\n",
      "2025-08-31 15:37:55,768 - INFO - Processed Bankex_10.csv: 9928 rows, 53 features\n",
      "2025-08-31 15:37:55,768 - INFO - Replaced existing file: data\\final\\features_Bankex_10.csv\n",
      "2025-08-31 15:37:55,776 - INFO - Dropped 'volume' column from Bankex_120.csv\n",
      "2025-08-31 15:37:55,781 - INFO - Loaded 1067 rows from Bankex_120.csv\n",
      "2025-08-31 15:37:55,781 - INFO - Processing in-memory DataFrame with 1067 rows...\n",
      "2025-08-31 15:37:55,789 - INFO - ‚úÖ Enhanced datetime processing: epoch as feature, readable as index\n",
      "2025-08-31 15:37:56,813 - INFO - Removed 199 rows with NaN values. Final dataset: 868 rows\n",
      "2025-08-31 15:37:56,816 - INFO - Generated 48 features for DataFrame\n",
      "2025-08-31 15:37:56,868 - INFO - Processed Bankex_120.csv: 868 rows, 53 features\n",
      "2025-08-31 15:37:56,868 - INFO - Replaced existing file: data\\final\\features_Bankex_120.csv\n",
      "2025-08-31 15:37:56,885 - INFO - Dropped 'volume' column from Bankex_15.csv\n",
      "2025-08-31 15:37:56,893 - INFO - Loaded 6661 rows from Bankex_15.csv\n",
      "2025-08-31 15:37:56,893 - INFO - Processing in-memory DataFrame with 6661 rows...\n",
      "2025-08-31 15:37:56,903 - INFO - ‚úÖ Enhanced datetime processing: epoch as feature, readable as index\n",
      "2025-08-31 15:38:03,626 - INFO - Removed 199 rows with NaN values. Final dataset: 6462 rows\n",
      "2025-08-31 15:38:03,626 - INFO - Generated 48 features for DataFrame\n",
      "2025-08-31 15:38:04,006 - INFO - Processed Bankex_15.csv: 6462 rows, 53 features\n",
      "2025-08-31 15:38:04,006 - INFO - Replaced existing file: data\\final\\features_Bankex_15.csv\n",
      "2025-08-31 15:38:04,011 - INFO - Dropped 'volume' column from Bankex_180.csv\n",
      "2025-08-31 15:38:04,016 - INFO - Loaded 802 rows from Bankex_180.csv\n",
      "2025-08-31 15:38:04,016 - INFO - Processing in-memory DataFrame with 802 rows...\n",
      "2025-08-31 15:38:04,028 - INFO - ‚úÖ Enhanced datetime processing: epoch as feature, readable as index\n",
      "2025-08-31 15:38:04,866 - INFO - Removed 199 rows with NaN values. Final dataset: 603 rows\n",
      "2025-08-31 15:38:04,866 - INFO - Generated 48 features for DataFrame\n",
      "2025-08-31 15:38:04,907 - INFO - Processed Bankex_180.csv: 603 rows, 53 features\n",
      "2025-08-31 15:38:04,907 - INFO - Replaced existing file: data\\final\\features_Bankex_180.csv\n",
      "2025-08-31 15:38:04,977 - INFO - Dropped 'volume' column from Bankex_2.csv\n",
      "2025-08-31 15:38:05,000 - INFO - Loaded 50093 rows from Bankex_2.csv\n",
      "2025-08-31 15:38:05,001 - INFO - Processing in-memory DataFrame with 50093 rows...\n",
      "2025-08-31 15:38:05,026 - INFO - ‚úÖ Enhanced datetime processing: epoch as feature, readable as index\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     result = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_complete_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_data_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     43\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Data Processing Pipeline Completed Successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src\\data_processing\\pipeline.py:123\u001b[39m, in \u001b[36mDataProcessingPipeline.run_complete_pipeline\u001b[39m\u001b[34m(self, input_dir, output_dir)\u001b[39m\n\u001b[32m    120\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mSTEP 1: FEATURE GENERATION\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    121\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m feature_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_feature_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feature_results[\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    127\u001b[39m         \u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    128\u001b[39m         \u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeature generation failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_results[\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    129\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mstep_failed\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mfeature_generation\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    130\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src\\data_processing\\pipeline.py:187\u001b[39m, in \u001b[36mDataProcessingPipeline.run_feature_generation\u001b[39m\u001b[34m(self, input_dir, output_dir)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# Get the processor class and run it\u001b[39;00m\n\u001b[32m    186\u001b[39m processor = feature_module.DynamicFileProcessor()\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m results = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_all_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[32m    190\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mFeature generator completed successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src/data_processing/feature_generator.py:454\u001b[39m, in \u001b[36mDynamicFileProcessor.process_all_files\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    451\u001b[39m df = \u001b[38;5;28mself\u001b[39m.load_and_validate_data(file_path)\n\u001b[32m    453\u001b[39m \u001b[38;5;66;03m# Process the dataframe\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m processed_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;66;03m# Save processed file (replace existing)\u001b[39;00m\n\u001b[32m    457\u001b[39m output_path = \u001b[38;5;28mself\u001b[39m.processed_folder / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfeatures_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src/data_processing/feature_generator.py:426\u001b[39m, in \u001b[36mDynamicFileProcessor.process_dataframe\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    423\u001b[39m close_prices = df[\u001b[33m'\u001b[39m\u001b[33mclose\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    425\u001b[39m \u001b[38;5;66;03m# Generate all features\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m features_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_all_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopen_prices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhigh_prices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_prices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_prices\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Combine with original data\u001b[39;00m\n\u001b[32m    431\u001b[39m result_df = df.join(features_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src/data_processing/feature_generator.py:264\u001b[39m, in \u001b[36mDynamicFileProcessor.generate_all_features\u001b[39m\u001b[34m(self, open_prices, high_prices, low_prices, close_prices)\u001b[39m\n\u001b[32m    254\u001b[39m     features.update({\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mbb_upper\u001b[39m\u001b[33m'\u001b[39m: bb_data[bb_upper_col],\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mbb_middle\u001b[39m\u001b[33m'\u001b[39m: bb_data[bb_middle_col],\n\u001b[32m   (...)\u001b[39m\u001b[32m    259\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mbb_position\u001b[39m\u001b[33m'\u001b[39m: (close_prices - bb_data[bb_lower_col]) / (bb_data[bb_upper_col] - bb_data[bb_lower_col]) * \u001b[32m100\u001b[39m\n\u001b[32m    260\u001b[39m     })\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# === MARKET STRUCTURE ===\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# Trend Analysis\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m trend_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmarket_structure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrend_strength\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclose_prices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m features.update(trend_data)\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# === PRICE ACTION FEATURES ===\u001b[39;00m\n\u001b[32m    268\u001b[39m \u001b[38;5;66;03m# Price changes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src/data_processing/feature_generator.py:67\u001b[39m, in \u001b[36mMarketStructureAnalyzer.trend_strength\u001b[39m\u001b[34m(close, period)\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m trend_strength = \u001b[43mclose\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrolling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalculate_r_squared\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     70\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrend_slope\u001b[39m\u001b[33m'\u001b[39m: trend_slope,\n\u001b[32m     71\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrend_strength\u001b[39m\u001b[33m'\u001b[39m: trend_strength,\n\u001b[32m     72\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrend_direction\u001b[39m\u001b[33m'\u001b[39m: np.where(trend_slope > \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m     73\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:2049\u001b[39m, in \u001b[36mRolling.apply\u001b[39m\u001b[34m(self, func, raw, engine, engine_kwargs, args, kwargs)\u001b[39m\n\u001b[32m   2016\u001b[39m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[32m   2017\u001b[39m     template_header,\n\u001b[32m   2018\u001b[39m     create_section_header(\u001b[33m\"\u001b[39m\u001b[33mParameters\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2047\u001b[39m     kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2048\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m2049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:1508\u001b[39m, in \u001b[36mRollingAndExpandingMixin.apply\u001b[39m\u001b[34m(self, func, raw, engine, engine_kwargs, args, kwargs)\u001b[39m\n\u001b[32m   1505\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1506\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mengine must be either \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnumba\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcython\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1508\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapply_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapply\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnumba_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumba_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1512\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:619\u001b[39m, in \u001b[36mBaseWindow._apply\u001b[39m\u001b[34m(self, func, name, numeric_only, numba_args, **kwargs)\u001b[39m\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.method == \u001b[33m\"\u001b[39m\u001b[33msingle\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_columnwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomogeneous_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply_tablewise(homogeneous_func, name, numeric_only)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:472\u001b[39m, in \u001b[36mBaseWindow._apply_columnwise\u001b[39m\u001b[34m(self, homogeneous_func, name, numeric_only)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_numeric_only(name, numeric_only)\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomogeneous_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m obj = \u001b[38;5;28mself\u001b[39m._create_data(\u001b[38;5;28mself\u001b[39m._selected_obj, numeric_only)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# GH 12541: Special case for count where we support date-like types\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:456\u001b[39m, in \u001b[36mBaseWindow._apply_series\u001b[39m\u001b[34m(self, homogeneous_func, name)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DataError(\u001b[33m\"\u001b[39m\u001b[33mNo numeric types to aggregate\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m result = \u001b[43mhomogeneous_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m index = \u001b[38;5;28mself\u001b[39m._slice_axis_for_step(obj.index, result)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor(result, index=index, name=obj.name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:614\u001b[39m, in \u001b[36mBaseWindow._apply.<locals>.homogeneous_func\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(x, start, end, min_periods, *numba_args)\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(\u001b[38;5;28mall\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m     result = \u001b[43mcalc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:611\u001b[39m, in \u001b[36mBaseWindow._apply.<locals>.homogeneous_func.<locals>.calc\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    602\u001b[39m start, end = window_indexer.get_window_bounds(\n\u001b[32m    603\u001b[39m     num_values=\u001b[38;5;28mlen\u001b[39m(x),\n\u001b[32m    604\u001b[39m     min_periods=min_periods,\n\u001b[32m   (...)\u001b[39m\u001b[32m    607\u001b[39m     step=\u001b[38;5;28mself\u001b[39m.step,\n\u001b[32m    608\u001b[39m )\n\u001b[32m    609\u001b[39m \u001b[38;5;28mself\u001b[39m._check_window_bounds(start, end, \u001b[38;5;28mlen\u001b[39m(x))\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mnumba_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:1535\u001b[39m, in \u001b[36mRollingAndExpandingMixin._generate_cython_apply_func.<locals>.apply_func\u001b[39m\u001b[34m(values, begin, end, min_periods, raw)\u001b[39m\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw:\n\u001b[32m   1533\u001b[39m     \u001b[38;5;66;03m# GH 45912\u001b[39;00m\n\u001b[32m   1534\u001b[39m     values = Series(values, index=\u001b[38;5;28mself\u001b[39m._on, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1535\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwindow_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/window/aggregations.pyx:1422\u001b[39m, in \u001b[36mpandas._libs.window.aggregations.roll_apply\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src/data_processing/feature_generator.py:57\u001b[39m, in \u001b[36mMarketStructureAnalyzer.trend_strength.<locals>.calculate_r_squared\u001b[39m\u001b[34m(series)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(series) < \u001b[32m2\u001b[39m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m x = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     59\u001b[39m     slope, intercept = np.polyfit(x, series, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Import data processing pipeline\n",
    "from src.data_processing.pipeline import DataProcessingPipeline\n",
    "\n",
    "print(\"üîÑ Initializing Data Processing Pipeline...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = DataProcessingPipeline()\n",
    "\n",
    "# Check if raw data exists\n",
    "raw_data_path = Path('data/raw')\n",
    "final_data_path = Path('data/final')\n",
    "\n",
    "print(f\"üìÇ Raw data path: {raw_data_path}\")\n",
    "print(f\"üìÇ Final data path: {final_data_path}\")\n",
    "\n",
    "# List raw data files\n",
    "if raw_data_path.exists():\n",
    "    raw_files = list(raw_data_path.glob(\"*.csv\"))\n",
    "    print(f\"üìÑ Found {len(raw_files)} raw data files:\")\n",
    "    for file in raw_files[:5]:  # Show first 5 files\n",
    "        print(f\"  - {file.name}\")\n",
    "    if len(raw_files) > 5:\n",
    "        print(f\"  ... and {len(raw_files) - 5} more files\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No raw data directory found\")\n",
    "\n",
    "# Run feature generation pipeline\n",
    "print(\"\\nüöÄ Running Feature Generation Pipeline...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    result = pipeline.run_complete_pipeline(\n",
    "        input_dir=str(raw_data_path),\n",
    "        output_dir=str(final_data_path)\n",
    "    )\n",
    "    \n",
    "    if result['success']:\n",
    "        print(\"\\n‚úÖ Data Processing Pipeline Completed Successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üìä Summary:\")\n",
    "        print(f\"  ‚Ä¢ Total files processed: {result.get('total_files_processed', 'Unknown')}\")\n",
    "        print(f\"  ‚Ä¢ Total rows processed: {result.get('total_rows_processed', 0):,}\")\n",
    "        print(f\"  ‚Ä¢ Processing time: {result.get('total_time_formatted', 'Unknown')}\")\n",
    "        print(f\"  ‚Ä¢ Output directory: {result.get('output_directory', 'Unknown')}\")\n",
    "        \n",
    "        # List final data files\n",
    "        if final_data_path.exists():\n",
    "            final_files = list(final_data_path.glob(\"features_*.csv\"))\n",
    "            print(f\"\\nüìà Generated {len(final_files)} feature files:\")\n",
    "            for file in final_files:\n",
    "                print(f\"  - {file.name}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Data processing failed: {result.get('error', 'Unknown error')}\")\n",
    "        raise Exception(\"Data processing pipeline failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Pipeline execution failed: {str(e)}\")\n",
    "    # Continue anyway if some data exists\n",
    "    if final_data_path.exists() and list(final_data_path.glob(\"features_*.csv\")):\n",
    "        print(\"‚ö†Ô∏è Using existing processed data files\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(\"\\nüéØ Data processing complete. Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HRM Training Configuration\n",
    "\n",
    "Configure the HRM training parameters and initialize the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è HRM Configuration Loaded:\n",
      "==================================================\n",
      "üß† Model Architecture:\n",
      "  ‚Ä¢ Hidden Dimension: 512\n",
      "  ‚Ä¢ H-Module Layers: 8\n",
      "  ‚Ä¢ L-Module Layers: 6\n",
      "  ‚Ä¢ Attention Heads: 16\n",
      "  ‚Ä¢ H-Cycles: 2\n",
      "  ‚Ä¢ L-Cycles: 6\n",
      "\n",
      "üîÑ Hierarchical Processing:\n",
      "  ‚Ä¢ H-Module Lookback: 100 candles\n",
      "  ‚Ä¢ L-Module Lookback: 15 candles\n",
      "\n",
      "üéì Training Configuration:\n",
      "  ‚Ä¢ Base Learning Rate: 0.0001\n",
      "  ‚Ä¢ Strategic LR: 5e-05\n",
      "  ‚Ä¢ Tactical LR: 0.0001\n",
      "  ‚Ä¢ Optimizer: AdamW\n",
      "\n",
      "üéØ Notebook Training Parameters:\n",
      "  ‚Ä¢ epochs: 100\n",
      "  ‚Ä¢ save_frequency: 25\n",
      "  ‚Ä¢ log_frequency: 5\n",
      "  ‚Ä¢ validation_frequency: 10\n",
      "  ‚Ä¢ debug_mode: True\n",
      "  ‚Ä¢ config_path: config/hrm_config.yaml\n",
      "  ‚Ä¢ data_path: data/final\n",
      "\n",
      "üì± Device Configuration:\n",
      "  ‚Ä¢ Device: cpu\n",
      "  ‚Ä¢ Device Type: cpu\n",
      "  ‚Ä¢ Batch Size: 16\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Load HRM configuration\n",
    "with open('config/hrm_config.yaml', 'r') as f:\n",
    "    hrm_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"‚öôÔ∏è HRM Configuration Loaded:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üß† Model Architecture:\")\n",
    "print(f\"  ‚Ä¢ Hidden Dimension: {hrm_config['model_architecture']['hidden_dim']}\")\n",
    "print(f\"  ‚Ä¢ H-Module Layers: {hrm_config['model_architecture']['H_layers']}\")\n",
    "print(f\"  ‚Ä¢ L-Module Layers: {hrm_config['model_architecture']['L_layers']}\")\n",
    "print(f\"  ‚Ä¢ Attention Heads: {hrm_config['model_architecture']['num_heads']}\")\n",
    "print(f\"  ‚Ä¢ H-Cycles: {hrm_config['model_architecture']['H_cycles']}\")\n",
    "print(f\"  ‚Ä¢ L-Cycles: {hrm_config['model_architecture']['L_cycles']}\")\n",
    "\n",
    "print(f\"\\nüîÑ Hierarchical Processing:\")\n",
    "print(f\"  ‚Ä¢ H-Module Lookback: {hrm_config['hierarchical_processing']['high_level_lookback']} candles\")\n",
    "print(f\"  ‚Ä¢ L-Module Lookback: {hrm_config['hierarchical_processing']['low_level_lookback']} candles\")\n",
    "\n",
    "print(f\"\\nüéì Training Configuration:\")\n",
    "print(f\"  ‚Ä¢ Base Learning Rate: {hrm_config['training']['learning_rates']['base_lr']}\")\n",
    "print(f\"  ‚Ä¢ Strategic LR: {hrm_config['training']['learning_rates']['strategic_lr']}\")\n",
    "print(f\"  ‚Ä¢ Tactical LR: {hrm_config['training']['learning_rates']['tactical_lr']}\")\n",
    "print(f\"  ‚Ä¢ Optimizer: {hrm_config['training']['optimizer']}\")\n",
    "\n",
    "# Training parameters for notebook environment\n",
    "TRAINING_PARAMS = {\n",
    "    'epochs': 100,  # Start with 100 epochs, can be increased\n",
    "    'save_frequency': 25,  # Save checkpoint every 25 epochs\n",
    "    'log_frequency': 5,   # Log progress every 5 epochs\n",
    "    'validation_frequency': 10,  # Validate every 10 epochs\n",
    "    'debug_mode': True,  # Set to True for detailed step-by-step logging\n",
    "    'config_path': 'config/hrm_config.yaml',\n",
    "    'data_path': 'data/final'\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ Notebook Training Parameters:\")\n",
    "for key, value in TRAINING_PARAMS.items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüì± Device Configuration:\")\n",
    "print(f\"  ‚Ä¢ Device: {DEVICE}\")\n",
    "print(f\"  ‚Ä¢ Device Type: {DEVICE_TYPE}\")\n",
    "print(f\"  ‚Ä¢ Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize HRM Training Pipeline\n",
    "\n",
    "Initialize the HRM training pipeline with automatic device detection and parallel training support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 15:40:26,638 - INFO - Loaded configuration from config/hrm_config.yaml\n",
      "2025-08-31 15:40:26,642 - INFO - Validating 2 instrument symbols: ['Bankex', 'Bank_Nifty']\n",
      "2025-08-31 15:40:26,643 - INFO - Found valid instrument: Bankex (ID: 2)\n",
      "2025-08-31 15:40:26,644 - INFO - Found valid instrument: Bank_Nifty (ID: 0)\n",
      "2025-08-31 15:40:26,644 - INFO - Found 2 valid instruments for training\n",
      "2025-08-31 15:40:26,646 - INFO - HRM Training Pipeline initialized on cpu\n",
      "2025-08-31 15:40:26,648 - INFO - Debug mode: ON - detailed step-by-step logging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing HRM Training Pipeline...\n",
      "================================================================================\n",
      "‚úÖ Training pipeline initialized successfully!\n",
      "üìä Available instruments: 2\n",
      "\n",
      "üìà Available Training Instruments:\n",
      "  1. Bankex\n",
      "  2. Bank_Nifty\n",
      "\n",
      "üéØ Auto-selected instrument: Bankex\n",
      "\n",
      "üî• Ready to start HRM training!\n"
     ]
    }
   ],
   "source": [
    "from run_training import HRMTrainingPipeline\n",
    "import logging\n",
    "\n",
    "# Setup enhanced logging for notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Only console output for notebook\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üöÄ Initializing HRM Training Pipeline...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Initialize training pipeline with device detection\n",
    "    pipeline = HRMTrainingPipeline(\n",
    "        config_path=TRAINING_PARAMS['config_path'],\n",
    "        data_path=TRAINING_PARAMS['data_path'],\n",
    "        device=str(DEVICE),\n",
    "        debug_mode=TRAINING_PARAMS['debug_mode']\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Training pipeline initialized successfully!\")\n",
    "    print(f\"üìä Available instruments: {len(pipeline.available_instruments)}\")\n",
    "    \n",
    "    # List available instruments\n",
    "    if pipeline.available_instruments:\n",
    "        print(\"\\nüìà Available Training Instruments:\")\n",
    "        for i, instrument in enumerate(pipeline.available_instruments, 1):\n",
    "            print(f\"  {i}. {instrument}\")\n",
    "        \n",
    "        # Auto-select first instrument for training\n",
    "        selected_instrument = pipeline.available_instruments[0]\n",
    "        print(f\"\\nüéØ Auto-selected instrument: {selected_instrument}\")\n",
    "    else:\n",
    "        raise ValueError(\"No instruments available for training\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize training pipeline: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nüî• Ready to start HRM training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loss Visualization Setup\n",
    "\n",
    "Setup real-time visualization for training metrics and loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Training visualizer initialized!\n",
      "üìà Real-time plots will be updated during training\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class HRMTrainingVisualizer:\n",
    "    \"\"\"Real-time training visualization for Jupyter notebooks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_history = []\n",
    "        self.loss_history = []\n",
    "        self.reward_history = []\n",
    "        self.best_reward = float('-inf')\n",
    "        \n",
    "        # Setup plots\n",
    "        self.fig = None\n",
    "        self.setup_plots()\n",
    "    \n",
    "    def setup_plots(self):\n",
    "        \"\"\"Initialize the plotting framework\"\"\"\n",
    "        plt.style.use('default')\n",
    "        plt.rcParams['figure.figsize'] = (15, 10)\n",
    "        \n",
    "    def update_metrics(self, episode, metrics):\n",
    "        \"\"\"Update training metrics\"\"\"\n",
    "        self.training_history.append({\n",
    "            'episode': episode,\n",
    "            'total_reward': metrics.get('total_reward', 0),\n",
    "            'avg_reward': metrics.get('avg_reward', 0),\n",
    "            'total_loss': metrics.get('total_loss', 0),\n",
    "            'avg_loss': metrics.get('avg_loss', 0),\n",
    "            'steps': metrics.get('steps', 0)\n",
    "        })\n",
    "        \n",
    "        self.reward_history.append(metrics.get('avg_reward', 0))\n",
    "        self.loss_history.append(metrics.get('avg_loss', 0))\n",
    "        \n",
    "        if metrics.get('avg_reward', 0) > self.best_reward:\n",
    "            self.best_reward = metrics.get('avg_reward', 0)\n",
    "    \n",
    "    def create_training_plots(self):\n",
    "        \"\"\"Create comprehensive training plots\"\"\"\n",
    "        if len(self.training_history) < 2:\n",
    "            return\n",
    "        \n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Average Reward per Episode', 'Average Loss per Episode', \n",
    "                          'Total Reward Trend', 'Training Progress Summary'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        episodes = [h['episode'] for h in self.training_history]\n",
    "        \n",
    "        # Reward plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=episodes, y=self.reward_history, \n",
    "                      mode='lines+markers', name='Avg Reward',\n",
    "                      line=dict(color='green', width=2)),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Loss plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=episodes, y=self.loss_history,\n",
    "                      mode='lines+markers', name='Avg Loss',\n",
    "                      line=dict(color='red', width=2)),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Total reward trend\n",
    "        total_rewards = [h['total_reward'] for h in self.training_history]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=episodes, y=total_rewards,\n",
    "                      mode='lines+markers', name='Total Reward',\n",
    "                      line=dict(color='blue', width=2)),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Summary metrics\n",
    "        if len(self.reward_history) >= 10:\n",
    "            recent_avg_reward = np.mean(self.reward_history[-10:])\n",
    "            recent_avg_loss = np.mean(self.loss_history[-10:])\n",
    "        else:\n",
    "            recent_avg_reward = np.mean(self.reward_history)\n",
    "            recent_avg_loss = np.mean(self.loss_history)\n",
    "        \n",
    "        # Create summary text\n",
    "        summary_text = f\"\"\"\n",
    "        üìä Training Summary (Last 10 Episodes)\n",
    "        ‚Ä¢ Best Reward: {self.best_reward:.4f}\n",
    "        ‚Ä¢ Recent Avg Reward: {recent_avg_reward:.4f}\n",
    "        ‚Ä¢ Recent Avg Loss: {recent_avg_loss:.4f}\n",
    "        ‚Ä¢ Total Episodes: {len(self.training_history)}\n",
    "        \"\"\"\n",
    "        \n",
    "        fig.add_annotation(\n",
    "            text=summary_text,\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.75, y=0.3, xanchor='left', yanchor='top',\n",
    "            showarrow=False,\n",
    "            font=dict(size=12, family=\"monospace\"),\n",
    "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "            bordercolor=\"black\",\n",
    "            borderwidth=1\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title_text=\"HRM Training Progress - Real-time Monitoring\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def display_current_metrics(self):\n",
    "        \"\"\"Display current training metrics\"\"\"\n",
    "        if not self.training_history:\n",
    "            return\n",
    "            \n",
    "        latest = self.training_history[-1]\n",
    "        \n",
    "        print(f\"\\nüìà Episode {latest['episode']} Results:\")\n",
    "        print(f\"  üéØ Average Reward: {latest['avg_reward']:.4f}\")\n",
    "        print(f\"  üìâ Average Loss: {latest['avg_loss']:.4f}\")\n",
    "        print(f\"  üéÆ Steps Completed: {latest['steps']}\")\n",
    "        print(f\"  üèÜ Best Reward So Far: {self.best_reward:.4f}\")\n",
    "        \n",
    "        if latest['avg_reward'] == self.best_reward:\n",
    "            print(\"  üåü NEW BEST PERFORMANCE! üåü\")\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = HRMTrainingVisualizer()\n",
    "print(\"üìä Training visualizer initialized!\")\n",
    "print(\"üìà Real-time plots will be updated during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start HRM Training\n",
    "\n",
    "Begin the HRM training process with real-time monitoring and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting HRM Training Process...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148e892879254efa8f27421384ca93cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, bar_style='info', description='Training:', style=ProgressStyle(bar_color='‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 15:40:39,733 - INFO - setup_training called with: Bankex, timeframe: 5\n",
      "2025-08-31 15:40:39,734 - INFO - Available instruments: ['Bankex', 'Bank_Nifty']\n",
      "2025-08-31 15:40:39,737 - WARNING - Exact file features_Bankex_5.csv not found. Available files:\n",
      "2025-08-31 15:40:39,738 - WARNING -   features_Bankex_10.csv\n",
      "2025-08-31 15:40:39,739 - WARNING -   features_Bankex_120.csv\n",
      "2025-08-31 15:40:39,740 - WARNING -   features_Bankex_15.csv\n",
      "2025-08-31 15:40:39,741 - WARNING -   features_Bankex_180.csv\n",
      "2025-08-31 15:40:39,741 - INFO - Using available file: features_Bankex_10.csv -> symbol: Bankex_10\n",
      "2025-08-31 15:40:39,793 - INFO - Trainer initialized for Bankex\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error in episode 1: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 2: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 3: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 4: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 5: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 6: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 7: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 8: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 9: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 10: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 11: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 12: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 13: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 14: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 15: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 16: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 17: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 18: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 19: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 20: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 21: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 22: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 23: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 24: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 25: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 26: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 27: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 28: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 29: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 30: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 31: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 32: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 33: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 34: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 35: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 36: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 37: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 38: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 39: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 40: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 41: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 42: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 43: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 44: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 45: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 46: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 47: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 48: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 49: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 50: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 51: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 52: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 53: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 54: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 55: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 56: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 57: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 58: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 59: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 60: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 61: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 62: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 63: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 64: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 65: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 66: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 67: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 68: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 69: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 70: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 71: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 72: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 73: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 74: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 75: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 76: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 77: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 78: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 79: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 80: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 81: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 82: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 83: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 84: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 85: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 86: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 87: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 88: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 89: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 90: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 91: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 92: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 93: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 94: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 95: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 96: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 97: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 98: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 99: 'HRMTrainer' object has no attribute 'env'\n",
      "‚ùå Error in episode 100: 'HRMTrainer' object has no attribute 'env'\n",
      "\n",
      "‚úÖ Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "print(\"üöÄ Starting HRM Training Process...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create training progress widgets\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=TRAINING_PARAMS['epochs'],\n",
    "    description='Training:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': 'green'},\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "status_text = widgets.HTML(value=\"<b>Initializing training...</b>\")\n",
    "metrics_text = widgets.HTML(value=\"\")\n",
    "\n",
    "display(widgets.VBox([progress_bar, status_text, metrics_text]))\n",
    "\n",
    "# Custom training loop with visualization\n",
    "class HRMTrainingMonitor:\n",
    "    def __init__(self, pipeline, visualizer):\n",
    "        self.pipeline = pipeline\n",
    "        self.visualizer = visualizer\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def run_training_with_monitoring(self, epochs, instrument_symbol):\n",
    "        \"\"\"Run training with real-time monitoring\"\"\"\n",
    "        \n",
    "        # Setup training \n",
    "        self.pipeline.setup_training(instrument_symbol)\n",
    "        \n",
    "        status_text.value = f\"<b>üéØ Training on {instrument_symbol} for {epochs} epochs</b>\"\n",
    "        \n",
    "        training_history = []\n",
    "        \n",
    "        for episode in range(epochs):\n",
    "            episode_start_time = time.time()\n",
    "            \n",
    "            # Run single episode\n",
    "            try:\n",
    "                metrics = self.pipeline.trainer.train_episode()\n",
    "                training_history.append(metrics)\n",
    "                \n",
    "                # Update visualizer\n",
    "                self.visualizer.update_metrics(episode + 1, metrics)\n",
    "                \n",
    "                # Advance to next episode\n",
    "                self.pipeline.trainer._advance_to_next_episode()\n",
    "                \n",
    "                # Update learning rate\n",
    "                if self.pipeline.trainer.scheduler:\n",
    "                    self.pipeline.trainer.scheduler.step()\n",
    "                \n",
    "                # Update progress\n",
    "                progress_bar.value = episode + 1\n",
    "                \n",
    "                # Calculate timing\n",
    "                episode_time = time.time() - episode_start_time\n",
    "                elapsed_time = time.time() - self.start_time\n",
    "                estimated_total = elapsed_time * epochs / (episode + 1)\n",
    "                remaining_time = estimated_total - elapsed_time\n",
    "                \n",
    "                # Update status\n",
    "                status_text.value = f\"\"\"\n",
    "                <b>üìà Episode {episode + 1}/{epochs} completed</b><br>\n",
    "                ‚è±Ô∏è Episode Time: {episode_time:.1f}s<br>\n",
    "                üïê Elapsed: {elapsed_time/60:.1f}min | Remaining: {remaining_time/60:.1f}min\n",
    "                \"\"\"\n",
    "                \n",
    "                # Update metrics display\n",
    "                recent_rewards = [h['avg_reward'] for h in training_history[-5:]]\n",
    "                recent_losses = [h['avg_loss'] for h in training_history[-5:]]\n",
    "                \n",
    "                metrics_text.value = f\"\"\"\n",
    "                <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
    "                <b>üìä Current Metrics:</b><br>\n",
    "                üéØ Avg Reward: <span style=\"color: green;\"><b>{metrics['avg_reward']:.4f}</b></span><br>\n",
    "                üìâ Avg Loss: <span style=\"color: red;\"><b>{metrics['avg_loss']:.4f}</b></span><br>\n",
    "                üéÆ Steps: {metrics['steps']}<br>\n",
    "                üèÜ Best Reward: <span style=\"color: blue;\"><b>{self.visualizer.best_reward:.4f}</b></span><br>\n",
    "                üìà Recent Avg Reward (last 5): {np.mean(recent_rewards):.4f}<br>\n",
    "                üìâ Recent Avg Loss (last 5): {np.mean(recent_losses):.4f}\n",
    "                </div>\n",
    "                \"\"\"\n",
    "                \n",
    "                # Save checkpoints\n",
    "                if (episode + 1) % TRAINING_PARAMS['save_frequency'] == 0:\n",
    "                    self.pipeline.trainer._save_latest_checkpoint(episode)\n",
    "                    \n",
    "                # Check for best model\n",
    "                if metrics['avg_reward'] > self.pipeline.trainer.best_performance:\n",
    "                    self.pipeline.trainer.best_performance = metrics['avg_reward']\n",
    "                    self.pipeline.trainer._save_best_model()\n",
    "                    \n",
    "                # Update plots every 10 episodes\n",
    "                if (episode + 1) % 10 == 0:\n",
    "                    clear_output(wait=True)\n",
    "                    display(widgets.VBox([progress_bar, status_text, metrics_text]))\n",
    "                    \n",
    "                    # Show training plots\n",
    "                    fig = self.visualizer.create_training_plots()\n",
    "                    if fig:\n",
    "                        fig.show()\n",
    "                        \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error in episode {episode + 1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return training_history\n",
    "\n",
    "# Initialize training monitor\n",
    "monitor = HRMTrainingMonitor(pipeline, visualizer)\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    training_results = monitor.run_training_with_monitoring(\n",
    "        epochs=TRAINING_PARAMS['epochs'],\n",
    "        instrument_symbol=selected_instrument\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Results Analysis\n",
    "\n",
    "Analyze the training results and evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üìä Analyzing Training Results...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Final training visualization\n",
    "if visualizer.training_history:\n",
    "    print(\"üìà Generating Final Training Report...\")\n",
    "    \n",
    "    # Create comprehensive plots\n",
    "    final_fig = visualizer.create_training_plots()\n",
    "    if final_fig:\n",
    "        final_fig.update_layout(title_text=\"HRM Training Results - Final Report\")\n",
    "        final_fig.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    rewards = visualizer.reward_history\n",
    "    losses = visualizer.loss_history\n",
    "    \n",
    "    print(f\"\\nüìà Training Statistics:\")\n",
    "    print(f\"  üéØ Total Episodes Completed: {len(rewards)}\")\n",
    "    print(f\"  üèÜ Best Reward Achieved: {visualizer.best_reward:.4f}\")\n",
    "    print(f\"  üìä Final Average Reward: {rewards[-1]:.4f}\")\n",
    "    print(f\"  üìâ Final Average Loss: {losses[-1]:.4f}\")\n",
    "    \n",
    "    if len(rewards) >= 20:\n",
    "        print(f\"\\nüìä Performance Trends:\")\n",
    "        early_reward = np.mean(rewards[:10])\n",
    "        late_reward = np.mean(rewards[-10:])\n",
    "        reward_improvement = late_reward - early_reward\n",
    "        \n",
    "        early_loss = np.mean(losses[:10])\n",
    "        late_loss = np.mean(losses[-10:])\n",
    "        loss_improvement = early_loss - late_loss\n",
    "        \n",
    "        print(f\"  üìà Reward Improvement: {reward_improvement:.4f} ({reward_improvement/early_reward*100:+.1f}%)\")\n",
    "        print(f\"  üìâ Loss Improvement: {loss_improvement:.4f} ({loss_improvement/early_loss*100:+.1f}%)\")\n",
    "    \n",
    "    # Model performance analysis\n",
    "    print(f\"\\nüß† Model Performance Analysis:\")\n",
    "    if len(rewards) > 0:\n",
    "        reward_variance = np.var(rewards)\n",
    "        reward_stability = 1.0 / (1.0 + reward_variance) if reward_variance > 0 else 1.0\n",
    "        print(f\"  üìä Reward Variance: {reward_variance:.4f}\")\n",
    "        print(f\"  üéØ Training Stability: {reward_stability:.3f}\")\n",
    "        \n",
    "        # Performance rating\n",
    "        if visualizer.best_reward > 0.1:\n",
    "            performance_rating = \"üåü Excellent\"\n",
    "        elif visualizer.best_reward > 0.05:\n",
    "            performance_rating = \"‚úÖ Good\"\n",
    "        elif visualizer.best_reward > 0.0:\n",
    "            performance_rating = \"‚ö†Ô∏è Fair\"\n",
    "        else:\n",
    "            performance_rating = \"‚ùå Poor\"\n",
    "            \n",
    "        print(f\"  üèÜ Overall Performance: {performance_rating}\")\n",
    "    \n",
    "    # Save training summary\n",
    "    summary_data = {\n",
    "        'episode': list(range(1, len(rewards) + 1)),\n",
    "        'avg_reward': rewards,\n",
    "        'avg_loss': losses,\n",
    "        'best_reward_so_far': [max(rewards[:i+1]) for i in range(len(rewards))]\n",
    "    }\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_dir = Path(\"training_results\")\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_path = results_dir / f\"hrm_training_summary_{timestamp}.csv\"\n",
    "    df_summary.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ Training summary saved to: {csv_path}\")\n",
    "    \n",
    "    # Display final data sample\n",
    "    print(f\"\\nüìã Training Summary (Last 10 Episodes):\")\n",
    "    print(df_summary.tail(10).to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training history available\")\n",
    "\n",
    "print(\"\\nüéâ Training analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Testing\n",
    "\n",
    "Evaluate the trained HRM model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Evaluating Trained HRM Model...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Run model evaluation\n",
    "    if hasattr(pipeline, 'trainer') and pipeline.trainer.model is not None:\n",
    "        print(\"üß™ Running model evaluation on test episodes...\")\n",
    "        \n",
    "        eval_results = pipeline.trainer.evaluate(\n",
    "            episodes=20,  # Evaluate on 20 test episodes\n",
    "            symbol=selected_instrument\n",
    "        )\n",
    "        \n",
    "        print(\"\\nüìä Model Evaluation Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for metric, value in eval_results.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  üìà {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  üìà {metric.replace('_', ' ').title()}: {value}\")\n",
    "        \n",
    "        # Model summary\n",
    "        print(\"\\nüß† HRM Model Architecture Summary:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        model_summary = pipeline.trainer.model.get_model_summary()\n",
    "        \n",
    "        print(f\"  üî¢ Total Parameters: {model_summary['total_parameters']:,}\")\n",
    "        print(f\"  üéì Trainable Parameters: {model_summary['trainable_parameters']:,}\")\n",
    "        print(f\"  üß† H-Module Parameters: {model_summary['h_module_parameters']:,}\")\n",
    "        print(f\"  ‚ö° L-Module Parameters: {model_summary['l_module_parameters']:,}\")\n",
    "        print(f\"  üîÑ ACT Module Parameters: {model_summary['act_module_parameters']:,}\")\n",
    "        print(f\"  üìö Deep Supervision Parameters: {model_summary['deep_supervision_parameters']:,}\")\n",
    "        print(f\"  üìä Hidden Dimension: {model_summary['hidden_dimension']}\")\n",
    "        print(f\"  üîÑ H-Cycles: {model_summary['H_cycles']} | L-Cycles: {model_summary['L_cycles']}\")\n",
    "        print(f\"  üëÅÔ∏è H-Lookback: {model_summary['h_lookback_window']} | L-Lookback: {model_summary['l_lookback_window']}\")\n",
    "        print(f\"  üíª Device: {model_summary['device']}\")\n",
    "        \n",
    "        # Checkpoint validation\n",
    "        print(\"\\nüîç Validating Model Checkpoints...\")\n",
    "        if pipeline.validate_checkpoints():\n",
    "            print(\"‚úÖ Model checkpoints are valid and ready for deployment\")\n",
    "            \n",
    "            # List saved models\n",
    "            model_dir = Path(\"models/hrm\")\n",
    "            checkpoint_dir = Path(\"checkpoints/hrm\")\n",
    "            \n",
    "            if model_dir.exists():\n",
    "                model_files = list(model_dir.glob(\"*.pt\"))\n",
    "                print(f\"\\nüìÅ Saved Models ({len(model_files)}):\")\n",
    "                for model_file in model_files:\n",
    "                    print(f\"  üíæ {model_file.name}\")\n",
    "            \n",
    "            if checkpoint_dir.exists():\n",
    "                checkpoint_files = list(checkpoint_dir.glob(\"*.pt\"))\n",
    "                print(f\"\\nüìÅ Training Checkpoints ({len(checkpoint_files)}):\")\n",
    "                for checkpoint_file in checkpoint_files:\n",
    "                    print(f\"  üíæ {checkpoint_file.name}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Checkpoint validation failed\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No trained model available for evaluation\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nüéØ Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results and Model\n",
    "\n",
    "Export training results, model files, and create deployment package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üì¶ Creating Export Package...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create export directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "export_dir = Path(f\"hrm_export_{timestamp}\")\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Export directory: {export_dir}\")\n",
    "\n",
    "try:\n",
    "    # 1. Copy model files\n",
    "    model_dir = Path(\"models/hrm\")\n",
    "    if model_dir.exists():\n",
    "        export_models_dir = export_dir / \"models\"\n",
    "        shutil.copytree(model_dir, export_models_dir)\n",
    "        print(f\"‚úÖ Models exported to: {export_models_dir}\")\n",
    "    \n",
    "    # 2. Copy checkpoints\n",
    "    checkpoint_dir = Path(\"checkpoints/hrm\")\n",
    "    if checkpoint_dir.exists():\n",
    "        export_checkpoints_dir = export_dir / \"checkpoints\"\n",
    "        shutil.copytree(checkpoint_dir, export_checkpoints_dir)\n",
    "        print(f\"‚úÖ Checkpoints exported to: {export_checkpoints_dir}\")\n",
    "    \n",
    "    # 3. Copy training results\n",
    "    results_dir = Path(\"training_results\")\n",
    "    if results_dir.exists():\n",
    "        export_results_dir = export_dir / \"training_results\"\n",
    "        shutil.copytree(results_dir, export_results_dir)\n",
    "        print(f\"‚úÖ Training results exported to: {export_results_dir}\")\n",
    "    \n",
    "    # 4. Copy configuration\n",
    "    config_files = ['config/hrm_config.yaml']\n",
    "    export_config_dir = export_dir / \"config\"\n",
    "    export_config_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for config_file in config_files:\n",
    "        if Path(config_file).exists():\n",
    "            shutil.copy2(config_file, export_config_dir)\n",
    "            print(f\"‚úÖ Config exported: {config_file}\")\n",
    "    \n",
    "    # 5. Create training summary report\n",
    "    summary_report = {\n",
    "        \"training_info\": {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"device_used\": str(DEVICE),\n",
    "            \"device_type\": DEVICE_TYPE,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epochs_completed\": len(visualizer.training_history) if visualizer.training_history else 0,\n",
    "            \"selected_instrument\": selected_instrument if 'selected_instrument' in locals() else \"Unknown\"\n",
    "        },\n",
    "        \"training_parameters\": TRAINING_PARAMS,\n",
    "        \"performance_summary\": {\n",
    "            \"best_reward\": float(visualizer.best_reward) if visualizer.training_history else 0.0,\n",
    "            \"final_reward\": float(visualizer.reward_history[-1]) if visualizer.reward_history else 0.0,\n",
    "            \"final_loss\": float(visualizer.loss_history[-1]) if visualizer.loss_history else 0.0,\n",
    "            \"total_episodes\": len(visualizer.training_history) if visualizer.training_history else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add evaluation results if available\n",
    "    if 'eval_results' in locals():\n",
    "        summary_report[\"evaluation_results\"] = {\n",
    "            k: float(v) if isinstance(v, (int, float)) else v \n",
    "            for k, v in eval_results.items()\n",
    "        }\n",
    "    \n",
    "    # Save summary report\n",
    "    summary_file = export_dir / \"training_summary.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2)\n",
    "    print(f\"‚úÖ Summary report saved: {summary_file}\")\n",
    "    \n",
    "    # 6. Create README for export\n",
    "    readme_content = f\"\"\"\n",
    "# HRM Trading Model Export Package\n",
    "\n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Training Device:** {DEVICE_TYPE.upper()}\n",
    "**Instrument:** {selected_instrument if 'selected_instrument' in locals() else 'Unknown'}\n",
    "\n",
    "## Contents\n",
    "\n",
    "- `models/` - Trained HRM models (best model: hrm_best_model.pt)\n",
    "- `checkpoints/` - Training checkpoints for resuming\n",
    "- `training_results/` - Training logs and metrics\n",
    "- `config/` - Model configuration files\n",
    "- `training_summary.json` - Complete training summary\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "- **Total Parameters:** {model_summary['total_parameters']:,} (‚âà27M)\n",
    "- **Architecture:** Hierarchical Reasoning Model (HRM)\n",
    "- **H-Module Lookback:** {model_summary['h_lookback_window']} candles\n",
    "- **L-Module Lookback:** {model_summary['l_lookback_window']} candles\n",
    "- **Hidden Dimension:** {model_summary['hidden_dimension']}\n",
    "\n",
    "## Performance\n",
    "\n",
    "- **Best Reward:** {visualizer.best_reward:.4f}\n",
    "- **Episodes Completed:** {len(visualizer.training_history) if visualizer.training_history else 0}\n",
    "- **Training Status:** {'Completed' if len(visualizer.training_history) >= TRAINING_PARAMS['epochs'] else 'Partial'}\n",
    "\n",
    "## Usage\n",
    "\n",
    "Load the best model:\n",
    "```python\n",
    "import torch\n",
    "model_data = torch.load('models/hrm_best_model.pt')\n",
    "# Model ready for inference\n",
    "```\n",
    "\n",
    "Resume training:\n",
    "```python\n",
    "trainer.load_checkpoint('checkpoints/latest_checkpoint.pt')\n",
    "# Continue training from checkpoint\n",
    "```\n",
    "    \"\"\"\n",
    "    \n",
    "    readme_file = export_dir / \"README.md\"\n",
    "    with open(readme_file, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    print(f\"‚úÖ README created: {readme_file}\")\n",
    "    \n",
    "    # 7. Create deployment script\n",
    "    deployment_script = f\"\"\"\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "HRM Model Deployment Script\n",
    "Auto-generated from training session: {timestamp}\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def load_hrm_model(model_path=\"models/hrm_best_model.pt\", device=\"auto\"):\n",
    "    \"\"\"Load trained HRM model for inference\"\"\"\n",
    "    \n",
    "    # Auto-detect device if requested\n",
    "    if device == \"auto\":\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "    \n",
    "    # Load model\n",
    "    model_data = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    print(f\"‚úÖ HRM Model loaded successfully\")\n",
    "    print(f\"üìä Performance: {{model_data['performance']:.4f}}\")\n",
    "    print(f\"üíª Device: {{device}}\")\n",
    "    print(f\"üìÖ Saved: {{model_data['timestamp']}}\")\n",
    "    \n",
    "    return model_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    model = load_hrm_model()\n",
    "    print(\"üöÄ HRM model ready for inference!\")\n",
    "    \"\"\"\n",
    "    \n",
    "    deploy_script_file = export_dir / \"deploy_model.py\"\n",
    "    with open(deploy_script_file, 'w') as f:\n",
    "        f.write(deployment_script)\n",
    "    print(f\"‚úÖ Deployment script created: {deploy_script_file}\")\n",
    "    \n",
    "    # 8. Calculate export size\n",
    "    def get_dir_size(path):\n",
    "        total_size = 0\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                total_size += os.path.getsize(filepath)\n",
    "        return total_size\n",
    "    \n",
    "    export_size_bytes = get_dir_size(export_dir)\n",
    "    export_size_mb = export_size_bytes / (1024 * 1024)\n",
    "    \n",
    "    print(f\"\\nüì¶ Export Package Summary:\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"üìÅ Location: {export_dir}\")\n",
    "    print(f\"üìè Size: {export_size_mb:.1f} MB\")\n",
    "    \n",
    "    # List contents\n",
    "    print(f\"\\nüìã Package Contents:\")\n",
    "    for item in sorted(export_dir.iterdir()):\n",
    "        if item.is_dir():\n",
    "            item_count = len(list(item.iterdir()))\n",
    "            print(f\"  üìÅ {item.name}/ ({item_count} items)\")\n",
    "        else:\n",
    "            item_size_kb = item.stat().st_size / 1024\n",
    "            print(f\"  üìÑ {item.name} ({item_size_kb:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Export package created successfully!\")\n",
    "    print(f\"üöÄ Ready for deployment or further analysis\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Export failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nüéâ Training and export process completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Summary and Next Steps\n",
    "\n",
    "Final summary of the training process and recommendations for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéä HRM TRAINING COMPLETED!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Final summary\n",
    "total_time = time.time() - monitor.start_time if 'monitor' in locals() else 0\n",
    "\n",
    "print(f\"\\nüìä FINAL TRAINING SUMMARY:\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"üß† Model: Hierarchical Reasoning Model (HRM)\")\n",
    "print(f\"üìà Instrument: {selected_instrument if 'selected_instrument' in locals() else 'Unknown'}\")\n",
    "print(f\"üíª Device: {DEVICE_TYPE.upper()} ({DEVICE})\")\n",
    "print(f\"‚è±Ô∏è Total Training Time: {total_time/3600:.1f} hours\")\n",
    "print(f\"üéØ Episodes Completed: {len(visualizer.training_history) if visualizer.training_history else 0}/{TRAINING_PARAMS['epochs']}\")\n",
    "\n",
    "if visualizer.training_history:\n",
    "    print(f\"üèÜ Best Performance: {visualizer.best_reward:.4f}\")\n",
    "    print(f\"üìà Final Performance: {visualizer.reward_history[-1]:.4f}\")\n",
    "    print(f\"üìâ Final Loss: {visualizer.loss_history[-1]:.4f}\")\n",
    "\n",
    "# Model specifications\n",
    "if 'model_summary' in locals():\n",
    "    print(f\"\\nüß† MODEL SPECIFICATIONS:\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"üìä Total Parameters: {model_summary['total_parameters']:,}\")\n",
    "    print(f\"üîÑ H-Cycles: {model_summary['H_cycles']} | L-Cycles: {model_summary['L_cycles']}\")\n",
    "    print(f\"üëÅÔ∏è H-Lookback: {model_summary['h_lookback_window']} | L-Lookback: {model_summary['l_lookback_window']}\")\n",
    "    print(f\"üî¢ Hidden Dimension: {model_summary['hidden_dimension']}\")\n",
    "\n",
    "# Performance evaluation\n",
    "if visualizer.training_history:\n",
    "    performance_level = \"üåü Excellent\" if visualizer.best_reward > 0.1 else \\\n",
    "                       \"‚úÖ Good\" if visualizer.best_reward > 0.05 else \\\n",
    "                       \"‚ö†Ô∏è Fair\" if visualizer.best_reward > 0.0 else \\\n",
    "                       \"‚ùå Needs Improvement\"\n",
    "    \n",
    "    print(f\"\\nüèÜ PERFORMANCE EVALUATION: {performance_level}\")\n",
    "\n",
    "# Next steps recommendations\n",
    "print(f\"\\nüöÄ RECOMMENDED NEXT STEPS:\")\n",
    "print(f\"=\" * 50)\n",
    "\n",
    "if visualizer.best_reward > 0.05:\n",
    "    print(f\"‚úÖ Model shows good performance:\")\n",
    "    print(f\"  ‚Ä¢ Deploy model for live testing\")\n",
    "    print(f\"  ‚Ä¢ Run extended evaluation on more instruments\")\n",
    "    print(f\"  ‚Ä¢ Consider ensemble with other models\")\n",
    "elif visualizer.best_reward > 0.0:\n",
    "    print(f\"‚ö†Ô∏è Model shows moderate performance:\")\n",
    "    print(f\"  ‚Ä¢ Continue training for more epochs\")\n",
    "    print(f\"  ‚Ä¢ Experiment with different hyperparameters\")\n",
    "    print(f\"  ‚Ä¢ Try different instruments or timeframes\")\n",
    "else:\n",
    "    print(f\"‚ùå Model needs improvement:\")\n",
    "    print(f\"  ‚Ä¢ Check data quality and preprocessing\")\n",
    "    print(f\"  ‚Ä¢ Adjust learning rates or architecture\")\n",
    "    print(f\"  ‚Ä¢ Consider curriculum learning approach\")\n",
    "\n",
    "print(f\"\\nüìÅ Export package available at: {export_dir if 'export_dir' in locals() else 'Not created'}\")\n",
    "\n",
    "# Additional recommendations\n",
    "print(f\"\\nüí° ADDITIONAL RECOMMENDATIONS:\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"üìä ‚Ä¢ Monitor model performance on different market conditions\")\n",
    "print(f\"üîÑ ‚Ä¢ Implement continuous learning for market adaptation\")\n",
    "print(f\"‚ö†Ô∏è ‚Ä¢ Add robust risk management and position sizing\")\n",
    "print(f\"üìà ‚Ä¢ Backtest on historical data before live deployment\")\n",
    "print(f\"üè≠ ‚Ä¢ Consider distributed training for larger datasets\")\n",
    "\n",
    "print(f\"\\nüéØ Training session completed successfully!\")\n",
    "print(f\"üìö Refer to the HRM research paper for implementation details\")\n",
    "print(f\"üîó Paper: 'Hierarchical Reasoning Model' by Guan Wang et al.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ Thank you for using the HRM Training Notebook!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "accelerator": "GPU",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
