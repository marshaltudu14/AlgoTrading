{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHZCW0Nz67OW"
      },
      "source": [
        "# HRM (Hierarchical Reasoning Model) Trading Training on Colab/Kaggle\n",
        "\n",
        "This notebook converts the HRM training pipeline to run on Google Colab or Kaggle notebooks with automatic TPU/GPU/CPU detection.\n",
        "\n",
        "## Overview\n",
        "- **HRM Architecture**: Brain-inspired hierarchical model with 27M parameters\n",
        "- **Training Approach**: Deep supervision with adaptive computation time\n",
        "- **Data Processing**: Automated pipeline from raw to training-ready data\n",
        "- **Hardware Support**: Automatic TPU/GPU/CPU detection with parallel training\n",
        "\n",
        "Based on the research paper: *Hierarchical Reasoning Model* by Guan Wang et al."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZormcV-r67OY"
      },
      "source": [
        "## 1. Setup Environment and Clone Repository\n",
        "\n",
        "Clone the private GitHub repository using the service token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyI6gCpQ67OY",
        "outputId": "4254b0dd-be78-4520-eb55-39fd9ceeae60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Running in Colab: False | Kaggle: False | TPU: False\n",
            "‚è≠Ô∏è Local environment detected. Skipping repository cloning.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Detect runtime environment\n",
        "def is_colab():\n",
        "    return \"COLAB_GPU\" in os.environ\n",
        "\n",
        "def is_kaggle():\n",
        "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
        "\n",
        "def check_tpu_availability():\n",
        "    try:\n",
        "        import requests\n",
        "        response = requests.get(\n",
        "            'http://metadata.google.internal/computeMetadata/v1/instance/attributes/accelerator-type',\n",
        "            headers={'Metadata-Flavor': 'Google'},\n",
        "            timeout=5\n",
        "        )\n",
        "        return 'tpu' in response.text.lower()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "in_cloud_env = is_colab() or is_kaggle()\n",
        "is_tpu_environment = check_tpu_availability() if in_cloud_env else False\n",
        "\n",
        "print(f\"üîç Running in Colab: {is_colab()} | Kaggle: {is_kaggle()} | TPU: {is_tpu_environment}\")\n",
        "\n",
        "# Repo details\n",
        "repo_url = \"https://{personal_access_token}@github.com/marshaltudu14/AlgoTrading.git\"\n",
        "repo_path = \"/content/AlgoTrading\"\n",
        "\n",
        "if in_cloud_env or is_tpu_environment:\n",
        "    # Clone repository only if running in cloud\n",
        "    if not os.path.exists(repo_path):\n",
        "        print(\"üì• Cloning AlgoTrading repository...\")\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"clone\", repo_url, repo_path],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Repository cloned successfully!\")\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to clone repository: {result.stderr}\")\n",
        "            raise Exception(\"Repository clone failed\")\n",
        "    else:\n",
        "        print(\"‚úÖ Repository already exists\")\n",
        "\n",
        "    # Change to repository directory\n",
        "    os.chdir(repo_path)\n",
        "    sys.path.append(repo_path)\n",
        "\n",
        "    print(f\"üìÅ Current directory: {os.getcwd()}\")\n",
        "    print(f\"üìã Repository contents:\")\n",
        "    for item in sorted(os.listdir('.')):\n",
        "        print(f\"  {item}\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è Local environment detected. Skipping repository cloning.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbKvHB-C67OZ"
      },
      "source": [
        "## 2. Install Dependencies\n",
        "\n",
        "Install all required packages including TPU support for Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1Q0AkMDj67Oa",
        "outputId": "a9f17757-708f-4422-96a2-dfe1cae870d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Running in Colab: False | Kaggle: False | TPU: False\n",
            "‚è≠Ô∏è Local environment detected. Skipping installation.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Detect runtime environment\n",
        "def is_colab():\n",
        "    return \"COLAB_GPU\" in os.environ\n",
        "\n",
        "def is_kaggle():\n",
        "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
        "\n",
        "def check_tpu_availability():\n",
        "    try:\n",
        "        import requests\n",
        "        response = requests.get(\n",
        "            'http://metadata.google.internal/computeMetadata/v1/instance/attributes/accelerator-type',\n",
        "            headers={'Metadata-Flavor': 'Google'},\n",
        "            timeout=5\n",
        "        )\n",
        "        return 'tpu' in response.text.lower()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "in_cloud_env = is_colab() or is_kaggle()\n",
        "is_tpu_environment = check_tpu_availability() if in_cloud_env else False\n",
        "\n",
        "print(f\"üîç Running in Colab: {is_colab()} | Kaggle: {is_kaggle()} | TPU: {is_tpu_environment}\")\n",
        "\n",
        "# Install only if in cloud (Colab/Kaggle/TPU)\n",
        "if in_cloud_env or is_tpu_environment:\n",
        "    print(\"üì¶ Installing base requirements...\")\n",
        "    !pip install -r requirements.txt\n",
        "\n",
        "    if is_tpu_environment:\n",
        "        print(\"üöÄ Installing TPU support...\")\n",
        "        !pip install torch_xla cloud-tpu-client\n",
        "\n",
        "    print(\"üìä Installing visualization libraries...\")\n",
        "    !pip install ipywidgets plotly kaleido\n",
        "\n",
        "    print(\"‚úÖ All dependencies installed successfully!\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è Local environment detected. Skipping installation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tafK7hNo67Oa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas_ta\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import get_distribution, DistributionNotFound\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.NaN = np.nan  # Fix missing alias\n",
        "import pandas_ta as ta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4RRkrVr67Ob",
        "outputId": "30f78b41-011b-44cb-f5fd-e1b1f4f0b6d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíª Local environment detected\n",
            "üìÅ Project path set to: c:\\AlgoTrading\n",
            "üìÇ Project exists: True\n",
            "‚úÖ Changed working directory to: c:\\AlgoTrading\n",
            "‚úÖ Added to Python path: c:\\AlgoTrading\n",
            "‚úÖ Found directory: src/\n",
            "‚úÖ Found directory: config/\n",
            "‚úÖ Found directory: data/\n",
            "\n",
            "üìã Project contents:\n",
            "  üìÅ .bmad-core/\n",
            "  üìÅ .claude/\n",
            "  üìÅ .gemini/\n",
            "  üìÅ .git/\n",
            "  üìÑ .gitignore\n",
            "  üìÅ .pytest_cache/\n",
            "  üìÑ .roomodes\n",
            "  üìÅ __pycache__/\n",
            "  üìÅ checkpoints/\n",
            "  üìÅ config/\n",
            "  üìÅ data/\n",
            "  üìÑ data_processing_pipeline.log\n",
            "  üìÅ docs/\n",
            "  üìÑ fetch_training_data.py\n",
            "  üìÑ fyers_docs.txt\n",
            "  üìÑ fyersApi.log\n",
            "  üìÑ fyersRequests.log\n",
            "  üìÅ HRM/\n",
            "  üìÑ hrm-research-paper.txt\n",
            "  üìÑ HRM_Training_Notebook.ipynb\n",
            "  üìÅ logs/\n",
            "  üìÅ models/\n",
            "  üìÅ reports/\n",
            "  üìÑ requirements.txt\n",
            "  üìÑ run_training.py\n",
            "  üìÅ src/\n",
            "  üìÅ temp/\n",
            "  üìÅ tests/\n",
            "  üìÅ venv/\n",
            "  üìÅ web-bundles/\n",
            "\n",
            "üéØ PROJECT_PATH variable set: c:\\AlgoTrading\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment and set project path\n",
        "def is_colab():\n",
        "    return \"COLAB_GPU\" in os.environ\n",
        "\n",
        "def is_kaggle():\n",
        "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
        "\n",
        "# Determine project path based on environment\n",
        "in_cloud_env = is_colab() or is_kaggle()\n",
        "\n",
        "if in_cloud_env:\n",
        "    # Cloud environment: Use cloned repository path\n",
        "    PROJECT_PATH = \"/content/AlgoTrading\"\n",
        "    print(f\"‚òÅÔ∏è Cloud environment detected\")\n",
        "else:\n",
        "    # Local environment: Use current directory or set custom path\n",
        "    # You can modify this path if your project is in a different location\n",
        "    PROJECT_PATH = os.getcwd()\n",
        "    print(f\"üíª Local environment detected\")\n",
        "\n",
        "# Convert to Path object for easier manipulation\n",
        "project_path = Path(PROJECT_PATH)\n",
        "\n",
        "print(f\"üìÅ Project path set to: {project_path}\")\n",
        "print(f\"üìÇ Project exists: {project_path.exists()}\")\n",
        "\n",
        "# Change to project directory if it exists\n",
        "if project_path.exists():\n",
        "    os.chdir(str(project_path))\n",
        "    print(f\"‚úÖ Changed working directory to: {os.getcwd()}\")\n",
        "\n",
        "    # Add project path to Python path for imports\n",
        "    if str(project_path) not in sys.path:\n",
        "        sys.path.insert(0, str(project_path))\n",
        "        print(f\"‚úÖ Added to Python path: {project_path}\")\n",
        "\n",
        "    # Verify key directories exist\n",
        "    key_dirs = ['src', 'config', 'data']\n",
        "    for dir_name in key_dirs:\n",
        "        dir_path = project_path / dir_name\n",
        "        if dir_path.exists():\n",
        "            print(f\"‚úÖ Found directory: {dir_name}/\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Directory not found: {dir_name}/\")\n",
        "\n",
        "    # List contents to verify\n",
        "    print(f\"\\nüìã Project contents:\")\n",
        "    for item in sorted(project_path.iterdir()):\n",
        "        if item.is_dir():\n",
        "            print(f\"  üìÅ {item.name}/\")\n",
        "        else:\n",
        "            print(f\"  üìÑ {item.name}\")\n",
        "else:\n",
        "    print(f\"‚ùå Project path does not exist: {project_path}\")\n",
        "    print(\"‚ö†Ô∏è You may need to modify PROJECT_PATH variable above\")\n",
        "\n",
        "# Store project path for use in other cells\n",
        "globals()['PROJECT_PATH'] = str(project_path)\n",
        "print(f\"\\nüéØ PROJECT_PATH variable set: {PROJECT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7csSXZW67Ob"
      },
      "source": [
        "## 3. Device Detection and Setup\n",
        "\n",
        "Automatically detect and configure the best available device (TPU > GPU > CPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjFR2Tk767Ob",
        "outputId": "a21e2102-0bd2-4948-e6c1-f17567cf55d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-10 04:43:38,316 - INFO - Using CPU for training (TPU/GPU not available)\n",
            "2025-09-10 04:43:38,317 - INFO - High-Performance Training Optimizer initialized\n",
            "2025-09-10 04:43:38,319 - INFO - Mixed Precision: Disabled\n",
            "2025-09-10 04:43:38,320 - INFO - Gradient Accumulation: 1x\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "MAXIMUM GPU UTILIZATION CONFIGURATION\n",
            "================================================================================\n",
            "Selected Device: cpu\n",
            "Device Type: CPU\n",
            "CPU Cores: 4\n",
            "CPU Threads: 4\n",
            "WARNING: Using CPU instead of GPU - Training will be significantly slower!\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "HIGH-PERFORMANCE TRAINING CONFIGURATION\n",
            "================================================================================\n",
            "Target Hardware: 15GB VRAM + 12GB RAM (Colab/Kaggle)\n",
            "Device: cpu\n",
            "Batch Size: 16 (8x larger for 15GB VRAM)\n",
            "Gradient Accumulation: 1x\n",
            "Effective Batch Size: 16 (massive training batches)\n",
            "Mixed Precision: Disabled\n",
            "DataLoader Workers: 2\n",
            "Pin Memory: Disabled\n",
            "Model Compilation: Disabled\n",
            "\n",
            "EXPECTED PERFORMANCE IMPROVEMENTS:\n",
            "‚Ä¢ 8x larger batch sizes = Better gradient estimates\n",
            "‚Ä¢ Mixed precision = 1.5-2x speed boost with Tensor Cores\n",
            "‚Ä¢ Optimized data loading = 2-3x faster data pipeline\n",
            "‚Ä¢ Memory management = Stable training with full VRAM usage\n",
            "‚Ä¢ Total expected speedup: 5-10x faster than default settings\n",
            "================================================================================\n",
            "\n",
            "üéØ HIGH-PERFORMANCE TRAINING CONFIGURATION:\n",
            "  Device: cpu\n",
            "  Batch Size: 16 (optimized for 15GB VRAM)\n",
            "  Gradient Accumulation: 1x\n",
            "  Effective Batch Size: 16\n",
            "  Mixed Precision: ‚ùå Disabled\n",
            "  DataLoader Workers: 2\n",
            "  Expected Speedup: 5-10x faster than default settings\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Import optimized components\n",
        "from src.utils.device_manager import get_device_manager\n",
        "from src.utils.training_optimizer import get_training_optimizer\n",
        "\n",
        "# Initialize device manager\n",
        "device_manager = get_device_manager()\n",
        "device = device_manager.get_device()\n",
        "device_type = device_manager.get_device_type()\n",
        "\n",
        "# Print detailed device information\n",
        "device_manager.print_device_summary()\n",
        "\n",
        "# Initialize high-performance training optimizer\n",
        "training_optimizer = get_training_optimizer()\n",
        "performance_config = training_optimizer.get_optimized_training_config()\n",
        "\n",
        "# Print performance optimization summary\n",
        "training_optimizer.print_performance_summary()\n",
        "\n",
        "# Store optimized configuration for training\n",
        "DEVICE = device\n",
        "DEVICE_TYPE = device_type\n",
        "BATCH_SIZE = performance_config['batch_size']  # Optimized for 15GB VRAM\n",
        "GRADIENT_ACCUMULATION = performance_config['gradient_accumulation_steps']\n",
        "EFFECTIVE_BATCH_SIZE = performance_config['effective_batch_size']\n",
        "MIXED_PRECISION = performance_config['mixed_precision']\n",
        "DATALOADER_CONFIG = performance_config['dataloader_config']\n",
        "\n",
        "print(f\"\\nüéØ HIGH-PERFORMANCE TRAINING CONFIGURATION:\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE} (optimized for 15GB VRAM)\")\n",
        "print(f\"  Gradient Accumulation: {GRADIENT_ACCUMULATION}x\")\n",
        "print(f\"  Effective Batch Size: {EFFECTIVE_BATCH_SIZE}\")\n",
        "print(f\"  Mixed Precision: {'‚úÖ Enabled' if MIXED_PRECISION else '‚ùå Disabled'}\")\n",
        "print(f\"  DataLoader Workers: {DATALOADER_CONFIG['num_workers']}\")\n",
        "print(f\"  Expected Speedup: 5-10x faster than default settings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPq4rb3U67Oc"
      },
      "source": [
        "## 4. Data Processing Pipeline\n",
        "\n",
        "Run the data processing pipeline to convert raw data into training-ready features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7HAiB8L67Oc",
        "outputId": "aef79683-938d-412e-db27-ed0b5cb9162e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-10 04:43:38,449 - INFO - DataProcessingPipeline initialized\n",
            "2025-09-10 04:43:38,452 - INFO - ================================================================================\n",
            "2025-09-10 04:43:38,453 - INFO - STARTING COMPLETE DATA PROCESSING PIPELINE\n",
            "2025-09-10 04:43:38,455 - INFO - ================================================================================\n",
            "2025-09-10 04:43:38,458 - INFO - STEP 1: FEATURE GENERATION\n",
            "2025-09-10 04:43:38,459 - INFO - ----------------------------------------\n",
            "2025-09-10 04:43:38,461 - INFO - Running feature generator...\n",
            "2025-09-10 04:43:38,470 - INFO - Found 5 CSV files to process\n",
            "2025-09-10 04:43:38,472 - INFO - Processing 5 files in parallel using 4 workers\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Initializing Data Processing Pipeline...\n",
            "================================================================================\n",
            "üìÇ Raw data path: data\\raw\n",
            "üìÇ Final data path: data\\final\n",
            "üìÑ Found 5 raw data files:\n",
            "  - Bank_Nifty_1.csv\n",
            "  - Bank_Nifty_10.csv\n",
            "  - Bank_Nifty_2.csv\n",
            "  - Bank_Nifty_3.csv\n",
            "  - Bank_Nifty_5.csv\n",
            "\n",
            "üöÄ Running Feature Generation Pipeline (Parallel Processing)...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-10 04:43:47,079 - INFO - Feature generator completed successfully\n",
            "2025-09-10 04:43:47,241 - INFO - Feature generation completed: 5 files\n",
            "2025-09-10 04:43:47,244 - INFO - Pipeline report saved: reports\\pipeline\\pipeline_report_1757459627.txt\n",
            "2025-09-10 04:43:47,245 - INFO - ================================================================================\n",
            "2025-09-10 04:43:47,246 - INFO - PIPELINE COMPLETED SUCCESSFULLY\n",
            "2025-09-10 04:43:47,248 - INFO - ================================================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Data Processing Pipeline Completed Successfully!\n",
            "================================================================================\n",
            "üìä Summary:\n",
            "  ‚Ä¢ Total files processed: 5\n",
            "  ‚Ä¢ Total rows processed: 3,204\n",
            "  ‚Ä¢ Processing time: 8.8 seconds\n",
            "  ‚Ä¢ Output directory: data\\final\n",
            "\n",
            "üìà Generated 0 feature files:\n",
            "\n",
            "üéØ Data processing complete. Ready for training!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Import data processing pipeline\n",
        "from src.data_processing.pipeline import DataProcessingPipeline\n",
        "\n",
        "print(\"üîÑ Initializing Data Processing Pipeline...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Initialize pipeline\n",
        "pipeline = DataProcessingPipeline()\n",
        "\n",
        "# Check if raw data exists\n",
        "raw_data_path = Path('data/raw')\n",
        "final_data_path = Path('data/final')\n",
        "\n",
        "print(f\"üìÇ Raw data path: {raw_data_path}\")\n",
        "print(f\"üìÇ Final data path: {final_data_path}\")\n",
        "\n",
        "# List raw data files\n",
        "if raw_data_path.exists():\n",
        "    raw_files = list(raw_data_path.glob(\"*.csv\"))\n",
        "    print(f\"üìÑ Found {len(raw_files)} raw data files:\")\n",
        "    for file in raw_files[:5]:  # Show first 5 files\n",
        "        print(f\"  - {file.name}\")\n",
        "    if len(raw_files) > 5:\n",
        "        print(f\"  ... and {len(raw_files) - 5} more files\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No raw data directory found\")\n",
        "\n",
        "# Run feature generation pipeline with parallel processing\n",
        "print(\"\\nüöÄ Running Feature Generation Pipeline (Parallel Processing)...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    result = pipeline.run_complete_pipeline(\n",
        "        input_dir=str(raw_data_path),\n",
        "        output_dir=str(final_data_path),\n",
        "        parallel=True,  # Enable parallel processing\n",
        "        max_workers=None  # Use default (CPU count)\n",
        "    )\n",
        "\n",
        "    if result['success']:\n",
        "        print(\"\\n‚úÖ Data Processing Pipeline Completed Successfully!\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"üìä Summary:\")\n",
        "        print(f\"  ‚Ä¢ Total files processed: {result.get('total_files_processed', 'Unknown')}\")\n",
        "        print(f\"  ‚Ä¢ Total rows processed: {result.get('total_rows_processed', 0):,}\")\n",
        "        print(f\"  ‚Ä¢ Processing time: {result.get('total_time_formatted', 'Unknown')}\")\n",
        "        print(f\"  ‚Ä¢ Output directory: {result.get('output_directory', 'Unknown')}\")\n",
        "\n",
        "        # List final data files\n",
        "        if final_data_path.exists():\n",
        "            final_files = list(final_data_path.glob(\"features_*.csv\"))\n",
        "            print(f\"\\nüìà Generated {len(final_files)} feature files:\")\n",
        "            for file in final_files:\n",
        "                print(f\"  - {file.name}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Data processing failed: {result.get('error', 'Unknown error')}\")\n",
        "        raise Exception(\"Data processing pipeline failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Pipeline execution failed: {str(e)}\")\n",
        "    # Continue anyway if some data exists\n",
        "    if final_data_path.exists() and list(final_data_path.glob(\"features_*.csv\")):\n",
        "        print(\"‚ö†Ô∏è Using existing processed data files\")\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "print(\"\\nüéØ Data processing complete. Ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drO9dpfp67Oc"
      },
      "source": [
        "## 5. HRM Training Configuration\n",
        "\n",
        "Configure the HRM training parameters and initialize the trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI_L3BLG67Od",
        "outputId": "247f7cbb-73e1-43b8-eed4-0911d6c4ae1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ HIGH-PERFORMANCE Training Parameters (Colab/Kaggle Optimized):\n",
            "  ‚Ä¢ epochs: 100\n",
            "  ‚Ä¢ save_frequency: 20\n",
            "  ‚Ä¢ log_frequency: 5\n",
            "  ‚Ä¢ validation_frequency: 10\n",
            "  ‚Ä¢ debug_mode: True\n",
            "  ‚Ä¢ config_path: config/hrm_config.yaml\n",
            "  ‚Ä¢ data_path: data/final\n",
            "  ‚Ä¢ memory_efficient: False\n",
            "  ‚Ä¢ high_performance: True\n",
            "  ‚Ä¢ mixed_precision: False\n",
            "  ‚Ä¢ gradient_accumulation_steps: 1\n",
            "  ‚Ä¢ dataloader_config:\n",
            "    - num_workers: 2\n",
            "    - pin_memory: False\n",
            "    - prefetch_factor: 2\n",
            "    - persistent_workers: True\n",
            "    - batch_size: 16\n",
            "  ‚Ä¢ compile_model: True\n",
            "  ‚Ä¢ cloud_optimized: True\n",
            "  ‚Ä¢ vram_target: 15GB\n",
            "  ‚Ä¢ ram_target: 12GB\n",
            "\n",
            "üì± Device Configuration:\n",
            "  ‚Ä¢ Device: cpu\n",
            "  ‚Ä¢ Device Type: cpu\n",
            "  ‚Ä¢ Batch Size: 16 (8x larger for 15GB VRAM)\n",
            "  ‚Ä¢ Effective Batch Size: 16 (with gradient accumulation)\n",
            "\n",
            "üèóÔ∏è HIGH-PERFORMANCE Training Architecture:\n",
            "  ‚Ä¢ Multi-Data per Epoch: All instruments trained in each epoch\n",
            "  ‚Ä¢ Memory Strategy: Full VRAM utilization (15GB)\n",
            "  ‚Ä¢ Mixed Precision: FP32\n",
            "  ‚Ä¢ DataLoader Workers: 2 (optimized for 12GB RAM)\n",
            "  ‚Ä¢ Expected Performance: 5-10x faster than default settings\n",
            "  ‚Ä¢ Flow: Large Batches ‚Üí Mixed Precision ‚Üí Gradient Accumulation ‚Üí Maximum Speed\n"
          ]
        }
      ],
      "source": [
        "# HIGH-PERFORMANCE Training parameters for Colab/Kaggle (15GB VRAM + 12GB RAM)\n",
        "TRAINING_PARAMS = {\n",
        "    'epochs': 100,  # More epochs due to faster training\n",
        "    'save_frequency': 20,  # Save checkpoint every 20 epochs\n",
        "    'log_frequency': 5,   # Log progress every 5 epochs\n",
        "    'validation_frequency': 10,  # Validate every 10 epochs\n",
        "    'debug_mode': True,   # Disable debug mode for speed\n",
        "    'config_path': 'config/hrm_config.yaml',\n",
        "    'data_path': 'data/final',\n",
        "    'memory_efficient': False,  # Disabled - we have plenty of VRAM now\n",
        "\n",
        "    # High-performance optimizations\n",
        "    'high_performance': True,\n",
        "    'mixed_precision': MIXED_PRECISION,\n",
        "    'gradient_accumulation_steps': GRADIENT_ACCUMULATION,\n",
        "    'dataloader_config': DATALOADER_CONFIG,\n",
        "    'compile_model': True,  # PyTorch 2.0+ optimization\n",
        "\n",
        "    # Optimized for cloud environments\n",
        "    'cloud_optimized': True,\n",
        "    'vram_target': '15GB',\n",
        "    'ram_target': '12GB'\n",
        "}\n",
        "\n",
        "print(f\"\\nüöÄ HIGH-PERFORMANCE Training Parameters (Colab/Kaggle Optimized):\")\n",
        "for key, value in TRAINING_PARAMS.items():\n",
        "    if isinstance(value, dict):\n",
        "        print(f\"  ‚Ä¢ {key}:\")\n",
        "        for subkey, subvalue in value.items():\n",
        "            print(f\"    - {subkey}: {subvalue}\")\n",
        "    else:\n",
        "        print(f\"  ‚Ä¢ {key}: {value}\")\n",
        "\n",
        "print(f\"\\nüì± Device Configuration:\")\n",
        "print(f\"  ‚Ä¢ Device: {DEVICE}\")\n",
        "print(f\"  ‚Ä¢ Device Type: {DEVICE_TYPE}\")\n",
        "print(f\"  ‚Ä¢ Batch Size: {BATCH_SIZE} (8x larger for 15GB VRAM)\")\n",
        "print(f\"  ‚Ä¢ Effective Batch Size: {EFFECTIVE_BATCH_SIZE} (with gradient accumulation)\")\n",
        "\n",
        "print(f\"\\nüèóÔ∏è HIGH-PERFORMANCE Training Architecture:\")\n",
        "print(f\"  ‚Ä¢ Multi-Data per Epoch: All instruments trained in each epoch\")\n",
        "print(f\"  ‚Ä¢ Memory Strategy: Full VRAM utilization (15GB)\")\n",
        "print(f\"  ‚Ä¢ Mixed Precision: {'FP16 Enabled' if MIXED_PRECISION else 'FP32'}\")\n",
        "print(f\"  ‚Ä¢ DataLoader Workers: {DATALOADER_CONFIG['num_workers']} (optimized for 12GB RAM)\")\n",
        "print(f\"  ‚Ä¢ Expected Performance: 5-10x faster than default settings\")\n",
        "print(f\"  ‚Ä¢ Flow: Large Batches ‚Üí Mixed Precision ‚Üí Gradient Accumulation ‚Üí Maximum Speed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjd5iQWo67Od"
      },
      "source": [
        "## 6. Initialize HRM Training Pipeline\n",
        "\n",
        "Initialize the HRM training pipeline with automatic device detection and parallel training support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjYYrGZy67Od",
        "outputId": "3bd681cd-1517-4964-a0ce-217c37e027ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Initializing HRM Training Pipeline (Multi-Data per Epoch)...\n",
            "================================================================================\n",
            "‚úÖ HRM Trainer initialized successfully!\n",
            "üìä Available instruments: 5\n",
            "üíæ Memory-efficient training: Loading one data file at a time\n",
            "\n",
            "üìà Available Training Instruments (All will be used per epoch):\n",
            "  1. Bank_Nifty_1\n",
            "  2. Bank_Nifty_10\n",
            "  3. Bank_Nifty_2\n",
            "  4. Bank_Nifty_3\n",
            "  5. Bank_Nifty_5\n",
            "\n",
            "üéØ Training Architecture:\n",
            "  ‚Ä¢ Per Epoch: All 5 instruments\n",
            "  ‚Ä¢ Per Instrument: Multiple episodes (1500 rows each)\n",
            "  ‚Ä¢ Memory Usage: One data file loaded at a time\n",
            "  ‚Ä¢ Training Flow: Instrument 1 ‚Üí All episodes ‚Üí Instrument 2 ‚Üí All episodes ‚Üí ... ‚Üí Epoch Complete\n",
            "\n",
            "üî• Ready to start multi-data HRM training!\n"
          ]
        }
      ],
      "source": [
        "# Import the HRM trainer directly instead of using run_training pipeline\n",
        "from src.models.hrm_trainer import HRMTrainer\n",
        "import logging\n",
        "\n",
        "# Setup enhanced logging for notebook\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler()  # Only console output for notebook\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"üöÄ Initializing HRM Training Pipeline (Multi-Data per Epoch)...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    # Initialize HRM trainer with memory-efficient multi-data training\n",
        "    trainer = HRMTrainer(\n",
        "        config_path=TRAINING_PARAMS['config_path'],\n",
        "        data_path=TRAINING_PARAMS['data_path'],\n",
        "        device=str(DEVICE),\n",
        "        debug_mode=TRAINING_PARAMS['debug_mode']  # Now False for multi-data\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ HRM Trainer initialized successfully!\")\n",
        "\n",
        "    # Check available instruments for multi-data training\n",
        "    from pathlib import Path\n",
        "    data_files = list(Path(TRAINING_PARAMS['data_path']).glob(\"features_*.parquet\"))\n",
        "    available_instruments = [f.stem.replace('features_', '') for f in data_files]\n",
        "\n",
        "    print(f\"üìä Available instruments: {len(available_instruments)}\")\n",
        "    print(\"üíæ Memory-efficient training: Loading one data file at a time\")\n",
        "\n",
        "    # List available instruments\n",
        "    if available_instruments:\n",
        "        print(\"\\nüìà Available Training Instruments (All will be used per epoch):\")\n",
        "        for i, instrument in enumerate(available_instruments, 1):\n",
        "            print(f\"  {i}. {instrument}\")\n",
        "\n",
        "        print(f\"\\nüéØ Training Architecture:\")\n",
        "        print(f\"  ‚Ä¢ Per Epoch: All {len(available_instruments)} instruments\")\n",
        "        print(f\"  ‚Ä¢ Per Instrument: Multiple episodes (1500 rows each)\")\n",
        "        print(f\"  ‚Ä¢ Memory Usage: One data file loaded at a time\")\n",
        "        print(f\"  ‚Ä¢ Training Flow: Instrument 1 ‚Üí All episodes ‚Üí Instrument 2 ‚Üí All episodes ‚Üí ... ‚Üí Epoch Complete\")\n",
        "    else:\n",
        "        raise ValueError(\"No instruments available for training\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to initialize HRM trainer: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "print(\"\\nüî• Ready to start multi-data HRM training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL9sgXV267Oe"
      },
      "source": [
        "## 7. Training Loss Visualization Setup\n",
        "\n",
        "Setup real-time visualization for training metrics and loss curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCeLUwHP67Oe",
        "outputId": "dbbc0ae8-d0c7-49d5-fc0e-1dde5ab0ba0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Training visualizer initialized!\n",
            "üìà Real-time plots will be updated during training\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "from IPython.display import display, clear_output\n",
        "import threading\n",
        "import time\n",
        "\n",
        "class HRMTrainingVisualizer:\n",
        "    \"\"\"Real-time training visualization for Jupyter notebooks\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.training_history = []\n",
        "        self.loss_history = []\n",
        "        self.reward_history = []\n",
        "        self.best_reward = float('-inf')\n",
        "\n",
        "        # Setup plots\n",
        "        self.fig = None\n",
        "        self.setup_plots()\n",
        "\n",
        "    def setup_plots(self):\n",
        "        \"\"\"Initialize the plotting framework\"\"\"\n",
        "        plt.style.use('default')\n",
        "        plt.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "    def update_metrics(self, episode, metrics):\n",
        "        \"\"\"Update training metrics\"\"\"\n",
        "        self.training_history.append({\n",
        "            'episode': episode,\n",
        "            'total_reward': metrics.get('total_reward', 0),\n",
        "            'avg_reward': metrics.get('avg_reward', 0),\n",
        "            'total_loss': metrics.get('total_loss', 0),\n",
        "            'avg_loss': metrics.get('avg_loss', 0),\n",
        "            'steps': metrics.get('steps', 0)\n",
        "        })\n",
        "\n",
        "        self.reward_history.append(metrics.get('avg_reward', 0))\n",
        "        self.loss_history.append(metrics.get('avg_loss', 0))\n",
        "\n",
        "        if metrics.get('avg_reward', 0) > self.best_reward:\n",
        "            self.best_reward = metrics.get('avg_reward', 0)\n",
        "\n",
        "    def create_training_plots(self):\n",
        "        \"\"\"Create comprehensive training plots\"\"\"\n",
        "        if len(self.training_history) < 2:\n",
        "            return\n",
        "\n",
        "        # Create subplots\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('Average Reward per Episode', 'Average Loss per Episode',\n",
        "                          'Total Reward Trend', 'Training Progress Summary'),\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "        )\n",
        "\n",
        "        episodes = [h['episode'] for h in self.training_history]\n",
        "\n",
        "        # Reward plot\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=episodes, y=self.reward_history,\n",
        "                      mode='lines+markers', name='Avg Reward',\n",
        "                      line=dict(color='green', width=2)),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # Loss plot\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=episodes, y=self.loss_history,\n",
        "                      mode='lines+markers', name='Avg Loss',\n",
        "                      line=dict(color='red', width=2)),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # Total reward trend\n",
        "        total_rewards = [h['total_reward'] for h in self.training_history]\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=episodes, y=total_rewards,\n",
        "                      mode='lines+markers', name='Total Reward',\n",
        "                      line=dict(color='blue', width=2)),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # Summary metrics\n",
        "        if len(self.reward_history) >= 10:\n",
        "            recent_avg_reward = np.mean(self.reward_history[-10:])\n",
        "            recent_avg_loss = np.mean(self.loss_history[-10:])\n",
        "        else:\n",
        "            recent_avg_reward = np.mean(self.reward_history)\n",
        "            recent_avg_loss = np.mean(self.loss_history)\n",
        "\n",
        "        # Create summary text\n",
        "        summary_text = f\"\"\"\n",
        "        üìä Training Summary (Last 10 Episodes)\n",
        "        ‚Ä¢ Best Reward: {self.best_reward:.4f}\n",
        "        ‚Ä¢ Recent Avg Reward: {recent_avg_reward:.4f}\n",
        "        ‚Ä¢ Recent Avg Loss: {recent_avg_loss:.4f}\n",
        "        ‚Ä¢ Total Episodes: {len(self.training_history)}\n",
        "        \"\"\"\n",
        "\n",
        "        fig.add_annotation(\n",
        "            text=summary_text,\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0.75, y=0.3, xanchor='left', yanchor='top',\n",
        "            showarrow=False,\n",
        "            font=dict(size=12, family=\"monospace\"),\n",
        "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
        "            bordercolor=\"black\",\n",
        "            borderwidth=1\n",
        "        )\n",
        "\n",
        "        fig.update_layout(\n",
        "            height=800,\n",
        "            title_text=\"HRM Training Progress - Real-time Monitoring\",\n",
        "            showlegend=True\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def display_current_metrics(self):\n",
        "        \"\"\"Display current training metrics\"\"\"\n",
        "        if not self.training_history:\n",
        "            return\n",
        "\n",
        "        latest = self.training_history[-1]\n",
        "\n",
        "        print(f\"\\nüìà Episode {latest['episode']} Results:\")\n",
        "        print(f\"  üéØ Average Reward: {latest['avg_reward']:.4f}\")\n",
        "        print(f\"  üìâ Average Loss: {latest['avg_loss']:.4f}\")\n",
        "        print(f\"  üéÆ Steps Completed: {latest['steps']}\")\n",
        "        print(f\"  üèÜ Best Reward So Far: {self.best_reward:.4f}\")\n",
        "\n",
        "        if latest['avg_reward'] == self.best_reward:\n",
        "            print(\"  üåü NEW BEST PERFORMANCE! üåü\")\n",
        "\n",
        "# Initialize visualizer\n",
        "visualizer = HRMTrainingVisualizer()\n",
        "print(\"üìä Training visualizer initialized!\")\n",
        "print(\"üìà Real-time plots will be updated during training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFUhslqh67Oe"
      },
      "source": [
        "## 8. Start HRM Training\n",
        "\n",
        "Begin the HRM training process with real-time monitoring and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626,
          "referenced_widgets": [
            "175501aabf314945bb3f13c4dd99f91d",
            "0c1dd137baba4988bcfcb01cd862ea01",
            "22646573d04e4e5d862b09ce9695f0d3",
            "2e285078e3a84456958ec6d5e96f2174",
            "7b4cbd69439e4cf6afe49d374339aa97",
            "6b913efc14964d0aba6053a40efa4187",
            "121d893bdf974d0facf05c783f226d04",
            "83f621d082474f04bb008877dbd0a515",
            "f6ab6938593b4ceaaa5c48797a07c8cb",
            "836be5d8885041afb7e4a888dc0bbcd9",
            "4d94dce272af4e40b29bdb9c090e572f"
          ]
        },
        "id": "IVFhQlD167Oe",
        "outputId": "2f3aa3c8-1ba0-40ea-d2e5-d8e1753a8b53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting HRM Multi-Data Training Process...\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49b3fd25c5dc459c9d3aa3fe45c74ab3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(IntProgress(value=0, bar_style='info', description='Training:', style=ProgressStyle(bar_color='‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Setting up multi-data training for 5 instruments...\n",
            "üìã Instruments: ['Bank_Nifty_1', 'Bank_Nifty_10', 'Bank_Nifty_2', 'Bank_Nifty_3', 'Bank_Nifty_5']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-10 04:43:48,586 - INFO - Looking for data for symbol: Bank_Nifty_1 in directory: data/final\n",
            "2025-09-10 04:43:48,588 - INFO - Checking file: data/final\\features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:48,589 - INFO - File exists: data/final\\features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:48,626 - INFO - Loaded final data for Bank_Nifty_1: 426 rows from features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:48,665 - INFO - Looking for data for symbol: Bank_Nifty_10 in directory: data/final\n",
            "2025-09-10 04:43:48,666 - INFO - Checking file: data/final\\features_Bank_Nifty_10.parquet\n",
            "2025-09-10 04:43:48,670 - INFO - File exists: data/final\\features_Bank_Nifty_10.parquet\n",
            "2025-09-10 04:43:48,697 - INFO - Loaded final data for Bank_Nifty_10: 763 rows from features_Bank_Nifty_10.parquet\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Calculating training steps (memory efficient)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-10 04:43:48,719 - INFO - Looking for data for symbol: Bank_Nifty_2 in directory: data/final\n",
            "2025-09-10 04:43:48,723 - INFO - Checking file: data/final\\features_Bank_Nifty_2.parquet\n",
            "2025-09-10 04:43:48,726 - INFO - File exists: data/final\\features_Bank_Nifty_2.parquet\n",
            "2025-09-10 04:43:48,756 - INFO - Loaded final data for Bank_Nifty_2: 613 rows from features_Bank_Nifty_2.parquet\n",
            "2025-09-10 04:43:48,785 - INFO - Looking for data for symbol: Bank_Nifty_3 in directory: data/final\n",
            "2025-09-10 04:43:48,787 - INFO - Checking file: data/final\\features_Bank_Nifty_3.parquet\n",
            "2025-09-10 04:43:48,789 - INFO - File exists: data/final\\features_Bank_Nifty_3.parquet\n",
            "2025-09-10 04:43:48,820 - INFO - Loaded final data for Bank_Nifty_3: 676 rows from features_Bank_Nifty_3.parquet\n",
            "2025-09-10 04:43:48,845 - INFO - Looking for data for symbol: Bank_Nifty_5 in directory: data/final\n",
            "2025-09-10 04:43:48,848 - INFO - Checking file: data/final\\features_Bank_Nifty_5.parquet\n",
            "2025-09-10 04:43:48,849 - INFO - File exists: data/final\\features_Bank_Nifty_5.parquet\n",
            "2025-09-10 04:43:48,877 - INFO - Loaded final data for Bank_Nifty_5: 726 rows from features_Bank_Nifty_5.parquet\n",
            "2025-09-10 04:43:48,893 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:48,904 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:48,906 - INFO - Debug mode with 5 parallel environments: Detailed logs will show parallel instrument names\n",
            "2025-09-10 04:43:48,907 - INFO - Parallel environments enabled: 5 environments on CPU\n",
            "2025-09-10 04:43:48,911 - INFO - Initializing parallel environment manager with {'gpu_tpu': 16, 'cpu': 5} environments\n",
            "2025-09-10 04:43:48,928 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:48,958 - INFO - Loaded HRM configuration from config/hrm_config.yaml\n",
            "2025-09-10 04:43:48,970 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,025 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,047 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà Training Progress: 100 epochs √ó 5 total episodes = 500 steps\n",
            "üíæ Memory-efficient: One data file loaded at a time\n",
            "üöÄ Starting multi-data training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-10 04:43:49,091 - INFO - Parsed symbol: Bank_Nifty, timeframe: 1\n",
            "2025-09-10 04:43:49,101 - INFO - Resolved market context: Bank_Nifty (ID: 0), 1 (ID: 0)\n",
            "2025-09-10 04:43:49,116 - INFO - Created environment for Bank_Nifty_1\n",
            "2025-09-10 04:43:49,144 - INFO - Loaded HRM configuration from config/hrm_config.yaml\n",
            "2025-09-10 04:43:49,152 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,173 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,202 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,213 - INFO - Parsed symbol: Bank_Nifty, timeframe: 10\n",
            "2025-09-10 04:43:49,214 - INFO - Resolved market context: Bank_Nifty (ID: 0), 10 (ID: 4)\n",
            "2025-09-10 04:43:49,214 - INFO - Created environment for Bank_Nifty_10\n",
            "2025-09-10 04:43:49,237 - INFO - Loaded HRM configuration from config/hrm_config.yaml\n",
            "2025-09-10 04:43:49,246 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,258 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,280 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,285 - INFO - Parsed symbol: Bank_Nifty, timeframe: 2\n",
            "2025-09-10 04:43:49,288 - INFO - Resolved market context: Bank_Nifty (ID: 0), 2 (ID: 1)\n",
            "2025-09-10 04:43:49,289 - INFO - Created environment for Bank_Nifty_2\n",
            "2025-09-10 04:43:49,306 - INFO - Loaded HRM configuration from config/hrm_config.yaml\n",
            "2025-09-10 04:43:49,317 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,328 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,351 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,355 - INFO - Parsed symbol: Bank_Nifty, timeframe: 3\n",
            "2025-09-10 04:43:49,356 - INFO - Resolved market context: Bank_Nifty (ID: 0), 3 (ID: 2)\n",
            "2025-09-10 04:43:49,357 - INFO - Created environment for Bank_Nifty_3\n",
            "2025-09-10 04:43:49,375 - INFO - Loaded HRM configuration from config/hrm_config.yaml\n",
            "2025-09-10 04:43:49,387 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,399 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,424 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,428 - INFO - Parsed symbol: Bank_Nifty, timeframe: 5\n",
            "2025-09-10 04:43:49,430 - INFO - Resolved market context: Bank_Nifty (ID: 0), 5 (ID: 3)\n",
            "2025-09-10 04:43:49,432 - INFO - Created environment for Bank_Nifty_5\n",
            "2025-09-10 04:43:49,435 - INFO - Parallel Environment Manager initialized with 5 environments on CPU\n",
            "2025-09-10 04:43:49,437 - INFO - ParallelGroup Training: 5 instruments - Bank_Nifty_1, Bank_Nifty_10, Bank_Nifty_2, Bank_Nifty_3, Bank_Nifty_5\n",
            "2025-09-10 04:43:49,438 - INFO - Looking for data for symbol: Bank_Nifty_1 in directory: data/final\n",
            "2025-09-10 04:43:49,439 - INFO - Checking file: data/final\\features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:49,440 - INFO - File exists: data/final\\features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:49,467 - INFO - Loaded final data for Bank_Nifty_1: 426 rows from features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:49,486 - INFO - Loaded HRM configuration from config/hrm_config.yaml\n",
            "2025-09-10 04:43:49,496 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,508 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,532 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:49,535 - INFO - Parsed symbol: Bank_Nifty, timeframe: 1\n",
            "2025-09-10 04:43:49,536 - INFO - Resolved market context: Bank_Nifty (ID: 0), 1 (ID: 0)\n",
            "2025-09-10 04:43:49,541 - INFO - Episode data: rows 0 to 425 (426 rows)\n",
            "2025-09-10 04:43:49,542 - INFO - Time range: 2025-09-08 14:39:00 to 2025-09-09 15:29:00\n",
            "2025-09-10 04:43:52,118 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:52,232 - INFO - HRM Agent initialized: H-lookback=100, L-lookback=30, Hidden=1024, H-cycles=8, L-cycles=20\n",
            "2025-09-10 04:43:52,235 - INFO - HRM Trading Agent initialized with 266243180 parameters\n",
            "2025-09-10 04:43:55,096 - INFO - Training setup completed for 5 instruments\n",
            "2025-09-10 04:43:55,097 - INFO - Parallel environments: ENABLED\n",
            "2025-09-10 04:43:55,098 - INFO - Memory-efficient mode: Data files loaded one at a time during training\n",
            "2025-09-10 04:43:55,099 - INFO - Starting HRM training for 100 epochs\n",
            "2025-09-10 04:43:55,100 - INFO - Training on 5 instruments per epoch\n",
            "2025-09-10 04:43:55,102 - INFO - Debug mode: ON\n",
            "2025-09-10 04:43:55,102 - INFO - Device: cpu\n",
            "2025-09-10 04:43:55,104 - INFO - Calculating total episodes (loading data headers only)...\n",
            "2025-09-10 04:43:55,105 - INFO - Looking for data for symbol: Bank_Nifty_1 in directory: data/final\n",
            "2025-09-10 04:43:55,106 - INFO - Checking file: data/final\\features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:55,107 - INFO - File exists: data/final\\features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:55,137 - INFO - Loaded final data for Bank_Nifty_1: 426 rows from features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:55,139 - INFO - Looking for data for symbol: Bank_Nifty_10 in directory: data/final\n",
            "2025-09-10 04:43:55,141 - INFO - Checking file: data/final\\features_Bank_Nifty_10.parquet\n",
            "2025-09-10 04:43:55,141 - INFO - File exists: data/final\\features_Bank_Nifty_10.parquet\n",
            "2025-09-10 04:43:55,167 - INFO - Loaded final data for Bank_Nifty_10: 763 rows from features_Bank_Nifty_10.parquet\n",
            "2025-09-10 04:43:55,168 - INFO - Looking for data for symbol: Bank_Nifty_2 in directory: data/final\n",
            "2025-09-10 04:43:55,168 - INFO - Checking file: data/final\\features_Bank_Nifty_2.parquet\n",
            "2025-09-10 04:43:55,170 - INFO - File exists: data/final\\features_Bank_Nifty_2.parquet\n",
            "2025-09-10 04:43:55,193 - INFO - Loaded final data for Bank_Nifty_2: 613 rows from features_Bank_Nifty_2.parquet\n",
            "2025-09-10 04:43:55,194 - INFO - Looking for data for symbol: Bank_Nifty_3 in directory: data/final\n",
            "2025-09-10 04:43:55,195 - INFO - Checking file: data/final\\features_Bank_Nifty_3.parquet\n",
            "2025-09-10 04:43:55,196 - INFO - File exists: data/final\\features_Bank_Nifty_3.parquet\n",
            "2025-09-10 04:43:55,217 - INFO - Loaded final data for Bank_Nifty_3: 676 rows from features_Bank_Nifty_3.parquet\n",
            "2025-09-10 04:43:55,219 - INFO - Looking for data for symbol: Bank_Nifty_5 in directory: data/final\n",
            "2025-09-10 04:43:55,221 - INFO - Checking file: data/final\\features_Bank_Nifty_5.parquet\n",
            "2025-09-10 04:43:55,223 - INFO - File exists: data/final\\features_Bank_Nifty_5.parquet\n",
            "2025-09-10 04:43:55,247 - INFO - Loaded final data for Bank_Nifty_5: 726 rows from features_Bank_Nifty_5.parquet\n",
            "2025-09-10 04:43:55,248 - INFO - Total training steps: 100 epochs √ó 5 episodes = 500 steps\n",
            "2025-09-10 04:43:55,250 - INFO - Memory-efficient training: Loading only one data file at a time\n",
            "2025-09-10 04:43:55,251 - INFO - Starting Epoch 1/100\n",
            "2025-09-10 04:43:55,252 - INFO - üöÄ TRUE BATCH MODE: Loading and processing ALL datasets simultaneously for maximum parallelization\n",
            "2025-09-10 04:43:55,253 - INFO - Loading up to 4 datasets simultaneously (Target: 8.0GB)\n",
            "2025-09-10 04:43:55,254 - INFO - Looking for data for symbol: Bank_Nifty_1 in directory: data/final\n",
            "2025-09-10 04:43:55,258 - INFO - Checking file: data/final\\features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:55,261 - INFO - File exists: data/final\\features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:55,284 - INFO - Loaded final data for Bank_Nifty_1: 426 rows from features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:55,292 - INFO - ‚úì Batch loaded Bank_Nifty_1: 0.00GB (Total: 0.00GB)\n",
            "2025-09-10 04:43:55,303 - INFO - Looking for data for symbol: Bank_Nifty_10 in directory: data/final\n",
            "2025-09-10 04:43:55,315 - INFO - Checking file: data/final\\features_Bank_Nifty_10.parquet\n",
            "2025-09-10 04:43:55,324 - INFO - File exists: data/final\\features_Bank_Nifty_10.parquet\n",
            "2025-09-10 04:43:55,360 - INFO - Loaded final data for Bank_Nifty_10: 763 rows from features_Bank_Nifty_10.parquet\n",
            "2025-09-10 04:43:55,367 - INFO - ‚úì Batch loaded Bank_Nifty_10: 0.00GB (Total: 0.00GB)\n",
            "2025-09-10 04:43:55,368 - INFO - Looking for data for symbol: Bank_Nifty_2 in directory: data/final\n",
            "2025-09-10 04:43:55,369 - INFO - Checking file: data/final\\features_Bank_Nifty_2.parquet\n",
            "2025-09-10 04:43:55,371 - INFO - File exists: data/final\\features_Bank_Nifty_2.parquet\n",
            "2025-09-10 04:43:55,396 - INFO - Loaded final data for Bank_Nifty_2: 613 rows from features_Bank_Nifty_2.parquet\n",
            "2025-09-10 04:43:55,402 - INFO - ‚úì Batch loaded Bank_Nifty_2: 0.00GB (Total: 0.00GB)\n",
            "2025-09-10 04:43:55,406 - INFO - Looking for data for symbol: Bank_Nifty_3 in directory: data/final\n",
            "2025-09-10 04:43:55,408 - INFO - Checking file: data/final\\features_Bank_Nifty_3.parquet\n",
            "2025-09-10 04:43:55,409 - INFO - File exists: data/final\\features_Bank_Nifty_3.parquet\n",
            "2025-09-10 04:43:55,433 - INFO - Loaded final data for Bank_Nifty_3: 676 rows from features_Bank_Nifty_3.parquet\n",
            "2025-09-10 04:43:55,436 - INFO - ‚úì Batch loaded Bank_Nifty_3: 0.00GB (Total: 0.00GB)\n",
            "2025-09-10 04:43:55,437 - INFO - üîÑ Configuring parallel environments for 4 datasets\n",
            "2025-09-10 04:43:55,438 - INFO - Creating batched episodes: 4 total episodes across 4 datasets\n",
            "2025-09-10 04:43:55,464 - INFO - Created 1 batches with avg 4.0 episodes per batch\n",
            "2025-09-10 04:43:55,467 - INFO - Processing batch 1/1 with 4 episodes\n",
            "2025-09-10 04:43:55,485 - INFO - Loaded HRM configuration from config/hrm_config.yaml\n",
            "2025-09-10 04:43:55,502 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:55,520 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:55,544 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:55,548 - INFO - Parsed symbol: Bank_Nifty, timeframe: 1\n",
            "2025-09-10 04:43:55,550 - INFO - Resolved market context: Bank_Nifty (ID: 0), 1 (ID: 0)\n",
            "2025-09-10 04:43:55,551 - WARNING - Insufficient data for streaming mode, loading all FINAL data for Bank_Nifty_1\n",
            "2025-09-10 04:43:55,553 - INFO - Looking for data for symbol: Bank_Nifty_1 in directory: data/final\n",
            "2025-09-10 04:43:55,554 - INFO - Checking file: data/final\\features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:55,556 - INFO - File exists: data/final\\features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:55,579 - INFO - Loaded final data for Bank_Nifty_1: 426 rows from features_Bank_Nifty_1.parquet\n",
            "2025-09-10 04:43:55,581 - INFO - üìä Data Feeding Strategy Manager initialized\n",
            "2025-09-10 04:43:55,582 - INFO -    Total data length: 426\n",
            "2025-09-10 04:43:55,584 - INFO -    Data segments: 0\n",
            "2025-09-10 04:43:55,585 - INFO -    Market regimes identified: 0\n",
            "2025-09-10 04:43:55,586 - INFO - üéØ Data feeding strategy initialized: curriculum\n",
            "2025-09-10 04:43:55,601 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:55,603 - INFO - üîß Universal Model: Excluded raw OHLC prices, using 53 derived features\n",
            "2025-09-10 04:43:55,605 - INFO - üìä Observation space set: 53 features per step, total dim: 5306\n",
            "2025-09-10 04:43:55,606 - INFO - üìà Hierarchical lookback - High: 100, Low: 15\n",
            "2025-09-10 04:43:55,609 - INFO - üéØ Feature columns: ['open', 'high', 'low', 'close', 'datetime_epoch', 'sma_5', 'sma_10', 'sma_20', 'sma_50', 'sma_100']...\n",
            "2025-09-10 04:43:55,610 - INFO - No data segments available for curriculum episode, using random episode\n",
            "2025-09-10 04:43:58,214 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:58,330 - INFO - HRM Agent initialized: H-lookback=100, L-lookback=30, Hidden=1024, H-cycles=8, L-cycles=20\n",
            "2025-09-10 04:43:58,332 - INFO - HRM Trading Agent initialized with 266243180 parameters\n",
            "2025-09-10 04:43:58,333 - INFO - HRM Trading Environment reset completed\n",
            "2025-09-10 04:43:58,350 - INFO - Loaded HRM configuration from config/hrm_config.yaml\n",
            "2025-09-10 04:43:58,360 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:58,372 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:58,396 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:58,399 - INFO - Parsed symbol: Bank_Nifty, timeframe: 10\n",
            "2025-09-10 04:43:58,401 - INFO - Resolved market context: Bank_Nifty (ID: 0), 10 (ID: 4)\n",
            "2025-09-10 04:43:58,403 - WARNING - Insufficient data for streaming mode, loading all FINAL data for Bank_Nifty_10\n",
            "2025-09-10 04:43:58,406 - INFO - Looking for data for symbol: Bank_Nifty_10 in directory: data/final\n",
            "2025-09-10 04:43:58,407 - INFO - Checking file: data/final\\features_Bank_Nifty_10.parquet\n",
            "2025-09-10 04:43:58,409 - INFO - File exists: data/final\\features_Bank_Nifty_10.parquet\n",
            "2025-09-10 04:43:58,437 - INFO - Loaded final data for Bank_Nifty_10: 763 rows from features_Bank_Nifty_10.parquet\n",
            "2025-09-10 04:43:58,438 - INFO - üìä Data Feeding Strategy Manager initialized\n",
            "2025-09-10 04:43:58,440 - INFO -    Total data length: 763\n",
            "2025-09-10 04:43:58,442 - INFO -    Data segments: 0\n",
            "2025-09-10 04:43:58,443 - INFO -    Market regimes identified: 0\n",
            "2025-09-10 04:43:58,445 - INFO - üéØ Data feeding strategy initialized: curriculum\n",
            "2025-09-10 04:43:58,461 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:43:58,462 - INFO - üîß Universal Model: Excluded raw OHLC prices, using 53 derived features\n",
            "2025-09-10 04:43:58,463 - INFO - üìä Observation space set: 53 features per step, total dim: 5306\n",
            "2025-09-10 04:43:58,464 - INFO - üìà Hierarchical lookback - High: 100, Low: 15\n",
            "2025-09-10 04:43:58,465 - INFO - üéØ Feature columns: ['open', 'high', 'low', 'close', 'datetime_epoch', 'sma_5', 'sma_10', 'sma_20', 'sma_50', 'sma_100']...\n",
            "2025-09-10 04:43:58,466 - INFO - No data segments available for curriculum episode, using random episode\n",
            "2025-09-10 04:44:01,144 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:44:01,254 - INFO - HRM Agent initialized: H-lookback=100, L-lookback=30, Hidden=1024, H-cycles=8, L-cycles=20\n",
            "2025-09-10 04:44:01,257 - INFO - HRM Trading Agent initialized with 266243180 parameters\n",
            "2025-09-10 04:44:01,258 - INFO - HRM Trading Environment reset completed\n",
            "2025-09-10 04:44:01,275 - INFO - Loaded HRM configuration from config/hrm_config.yaml\n",
            "2025-09-10 04:44:01,287 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:44:01,299 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:44:01,319 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:44:01,322 - INFO - Parsed symbol: Bank_Nifty, timeframe: 2\n",
            "2025-09-10 04:44:01,325 - INFO - Resolved market context: Bank_Nifty (ID: 0), 2 (ID: 1)\n",
            "2025-09-10 04:44:01,326 - WARNING - Insufficient data for streaming mode, loading all FINAL data for Bank_Nifty_2\n",
            "2025-09-10 04:44:01,329 - INFO - Looking for data for symbol: Bank_Nifty_2 in directory: data/final\n",
            "2025-09-10 04:44:01,331 - INFO - Checking file: data/final\\features_Bank_Nifty_2.parquet\n",
            "2025-09-10 04:44:01,332 - INFO - File exists: data/final\\features_Bank_Nifty_2.parquet\n",
            "2025-09-10 04:44:01,363 - INFO - Loaded final data for Bank_Nifty_2: 613 rows from features_Bank_Nifty_2.parquet\n",
            "2025-09-10 04:44:01,365 - INFO - üìä Data Feeding Strategy Manager initialized\n",
            "2025-09-10 04:44:01,366 - INFO -    Total data length: 613\n",
            "2025-09-10 04:44:01,368 - INFO -    Data segments: 0\n",
            "2025-09-10 04:44:01,368 - INFO -    Market regimes identified: 0\n",
            "2025-09-10 04:44:01,369 - INFO - üéØ Data feeding strategy initialized: curriculum\n",
            "2025-09-10 04:44:01,381 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:44:01,383 - INFO - üîß Universal Model: Excluded raw OHLC prices, using 53 derived features\n",
            "2025-09-10 04:44:01,384 - INFO - üìä Observation space set: 53 features per step, total dim: 5306\n",
            "2025-09-10 04:44:01,385 - INFO - üìà Hierarchical lookback - High: 100, Low: 15\n",
            "2025-09-10 04:44:01,386 - INFO - üéØ Feature columns: ['open', 'high', 'low', 'close', 'datetime_epoch', 'sma_5', 'sma_10', 'sma_20', 'sma_50', 'sma_100']...\n",
            "2025-09-10 04:44:01,387 - INFO - No data segments available for curriculum episode, using random episode\n",
            "2025-09-10 04:44:04,063 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:44:04,180 - INFO - HRM Agent initialized: H-lookback=100, L-lookback=30, Hidden=1024, H-cycles=8, L-cycles=20\n",
            "2025-09-10 04:44:04,183 - INFO - HRM Trading Agent initialized with 266243180 parameters\n",
            "2025-09-10 04:44:04,184 - INFO - HRM Trading Environment reset completed\n",
            "2025-09-10 04:44:04,201 - INFO - Loaded HRM configuration from config/hrm_config.yaml\n",
            "2025-09-10 04:44:04,212 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:44:04,223 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:44:04,246 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:44:04,250 - INFO - Parsed symbol: Bank_Nifty, timeframe: 3\n",
            "2025-09-10 04:44:04,251 - INFO - Resolved market context: Bank_Nifty (ID: 0), 3 (ID: 2)\n",
            "2025-09-10 04:44:04,252 - WARNING - Insufficient data for streaming mode, loading all FINAL data for Bank_Nifty_3\n",
            "2025-09-10 04:44:04,254 - INFO - Looking for data for symbol: Bank_Nifty_3 in directory: data/final\n",
            "2025-09-10 04:44:04,256 - INFO - Checking file: data/final\\features_Bank_Nifty_3.parquet\n",
            "2025-09-10 04:44:04,257 - INFO - File exists: data/final\\features_Bank_Nifty_3.parquet\n",
            "2025-09-10 04:44:04,305 - INFO - Loaded final data for Bank_Nifty_3: 676 rows from features_Bank_Nifty_3.parquet\n",
            "2025-09-10 04:44:04,306 - INFO - üìä Data Feeding Strategy Manager initialized\n",
            "2025-09-10 04:44:04,307 - INFO -    Total data length: 676\n",
            "2025-09-10 04:44:04,308 - INFO -    Data segments: 0\n",
            "2025-09-10 04:44:04,310 - INFO -    Market regimes identified: 0\n",
            "2025-09-10 04:44:04,311 - INFO - üéØ Data feeding strategy initialized: curriculum\n",
            "2025-09-10 04:44:04,323 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:44:04,324 - INFO - üîß Universal Model: Excluded raw OHLC prices, using 53 derived features\n",
            "2025-09-10 04:44:04,327 - INFO - üìä Observation space set: 53 features per step, total dim: 5306\n",
            "2025-09-10 04:44:04,328 - INFO - üìà Hierarchical lookback - High: 100, Low: 15\n",
            "2025-09-10 04:44:04,329 - INFO - üéØ Feature columns: ['open', 'high', 'low', 'close', 'datetime_epoch', 'sma_5', 'sma_10', 'sma_20', 'sma_50', 'sma_100']...\n",
            "2025-09-10 04:44:04,333 - INFO - No data segments available for curriculum episode, using random episode\n",
            "2025-09-10 04:44:07,071 - INFO - ‚úÖ Configuration loaded from config/settings.yaml\n",
            "2025-09-10 04:44:07,192 - INFO - HRM Agent initialized: H-lookback=100, L-lookback=30, Hidden=1024, H-cycles=8, L-cycles=20\n",
            "2025-09-10 04:44:07,194 - INFO - HRM Trading Agent initialized with 266243180 parameters\n",
            "2025-09-10 04:44:07,196 - INFO - HRM Trading Environment reset completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ö†Ô∏è Training interrupted by user\n",
            "\n",
            "‚úÖ Multi-data training completed successfully!\n",
            "üéØ Trained on 5 instruments per epoch\n",
            "üíæ Memory-efficient: One data file loaded at a time\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "print(\"üöÄ Starting HRM Multi-Data Training Process...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create training progress widgets for multi-data training\n",
        "progress_bar = widgets.IntProgress(\n",
        "    value=0,\n",
        "    min=0,\n",
        "    max=100,  # Will be updated dynamically\n",
        "    description='Training:',\n",
        "    bar_style='info',\n",
        "    style={'bar_color': 'green'},\n",
        "    orientation='horizontal'\n",
        ")\n",
        "\n",
        "status_text = widgets.HTML(value=\"<b>Initializing multi-data training...</b>\")\n",
        "metrics_text = widgets.HTML(value=\"\")\n",
        "\n",
        "display(widgets.VBox([progress_bar, status_text, metrics_text]))\n",
        "\n",
        "# Custom training loop with visualization for multi-data\n",
        "class HRMTrainingMonitor:\n",
        "    def __init__(self, trainer, visualizer):\n",
        "        self.trainer = trainer\n",
        "        self.visualizer = visualizer\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def run_training_with_monitoring(self, epochs, available_instruments):\n",
        "        \"\"\"Run multi-data training with real-time monitoring\"\"\"\n",
        "\n",
        "        # Setup training with complete initialization for multi-data\n",
        "        print(f\"üîß Setting up multi-data training for {len(available_instruments)} instruments...\")\n",
        "        print(f\"üìã Instruments: {available_instruments}\")\n",
        "\n",
        "        status_text.value = f\"<b>üéØ Multi-Data Training: {len(available_instruments)} instruments √ó {epochs} epochs</b>\"\n",
        "\n",
        "        # Calculate total steps for progress bar (memory efficient)\n",
        "        total_steps = 0\n",
        "        print(\"üìä Calculating training steps (memory efficient)...\")\n",
        "        for symbol in available_instruments:\n",
        "            # Quick data length check (memory efficient)\n",
        "            from src.utils.data_loader import DataLoader\n",
        "            temp_loader = DataLoader(final_data_dir=TRAINING_PARAMS['data_path'])\n",
        "            temp_data = temp_loader.load_final_data_for_symbol(symbol)\n",
        "            episodes_per_file = max(1, (len(temp_data) - 1) // 1500)  # 1500 = episode_length\n",
        "            total_steps += episodes_per_file\n",
        "            del temp_data  # Free memory immediately\n",
        "\n",
        "        total_steps *= epochs\n",
        "        progress_bar.max = total_steps\n",
        "\n",
        "        print(f\"üìà Training Progress: {epochs} epochs √ó {total_steps // epochs} total episodes = {total_steps} steps\")\n",
        "        print(\"üíæ Memory-efficient: One data file loaded at a time\")\n",
        "\n",
        "        # Use the trainer's built-in multi-data training with progress monitoring\n",
        "        class ProgressCallback:\n",
        "            def __init__(self, progress_bar, status_text, metrics_text, visualizer):\n",
        "                self.progress_bar = progress_bar\n",
        "                self.status_text = status_text\n",
        "                self.metrics_text = metrics_text\n",
        "                self.visualizer = visualizer\n",
        "                self.step_count = 0\n",
        "\n",
        "            def update(self, epoch, instrument_idx, total_instruments, episode_metrics=None):\n",
        "                self.step_count += 1\n",
        "                self.progress_bar.value = self.step_count\n",
        "\n",
        "                current_instrument = available_instruments[instrument_idx] if instrument_idx < len(available_instruments) else \"Completing\"\n",
        "\n",
        "                self.status_text.value = f\"\"\"\n",
        "                <b>üìà Epoch {epoch + 1}/{epochs} - Processing: {current_instrument}</b><br>\n",
        "                üìä Instrument {instrument_idx + 1}/{total_instruments}<br>\n",
        "                üíæ Memory: One data file loaded at a time\n",
        "                \"\"\"\n",
        "\n",
        "                if episode_metrics:\n",
        "                    self.visualizer.update_metrics(self.step_count, episode_metrics)\n",
        "\n",
        "                    self.metrics_text.value = f\"\"\"\n",
        "                    <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
        "                    <b>üìä Current Metrics:</b><br>\n",
        "                    üéØ Avg Reward: <span style=\"color: green;\"><b>{episode_metrics.get('avg_reward', 0):.4f}</b></span><br>\n",
        "                    üìâ Avg Loss: <span style=\"color: red;\"><b>{episode_metrics.get('avg_loss', 0):.4f}</b></span><br>\n",
        "                    üéÆ Steps: {episode_metrics.get('steps', 0)}<br>\n",
        "                    üèÜ Best Reward: <span style=\"color: blue;\"><b>{self.visualizer.best_reward:.4f}</b></span>\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "\n",
        "        # Custom progress callback (simplified for now)\n",
        "        try:\n",
        "            print(\"üöÄ Starting multi-data training...\")\n",
        "            training_history = self.trainer.train(\n",
        "                epochs=epochs,\n",
        "                available_instruments=available_instruments,\n",
        "                save_frequency=TRAINING_PARAMS['save_frequency'],\n",
        "                log_frequency=TRAINING_PARAMS['log_frequency']\n",
        "            )\n",
        "\n",
        "            # Update visualizer with final results\n",
        "            for i, metrics in enumerate(training_history):\n",
        "                self.visualizer.update_metrics(i + 1, metrics)\n",
        "\n",
        "            # Final status update\n",
        "            status_text.value = \"<b>‚úÖ Multi-data training completed successfully!</b>\"\n",
        "            progress_bar.bar_style = 'success'\n",
        "\n",
        "            return training_history\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
        "            status_text.value = \"<b>‚ö†Ô∏è Training interrupted by user</b>\"\n",
        "            progress_bar.bar_style = 'warning'\n",
        "            return self.trainer.training_history\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Training error: {str(e)}\")\n",
        "            status_text.value = f\"<b>‚ùå Training error: {str(e)}</b>\"\n",
        "            progress_bar.bar_style = 'danger'\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return self.trainer.training_history\n",
        "\n",
        "# Initialize training monitor\n",
        "monitor = HRMTrainingMonitor(trainer, visualizer)\n",
        "\n",
        "# Start multi-data training\n",
        "try:\n",
        "    training_results = monitor.run_training_with_monitoring(\n",
        "        epochs=TRAINING_PARAMS['epochs'],\n",
        "        available_instruments=available_instruments  # Train on ALL instruments per epoch\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚úÖ Multi-data training completed successfully!\")\n",
        "    print(f\"üéØ Trained on {len(available_instruments)} instruments per epoch\")\n",
        "    print(\"üíæ Memory-efficient: One data file loaded at a time\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training failed: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip6OZSy167Of"
      },
      "source": [
        "## 9. Training Results Analysis\n",
        "\n",
        "Analyze the training results and evaluate model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rWPrdXk67Of"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üìä Analyzing Training Results...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Final training visualization\n",
        "if visualizer.training_history:\n",
        "    print(\"üìà Generating Final Training Report...\")\n",
        "\n",
        "    # Create comprehensive plots\n",
        "    final_fig = visualizer.create_training_plots()\n",
        "    if final_fig:\n",
        "        final_fig.update_layout(title_text=\"HRM Training Results - Final Report\")\n",
        "        final_fig.show()\n",
        "\n",
        "    # Statistical analysis\n",
        "    rewards = visualizer.reward_history\n",
        "    losses = visualizer.loss_history\n",
        "\n",
        "    print(f\"\\nüìà Training Statistics:\")\n",
        "    print(f\"  üéØ Total Episodes Completed: {len(rewards)}\")\n",
        "    print(f\"  üèÜ Best Reward Achieved: {visualizer.best_reward:.4f}\")\n",
        "    print(f\"  üìä Final Average Reward: {rewards[-1]:.4f}\")\n",
        "    print(f\"  üìâ Final Average Loss: {losses[-1]:.4f}\")\n",
        "\n",
        "    if len(rewards) >= 20:\n",
        "        print(f\"\\nüìä Performance Trends:\")\n",
        "        early_reward = np.mean(rewards[:10])\n",
        "        late_reward = np.mean(rewards[-10:])\n",
        "        reward_improvement = late_reward - early_reward\n",
        "\n",
        "        early_loss = np.mean(losses[:10])\n",
        "        late_loss = np.mean(losses[-10:])\n",
        "        loss_improvement = early_loss - late_loss\n",
        "\n",
        "        print(f\"  üìà Reward Improvement: {reward_improvement:.4f} ({reward_improvement/early_reward*100:+.1f}%)\")\n",
        "        print(f\"  üìâ Loss Improvement: {loss_improvement:.4f} ({loss_improvement/early_loss*100:+.1f}%)\")\n",
        "\n",
        "    # Model performance analysis\n",
        "    print(f\"\\nüß† Model Performance Analysis:\")\n",
        "    if len(rewards) > 0:\n",
        "        reward_variance = np.var(rewards)\n",
        "        reward_stability = 1.0 / (1.0 + reward_variance) if reward_variance > 0 else 1.0\n",
        "        print(f\"  üìä Reward Variance: {reward_variance:.4f}\")\n",
        "        print(f\"  üéØ Training Stability: {reward_stability:.3f}\")\n",
        "\n",
        "        # Performance rating\n",
        "        if visualizer.best_reward > 0.1:\n",
        "            performance_rating = \"üåü Excellent\"\n",
        "        elif visualizer.best_reward > 0.05:\n",
        "            performance_rating = \"‚úÖ Good\"\n",
        "        elif visualizer.best_reward > 0.0:\n",
        "            performance_rating = \"‚ö†Ô∏è Fair\"\n",
        "        else:\n",
        "            performance_rating = \"‚ùå Poor\"\n",
        "\n",
        "        print(f\"  üèÜ Overall Performance: {performance_rating}\")\n",
        "\n",
        "    # Save training summary\n",
        "    summary_data = {\n",
        "        'episode': list(range(1, len(rewards) + 1)),\n",
        "        'avg_reward': rewards,\n",
        "        'avg_loss': losses,\n",
        "        'best_reward_so_far': [max(rewards[:i+1]) for i in range(len(rewards))]\n",
        "    }\n",
        "\n",
        "    df_summary = pd.DataFrame(summary_data)\n",
        "\n",
        "    # Save to CSV\n",
        "    results_dir = Path(\"training_results\")\n",
        "    results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = results_dir / f\"hrm_training_summary_{timestamp}.csv\"\n",
        "    df_summary.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(f\"\\nüíæ Training summary saved to: {csv_path}\")\n",
        "\n",
        "    # Display final data sample\n",
        "    print(f\"\\nüìã Training Summary (Last 10 Episodes):\")\n",
        "    print(df_summary.tail(10).to_string(index=False))\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No training history available\")\n",
        "\n",
        "print(\"\\nüéâ Training analysis complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PomEXTd267Og"
      },
      "source": [
        "## 10. Model Evaluation and Testing\n",
        "\n",
        "Evaluate the trained HRM model on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mWoDgJC67Og"
      },
      "outputs": [],
      "source": [
        "print(\"üî¨ Evaluating Trained HRM Model...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    # Run model evaluation\n",
        "    if hasattr(trainer, 'model') and trainer.model is not None:\n",
        "        print(\"üß™ Running model evaluation on test episodes...\")\n",
        "        # Use first available instrument for evaluation\n",
        "        eval_instrument = available_instruments[0] if available_instruments else \"Bank_Nifty_5\"\n",
        "        print(f\"üìä Evaluating on instrument: {eval_instrument}\")\n",
        "\n",
        "        eval_results = trainer.evaluate(\n",
        "            episodes=20,  # Evaluate on 20 test episodes\n",
        "            symbol=eval_instrument\n",
        "        )\n",
        "\n",
        "        print(\"\\nüìä Model Evaluation Results:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        for metric, value in eval_results.items():\n",
        "            if isinstance(value, float):\n",
        "                print(f\"  üìà {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"  üìà {metric.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "        # Model summary\n",
        "        print(\"\\nüß† HRM Model Architecture Summary:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        model_summary = trainer.model.get_model_summary()\n",
        "\n",
        "        print(f\"  üî¢ Total Parameters: {model_summary['total_parameters']:,}\")\n",
        "        print(f\"  üéì Trainable Parameters: {model_summary['trainable_parameters']:,}\")\n",
        "        print(f\"  üß† H-Module Parameters: {model_summary['h_module_parameters']:,}\")\n",
        "        print(f\"  ‚ö° L-Module Parameters: {model_summary['l_module_parameters']:,}\")\n",
        "        print(f\"  üîÑ ACT Module Parameters: {model_summary['act_module_parameters']:,}\")\n",
        "        print(f\"  üìö Deep Supervision Parameters: {model_summary['deep_supervision_parameters']:,}\")\n",
        "        print(f\"  üìä Hidden Dimension: {model_summary['hidden_dimension']}\")\n",
        "        print(f\"  üîÑ H-Cycles: {model_summary['H_cycles']} | L-Cycles: {model_summary['L_cycles']}\")\n",
        "        print(f\"  üëÅÔ∏è H-Lookback: {model_summary['h_lookback_window']} | L-Lookback: {model_summary['l_lookback_window']}\")\n",
        "        print(f\"  üíª Device: {model_summary['device']}\")\n",
        "\n",
        "        # Checkpoint validation\n",
        "        print(\"\\nüîç Validating Model Checkpoints...\")\n",
        "        model_dir = Path(\"models/hrm\")\n",
        "        checkpoint_dir = Path(\"checkpoints/hrm\")\n",
        "\n",
        "        checkpoints_valid = False\n",
        "        if model_dir.exists() or checkpoint_dir.exists():\n",
        "            checkpoints_valid = True\n",
        "            print(\"‚úÖ Model checkpoints are valid and ready for deployment\")\n",
        "\n",
        "            # List saved models\n",
        "            if model_dir.exists():\n",
        "                model_files = list(model_dir.glob(\"*.pt\"))\n",
        "                print(f\"\\nüìÅ Saved Models ({len(model_files)}):\")\n",
        "                for model_file in model_files:\n",
        "                    print(f\"  üíæ {model_file.name}\")\n",
        "\n",
        "            if checkpoint_dir.exists():\n",
        "                checkpoint_files = list(checkpoint_dir.glob(\"*.pt\"))\n",
        "                print(f\"\\nüìÅ Training Checkpoints ({len(checkpoint_files)}):\")\n",
        "                for checkpoint_file in checkpoint_files:\n",
        "                    print(f\"  üíæ {checkpoint_file.name}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No checkpoint directories found yet\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No trained model available for evaluation\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Evaluation failed: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\nüéØ Model evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iINYDIkS67Og"
      },
      "source": [
        "## 11. Export Results and Model\n",
        "\n",
        "Export training results, model files, and create deployment package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlDj6a0r67Og"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üì¶ Creating Export Package...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create export directory\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "export_dir = Path(f\"hrm_export_{timestamp}\")\n",
        "export_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Export directory: {export_dir}\")\n",
        "\n",
        "try:\n",
        "    # 1. Copy model files\n",
        "    model_dir = Path(\"models/hrm\")\n",
        "    if model_dir.exists():\n",
        "        export_models_dir = export_dir / \"models\"\n",
        "        shutil.copytree(model_dir, export_models_dir)\n",
        "        print(f\"‚úÖ Models exported to: {export_models_dir}\")\n",
        "\n",
        "    # 2. Copy checkpoints\n",
        "    checkpoint_dir = Path(\"checkpoints/hrm\")\n",
        "    if checkpoint_dir.exists():\n",
        "        export_checkpoints_dir = export_dir / \"checkpoints\"\n",
        "        shutil.copytree(checkpoint_dir, export_checkpoints_dir)\n",
        "        print(f\"‚úÖ Checkpoints exported to: {export_checkpoints_dir}\")\n",
        "\n",
        "    # 3. Copy training results\n",
        "    results_dir = Path(\"training_results\")\n",
        "    if results_dir.exists():\n",
        "        export_results_dir = export_dir / \"training_results\"\n",
        "        shutil.copytree(results_dir, export_results_dir)\n",
        "        print(f\"‚úÖ Training results exported to: {export_results_dir}\")\n",
        "\n",
        "    # 4. Copy configuration\n",
        "    config_files = ['config/hrm_config.yaml']\n",
        "    export_config_dir = export_dir / \"config\"\n",
        "    export_config_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    for config_file in config_files:\n",
        "        if Path(config_file).exists():\n",
        "            shutil.copy2(config_file, export_config_dir)\n",
        "            print(f\"‚úÖ Config exported: {config_file}\")\n",
        "\n",
        "    # 5. Create training summary report\n",
        "    summary_report = {\n",
        "        \"training_info\": {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"device_used\": str(DEVICE),\n",
        "            \"device_type\": DEVICE_TYPE,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"epochs_completed\": len(visualizer.training_history) if visualizer.training_history else 0,\n",
        "            \"selected_instrument\": available_instruments[0] if available_instruments else \"Unknown\"\n",
        "        },\n",
        "        \"training_parameters\": TRAINING_PARAMS,\n",
        "        \"performance_summary\": {\n",
        "            \"best_reward\": float(visualizer.best_reward) if visualizer.training_history else 0.0,\n",
        "            \"final_reward\": float(visualizer.reward_history[-1]) if visualizer.reward_history else 0.0,\n",
        "            \"final_loss\": float(visualizer.loss_history[-1]) if visualizer.loss_history else 0.0,\n",
        "            \"total_episodes\": len(visualizer.training_history) if visualizer.training_history else 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Add evaluation results if available\n",
        "    if 'eval_results' in locals():\n",
        "        summary_report[\"evaluation_results\"] = {\n",
        "            k: float(v) if isinstance(v, (int, float)) else v\n",
        "            for k, v in eval_results.items()\n",
        "        }\n",
        "\n",
        "    # Save summary report\n",
        "    summary_file = export_dir / \"training_summary.json\"\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(summary_report, f, indent=2)\n",
        "    print(f\"‚úÖ Summary report saved: {summary_file}\")\n",
        "\n",
        "    # 6. Create README for export\n",
        "    readme_content = f\"\"\"\n",
        "# HRM Trading Model Export Package\n",
        "\n",
        "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "**Training Device:** {DEVICE_TYPE.upper()}\n",
        "**Instrument:** {available_instruments[0] if available_instruments else 'Unknown'}\n",
        "\n",
        "## Contents\n",
        "\n",
        "- `models/` - Trained HRM models (best model: hrm_best_model.pt)\n",
        "- `checkpoints/` - Training checkpoints for resuming\n",
        "- `training_results/` - Training logs and metrics\n",
        "- `config/` - Model configuration files\n",
        "- `training_summary.json` - Complete training summary\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "- **Total Parameters:** {model_summary['total_parameters']:,} (‚âà27M)\n",
        "- **Architecture:** Hierarchical Reasoning Model (HRM)\n",
        "- **H-Module Lookback:** {model_summary['h_lookback_window']} candles\n",
        "- **L-Module Lookback:** {model_summary['l_lookback_window']} candles\n",
        "- **Hidden Dimension:** {model_summary['hidden_dimension']}\n",
        "\n",
        "## Performance\n",
        "\n",
        "- **Best Reward:** {visualizer.best_reward:.4f}\n",
        "- **Episodes Completed:** {len(visualizer.training_history) if visualizer.training_history else 0}\n",
        "- **Training Status:** {'Completed' if len(visualizer.training_history) >= TRAINING_PARAMS['epochs'] else 'Partial'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2y74wzv67Og"
      },
      "source": [
        "## 12. Training Summary and Next Steps\n",
        "\n",
        "Final summary of the training process and recommendations for next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNgNiqzi67Oh"
      },
      "outputs": [],
      "source": [
        "print(\"üéä HRM TRAINING COMPLETED!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Final summary\n",
        "total_time = time.time() - monitor.start_time if 'monitor' in locals() else 0\n",
        "\n",
        "print(f\"\\nüìä FINAL TRAINING SUMMARY:\")\n",
        "print(f\"=\" * 50)\n",
        "print(f\"üß† Model: Hierarchical Reasoning Model (HRM)\")\n",
        "print(f\"üìà Instrument: {selected_instrument if 'selected_instrument' in locals() else 'Unknown'}\")\n",
        "print(f\"üíª Device: {DEVICE_TYPE.upper()} ({DEVICE})\")\n",
        "print(f\"‚è±Ô∏è Total Training Time: {total_time/3600:.1f} hours\")\n",
        "print(f\"üéØ Episodes Completed: {len(visualizer.training_history) if visualizer.training_history else 0}/{TRAINING_PARAMS['epochs']}\")\n",
        "\n",
        "if visualizer.training_history:\n",
        "    print(f\"üèÜ Best Performance: {visualizer.best_reward:.4f}\")\n",
        "    print(f\"üìà Final Performance: {visualizer.reward_history[-1]:.4f}\")\n",
        "    print(f\"üìâ Final Loss: {visualizer.loss_history[-1]:.4f}\")\n",
        "\n",
        "# Model specifications\n",
        "if 'model_summary' in locals():\n",
        "    print(f\"\\nüß† MODEL SPECIFICATIONS:\")\n",
        "    print(f\"=\" * 50)\n",
        "    print(f\"üìä Total Parameters: {model_summary['total_parameters']:,}\")\n",
        "    print(f\"üîÑ H-Cycles: {model_summary['H_cycles']} | L-Cycles: {model_summary['L_cycles']}\")\n",
        "    print(f\"üëÅÔ∏è H-Lookback: {model_summary['h_lookback_window']} | L-Lookback: {model_summary['l_lookback_window']}\")\n",
        "    print(f\"üî¢ Hidden Dimension: {model_summary['hidden_dimension']}\")\n",
        "\n",
        "# Performance evaluation\n",
        "if visualizer.training_history:\n",
        "    performance_level = \"üåü Excellent\" if visualizer.best_reward > 0.1 else \\\n",
        "                       \"‚úÖ Good\" if visualizer.best_reward > 0.05 else \\\n",
        "                       \"‚ö†Ô∏è Fair\" if visualizer.best_reward > 0.0 else \\\n",
        "                       \"‚ùå Needs Improvement\"\n",
        "\n",
        "    print(f\"\\nüèÜ PERFORMANCE EVALUATION: {performance_level}\")\n",
        "\n",
        "# Next steps recommendations\n",
        "print(f\"\\nüöÄ RECOMMENDED NEXT STEPS:\")\n",
        "print(f\"=\" * 50)\n",
        "\n",
        "if visualizer.best_reward > 0.05:\n",
        "    print(f\"‚úÖ Model shows good performance:\")\n",
        "    print(f\"  ‚Ä¢ Deploy model for live testing\")\n",
        "    print(f\"  ‚Ä¢ Run extended evaluation on more instruments\")\n",
        "    print(f\"  ‚Ä¢ Consider ensemble with other models\")\n",
        "elif visualizer.best_reward > 0.0:\n",
        "    print(f\"‚ö†Ô∏è Model shows moderate performance:\")\n",
        "    print(f\"  ‚Ä¢ Continue training for more epochs\")\n",
        "    print(f\"  ‚Ä¢ Experiment with different hyperparameters\")\n",
        "    print(f\"  ‚Ä¢ Try different instruments or timeframes\")\n",
        "else:\n",
        "    print(f\"‚ùå Model needs improvement:\")\n",
        "    print(f\"  ‚Ä¢ Check data quality and preprocessing\")\n",
        "    print(f\"  ‚Ä¢ Adjust learning rates or architecture\")\n",
        "    print(f\"  ‚Ä¢ Consider curriculum learning approach\")\n",
        "\n",
        "print(f\"\\nüìÅ Export package available at: {export_dir if 'export_dir' in locals() else 'Not created'}\")\n",
        "\n",
        "# Additional recommendations\n",
        "print(f\"\\nüí° ADDITIONAL RECOMMENDATIONS:\")\n",
        "print(f\"=\" * 50)\n",
        "print(f\"üìä ‚Ä¢ Monitor model performance on different market conditions\")\n",
        "print(f\"üîÑ ‚Ä¢ Implement continuous learning for market adaptation\")\n",
        "print(f\"‚ö†Ô∏è ‚Ä¢ Add robust risk management and position sizing\")\n",
        "print(f\"üìà ‚Ä¢ Backtest on historical data before live deployment\")\n",
        "print(f\"üè≠ ‚Ä¢ Consider distributed training for larger datasets\")\n",
        "\n",
        "print(f\"\\nüéØ Training session completed successfully!\")\n",
        "print(f\"üìö Refer to the HRM research paper for implementation details\")\n",
        "print(f\"üîó Paper: 'Hierarchical Reasoning Model' by Guan Wang et al.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéâ Thank you for using the HRM Training Notebook!\")\n",
        "print(\"=\" * 80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c1dd137baba4988bcfcb01cd862ea01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "Training:",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b913efc14964d0aba6053a40efa4187",
            "max": 44000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_121d893bdf974d0facf05c783f226d04",
            "value": 0
          }
        },
        "121d893bdf974d0facf05c783f226d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "green",
            "description_width": ""
          }
        },
        "175501aabf314945bb3f13c4dd99f91d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c1dd137baba4988bcfcb01cd862ea01",
              "IPY_MODEL_22646573d04e4e5d862b09ce9695f0d3",
              "IPY_MODEL_2e285078e3a84456958ec6d5e96f2174"
            ],
            "layout": "IPY_MODEL_7b4cbd69439e4cf6afe49d374339aa97"
          }
        },
        "22646573d04e4e5d862b09ce9695f0d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83f621d082474f04bb008877dbd0a515",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f6ab6938593b4ceaaa5c48797a07c8cb",
            "value": "<b>üéØ Multi-Data Training: {len(available_instruments)} instruments √ó {epochs} epochs</b>"
          }
        },
        "2e285078e3a84456958ec6d5e96f2174": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_836be5d8885041afb7e4a888dc0bbcd9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4d94dce272af4e40b29bdb9c090e572f",
            "value": ""
          }
        },
        "4d94dce272af4e40b29bdb9c090e572f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b913efc14964d0aba6053a40efa4187": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b4cbd69439e4cf6afe49d374339aa97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "836be5d8885041afb7e4a888dc0bbcd9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f621d082474f04bb008877dbd0a515": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6ab6938593b4ceaaa5c48797a07c8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
