{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHZCW0Nz67OW"
      },
      "source": [
        "# HRM (Hierarchical Reasoning Model) Trading Training on Colab/Kaggle\n",
        "\n",
        "This notebook converts the HRM training pipeline to run on Google Colab or Kaggle notebooks with automatic TPU/GPU/CPU detection.\n",
        "\n",
        "## Overview\n",
        "- **HRM Architecture**: Brain-inspired hierarchical model with 27M parameters\n",
        "- **Training Approach**: Deep supervision with adaptive computation time\n",
        "- **Data Processing**: Automated pipeline from raw to training-ready data\n",
        "- **Hardware Support**: Automatic TPU/GPU/CPU detection with parallel training\n",
        "\n",
        "Based on the research paper: *Hierarchical Reasoning Model* by Guan Wang et al."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZormcV-r67OY"
      },
      "source": [
        "## 1. Setup Environment and Clone Repository\n",
        "\n",
        "Clone the private GitHub repository using the service token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pyI6gCpQ67OY",
        "outputId": "4254b0dd-be78-4520-eb55-39fd9ceeae60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Running in Colab: True | Kaggle: False | TPU: False\n",
            "ğŸ“¥ Cloning AlgoTrading repository...\n",
            "âœ… Repository cloned successfully!\n",
            "ğŸ“ Current directory: /content/AlgoTrading\n",
            "ğŸ“‹ Repository contents:\n",
            "  .bmad-core\n",
            "  .claude\n",
            "  .gemini\n",
            "  .git\n",
            "  .gitignore\n",
            "  .roomodes\n",
            "  HRM_Training_Notebook.ipynb\n",
            "  __pycache__\n",
            "  config\n",
            "  data\n",
            "  data_processing_pipeline.log\n",
            "  docs\n",
            "  fyers_docs.txt\n",
            "  hrm-research-paper.txt\n",
            "  qwen-cli-claude-delegation.md\n",
            "  requirements.txt\n",
            "  run_training.py\n",
            "  src\n",
            "  tests\n",
            "  web-bundles\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Detect runtime environment\n",
        "def is_colab():\n",
        "    return \"COLAB_GPU\" in os.environ\n",
        "\n",
        "def is_kaggle():\n",
        "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
        "\n",
        "def check_tpu_availability():\n",
        "    try:\n",
        "        import requests\n",
        "        response = requests.get(\n",
        "            'http://metadata.google.internal/computeMetadata/v1/instance/attributes/accelerator-type',\n",
        "            headers={'Metadata-Flavor': 'Google'},\n",
        "            timeout=5\n",
        "        )\n",
        "        return 'tpu' in response.text.lower()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "in_cloud_env = is_colab() or is_kaggle()\n",
        "is_tpu_environment = check_tpu_availability() if in_cloud_env else False\n",
        "\n",
        "print(f\"ğŸ” Running in Colab: {is_colab()} | Kaggle: {is_kaggle()} | TPU: {is_tpu_environment}\")\n",
        "\n",
        "# Repo details\n",
        "repo_url = \"https://{personal_access_token}@github.com/marshaltudu14/AlgoTrading.git\"\n",
        "repo_path = \"/content/AlgoTrading\"\n",
        "\n",
        "if in_cloud_env or is_tpu_environment:\n",
        "    # Clone repository only if running in cloud\n",
        "    if not os.path.exists(repo_path):\n",
        "        print(\"ğŸ“¥ Cloning AlgoTrading repository...\")\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"clone\", repo_url, repo_path],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(\"âœ… Repository cloned successfully!\")\n",
        "        else:\n",
        "            print(f\"âŒ Failed to clone repository: {result.stderr}\")\n",
        "            raise Exception(\"Repository clone failed\")\n",
        "    else:\n",
        "        print(\"âœ… Repository already exists\")\n",
        "\n",
        "    # Change to repository directory\n",
        "    os.chdir(repo_path)\n",
        "    sys.path.append(repo_path)\n",
        "\n",
        "    print(f\"ğŸ“ Current directory: {os.getcwd()}\")\n",
        "    print(f\"ğŸ“‹ Repository contents:\")\n",
        "    for item in sorted(os.listdir('.')):\n",
        "        print(f\"  {item}\")\n",
        "else:\n",
        "    print(\"â­ï¸ Local environment detected. Skipping repository cloning.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbKvHB-C67OZ"
      },
      "source": [
        "## 2. Install Dependencies\n",
        "\n",
        "Install all required packages including TPU support for Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1Q0AkMDj67Oa",
        "outputId": "a9f17757-708f-4422-96a2-dfe1cae870d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Running in Colab: True | Kaggle: False | TPU: False\n",
            "ğŸ“¦ Installing base requirements...\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (4.55.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (2.8.0+cu126)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (1.5.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (5.1.0)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (1.2.0)\n",
            "Collecting fyers_apiv3 (from -r requirements.txt (line 15))\n",
            "  Downloading fyers_apiv3-3.1.7-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pyotp (from -r requirements.txt (line 16))\n",
            "  Downloading pyotp-2.9.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (2.32.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (2025.2)\n",
            "Collecting pandas_ta (from -r requirements.txt (line 19))\n",
            "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 22)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 23)) (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 24)) (5.24.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 25)) (7.7.1)\n",
            "Collecting kaleido (from -r requirements.txt (line 26))\n",
            "  Downloading kaleido-1.0.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 29)) (0.116.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 31)) (2.11.7)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 33)) (0.0.20)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 34)) (15.0.1)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 35)) (2.10.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 36)) (1.1.1)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 37)) (24.1.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 40)) (8.4.1)\n",
            "Collecting pytest-asyncio (from -r requirements.txt (line 41))\n",
            "  Downloading pytest_asyncio-1.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 42)) (0.28.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 43)) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 44)) (4.67.1)\n",
            "Requirement already satisfied: ruff in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 45)) (0.12.10)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 46)) (13.9.4)\n",
            "Collecting memory-profiler (from -r requirements.txt (line 47))\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 48)) (6.0.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 49)) (2.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 52)) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 53)) (2025.8.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 54)) (3.10)\n",
            "Requirement already satisfied: h11 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 55)) (0.16.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 56)) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 57)) (4.15.0)\n",
            "Collecting gpustat (from -r requirements.txt (line 60))\n",
            "  Downloading gpustat-1.1.1.tar.gz (98 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py3nvml (from -r requirements.txt (line 61))\n",
            "  Downloading py3nvml-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting prefetch-generator (from -r requirements.txt (line 62))\n",
            "  Downloading prefetch_generator-1.0.3.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jupyterlab (from -r requirements.txt (line 63))\n",
            "  Downloading jupyterlab-4.4.6-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 64)) (6.5.7)\n",
            "Collecting ray[rllib] (from -r requirements.txt (line 12))\n",
            "  Downloading ray-2.49.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 30)) (0.35.0)\n",
            "Collecting python-jose[cryptography] (from -r requirements.txt (line 32))\n",
            "  Downloading python_jose-3.5.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (0.34.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (0.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r requirements.txt (line 7)) (11.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->-r requirements.txt (line 11)) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->-r requirements.txt (line 11)) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray[rllib]->-r requirements.txt (line 12)) (8.2.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray[rllib]->-r requirements.txt (line 12)) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray[rllib]->-r requirements.txt (line 12)) (1.1.1)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray[rllib]->-r requirements.txt (line 12)) (5.29.5)\n",
            "Collecting tensorboardX>=1.9 (from ray[rllib]->-r requirements.txt (line 12))\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from ray[rllib]->-r requirements.txt (line 12)) (18.1.0)\n",
            "Requirement already satisfied: dm_tree in /usr/local/lib/python3.12/dist-packages (from ray[rllib]->-r requirements.txt (line 12)) (0.1.9)\n",
            "Collecting gymnasium (from -r requirements.txt (line 11))\n",
            "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting lz4 (from ray[rllib]->-r requirements.txt (line 12))\n",
            "  Downloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting ormsgpack==1.7.0 (from ray[rllib]->-r requirements.txt (line 12))\n",
            "  Downloading ormsgpack-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests (from -r requirements.txt (line 17))\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting asyncio==3.4.3 (from fyers_apiv3->-r requirements.txt (line 15))\n",
            "  Downloading asyncio-3.4.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting aiohttp==3.9.3 (from fyers_apiv3->-r requirements.txt (line 15))\n",
            "  Downloading aiohttp-3.9.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting aws_lambda_powertools==1.25.5 (from fyers_apiv3->-r requirements.txt (line 15))\n",
            "  Downloading aws_lambda_powertools-1.25.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting websocket-client==1.6.1 (from fyers_apiv3->-r requirements.txt (line 15))\n",
            "  Downloading websocket_client-1.6.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting protobuf>=3.20.3 (from ray[rllib]->-r requirements.txt (line 12))\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 17)) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 17)) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp==3.9.3->fyers_apiv3->-r requirements.txt (line 15)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp==3.9.3->fyers_apiv3->-r requirements.txt (line 15)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp==3.9.3->fyers_apiv3->-r requirements.txt (line 15)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp==3.9.3->fyers_apiv3->-r requirements.txt (line 15)) (6.6.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp==3.9.3->fyers_apiv3->-r requirements.txt (line 15)) (1.20.1)\n",
            "Collecting aws-xray-sdk<3.0.0,>=2.8.0 (from aws_lambda_powertools==1.25.5->fyers_apiv3->-r requirements.txt (line 15))\n",
            "  Downloading aws_xray_sdk-2.14.0-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting boto3<2.0,>=1.18 (from aws_lambda_powertools==1.25.5->fyers_apiv3->-r requirements.txt (line 15))\n",
            "  Downloading boto3-1.40.21-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: fastjsonschema<3.0.0,>=2.14.5 in /usr/local/lib/python3.12/dist-packages (from aws_lambda_powertools==1.25.5->fyers_apiv3->-r requirements.txt (line 15)) (2.21.2)\n",
            "Collecting jmespath<0.11.0,>=0.10.0 (from aws_lambda_powertools==1.25.5->fyers_apiv3->-r requirements.txt (line 15))\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 22)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 22)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 22)) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 22)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 22)) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->-r requirements.txt (line 24)) (8.5.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->-r requirements.txt (line 25)) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->-r requirements.txt (line 25)) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->-r requirements.txt (line 25)) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->-r requirements.txt (line 25)) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->-r requirements.txt (line 25)) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->-r requirements.txt (line 25)) (3.0.15)\n",
            "Collecting choreographer>=1.0.5 (from kaleido->-r requirements.txt (line 26))\n",
            "  Downloading choreographer-1.0.10-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting logistro>=1.0.8 (from kaleido->-r requirements.txt (line 26))\n",
            "  Downloading logistro-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.12/dist-packages (from kaleido->-r requirements.txt (line 26)) (3.11.2)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi->-r requirements.txt (line 29)) (0.47.3)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]->-r requirements.txt (line 30))\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]->-r requirements.txt (line 30))\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]->-r requirements.txt (line 30))\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->-r requirements.txt (line 31)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->-r requirements.txt (line 31)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->-r requirements.txt (line 31)) (0.4.1)\n",
            "Collecting ecdsa!=0.15 (from python-jose[cryptography]->-r requirements.txt (line 32))\n",
            "  Downloading ecdsa-0.19.1-py2.py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: rsa!=4.1.1,!=4.4,<5.0,>=4.0 in /usr/local/lib/python3.12/dist-packages (from python-jose[cryptography]->-r requirements.txt (line 32)) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from python-jose[cryptography]->-r requirements.txt (line 32)) (0.6.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from python-jose[cryptography]->-r requirements.txt (line 32)) (43.0.3)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->-r requirements.txt (line 40)) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->-r requirements.txt (line 40)) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest->-r requirements.txt (line 40)) (2.19.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->-r requirements.txt (line 42)) (1.0.9)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->-r requirements.txt (line 46)) (4.0.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->-r requirements.txt (line 49)) (4.9.3)\n",
            "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.12/dist-packages (from gpustat->-r requirements.txt (line 60)) (12.575.51)\n",
            "Collecting blessed>=1.17.1 (from gpustat->-r requirements.txt (line 60))\n",
            "  Downloading blessed-1.21.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting xmltodict (from py3nvml->-r requirements.txt (line 61))\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.12/dist-packages (from jupyterlab->-r requirements.txt (line 63)) (5.8.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading jupyter_server-2.17.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->-r requirements.txt (line 63)) (0.2.4)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->-r requirements.txt (line 63)) (6.4.2)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from notebook->-r requirements.txt (line 64)) (26.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook->-r requirements.txt (line 64)) (25.1.0)\n",
            "Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.12/dist-packages (from notebook->-r requirements.txt (line 64)) (6.1.12)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (from notebook->-r requirements.txt (line 64)) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.12/dist-packages (from notebook->-r requirements.txt (line 64)) (7.16.6)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.12/dist-packages (from notebook->-r requirements.txt (line 64)) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook->-r requirements.txt (line 64)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook->-r requirements.txt (line 64)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook->-r requirements.txt (line 64)) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook->-r requirements.txt (line 64)) (1.3.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.12/dist-packages (from blessed>=1.17.1->gpustat->-r requirements.txt (line 60)) (0.2.13)\n",
            "Requirement already satisfied: simplejson>=3.19.3 in /usr/local/lib/python3.12/dist-packages (from choreographer>=1.0.5->kaleido->-r requirements.txt (line 26)) (3.20.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->python-jose[cryptography]->-r requirements.txt (line 32)) (1.17.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from ecdsa!=0.15->python-jose[cryptography]->-r requirements.txt (line 32)) (1.17.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 5)) (1.1.8)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 25)) (1.8.15)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 25)) (0.1.7)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 25))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 25)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 25)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 25)) (3.0.51)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 25)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 25)) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core->jupyterlab->-r requirements.txt (line 63)) (4.3.8)\n",
            "Collecting jupyter-client<8,>=5.3.4 (from notebook->-r requirements.txt (line 64))\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "INFO: pip is looking at multiple versions of jupyter-server to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.14.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.14.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.14.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading jupyter_server-2.13.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client<8,>=5.3.4->notebook->-r requirements.txt (line 64)) (0.4)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 63)) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray[rllib]->-r requirements.txt (line 12)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray[rllib]->-r requirements.txt (line 12)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray[rllib]->-r requirements.txt (line 12)) (0.27.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 46)) (0.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r requirements.txt (line 64)) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook->-r requirements.txt (line 64)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r requirements.txt (line 64)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r requirements.txt (line 64)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r requirements.txt (line 64)) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r requirements.txt (line 64)) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r requirements.txt (line 64)) (1.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.12/dist-packages (from terminado>=0.8.3->notebook->-r requirements.txt (line 64)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook->-r requirements.txt (line 64)) (25.1.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from dm_tree->ray[rllib]->-r requirements.txt (line 12)) (1.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.12/dist-packages (from dm_tree->ray[rllib]->-r requirements.txt (line 12)) (1.17.3)\n",
            "Collecting botocore>=1.11.3 (from aws-xray-sdk<3.0.0,>=2.8.0->aws_lambda_powertools==1.25.5->fyers_apiv3->-r requirements.txt (line 15))\n",
            "  Downloading botocore-1.40.21-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook->-r requirements.txt (line 64)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook->-r requirements.txt (line 64)) (1.4.0)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3<2.0,>=1.18->aws_lambda_powertools==1.25.5->fyers_apiv3->-r requirements.txt (line 15))\n",
            "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->python-jose[cryptography]->-r requirements.txt (line 32)) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 25)) (0.8.5)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.0->aiohttp==3.9.3->fyers_apiv3->-r requirements.txt (line 15)) (0.3.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert>=5->notebook->-r requirements.txt (line 64)) (2.7)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63)) (3.0.0)\n",
            "Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63)) (24.11.1)\n",
            "Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 63))\n",
            "  Downloading types_python_dateutil-2.9.0.20250822-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fyers_apiv3-3.1.7-py3-none-any.whl (37 kB)\n",
            "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.9.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aws_lambda_powertools-1.25.5-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m171.8/171.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websocket_client-1.6.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyotp-2.9.0-py3-none-any.whl (13 kB)\n",
            "Downloading kaleido-1.0.0-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_asyncio-1.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Downloading py3nvml-0.2.7-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.4.6-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m133.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading blessed-1.21.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading choreographer-1.0.10-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ecdsa-0.19.1-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.13.0-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading logistro-1.1.0-py3-none-any.whl (7.9 kB)\n",
            "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_jose-3.5.0-py2.py3-none-any.whl (34 kB)\n",
            "Downloading ray-2.49.0-cp312-cp312-manylinux2014_x86_64.whl (70.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading aws_xray_sdk-2.14.0-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.21-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading botocore-1.40.21-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20250822-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: pandas_ta, gpustat, prefetch-generator\n",
            "  Building wheel for pandas_ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas_ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218910 sha256=be1340c3c8089a09b14c6825276dd2952f3d1b3bbfe76cf7a81002464e5661d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/ed/18/2a12fd1b7906c63efca6accb351929f2c7f6bbc674e1c0ba5d\n",
            "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.1.1-py3-none-any.whl size=26720 sha256=574ec42bf4010c76e6bf7ddee3d1453999e16b3b5bd70f342cf0906cf29c20d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/11/1f/d78086f1f4693c23c6dc261e4f7da46bf3a9eb0717f2fb28b9\n",
            "  Building wheel for prefetch-generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prefetch-generator: filename=prefetch_generator-1.0.3-py3-none-any.whl size=4758 sha256=03c863ea66c15c5b43afb9fd42291f9120d6742caee15af2a440edd5370f58d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/88/c7/3b5afc342fc80a599ce41ba9000cf8a71261991c35cf088edf\n",
            "Successfully built pandas_ta gpustat prefetch-generator\n",
            "Installing collected packages: prefetch-generator, asyncio, xmltodict, websocket-client, uvloop, uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, requests, python-json-logger, pyotp, protobuf, overrides, ormsgpack, memory-profiler, lz4, logistro, lark, json5, jmespath, jedi, httptools, gymnasium, fqdn, ecdsa, blessed, async-lru, watchfiles, tensorboardX, rfc3987-syntax, python-jose, pytest-asyncio, py3nvml, jupyter-server-terminals, jupyter-client, gpustat, choreographer, botocore, arrow, aiohttp, s3transfer, pandas_ta, kaleido, isoduration, aws-xray-sdk, ray, boto3, jupyter-events, aws_lambda_powertools, fyers_apiv3, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab\n",
            "  Attempting uninstall: websocket-client\n",
            "    Found existing installation: websocket-client 1.8.0\n",
            "    Uninstalling websocket-client-1.8.0:\n",
            "      Successfully uninstalled websocket-client-1.8.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.2.0\n",
            "    Uninstalling gymnasium-1.2.0:\n",
            "      Successfully uninstalled gymnasium-1.2.0\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.12.15\n",
            "    Uninstalling aiohttp-3.12.15:\n",
            "      Successfully uninstalled aiohttp-3.12.15\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.16.0\n",
            "    Uninstalling jupyter-server-1.16.0:\n",
            "      Successfully uninstalled jupyter-server-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires jupyter-server==1.16.0, but you have jupyter-server 2.13.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.31.0 which is incompatible.\n",
            "datasets 4.0.0 requires requests>=2.32.2, but you have requests 2.31.0 which is incompatible.\n",
            "google-adk 1.12.0 requires requests<3.0.0,>=2.32.4, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.9.3 arrow-1.3.0 async-lru-2.0.5 asyncio-3.4.3 aws-xray-sdk-2.14.0 aws_lambda_powertools-1.25.5 blessed-1.21.0 boto3-1.40.21 botocore-1.40.21 choreographer-1.0.10 ecdsa-0.19.1 fqdn-1.5.1 fyers_apiv3-3.1.7 gpustat-1.1.1 gymnasium-1.1.1 httptools-0.6.4 isoduration-20.11.0 jedi-0.19.2 jmespath-0.10.0 json5-0.12.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.3.0 jupyter-server-2.13.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.6 jupyterlab-server-2.27.3 kaleido-1.0.0 lark-1.2.2 logistro-1.1.0 lz4-4.4.4 memory-profiler-0.61.0 ormsgpack-1.7.0 overrides-7.7.0 pandas_ta-0.3.14b0 prefetch-generator-1.0.3 protobuf-5.29.3 py3nvml-0.2.7 pyotp-2.9.0 pytest-asyncio-1.1.0 python-jose-3.5.0 python-json-logger-3.3.0 ray-2.49.0 requests-2.31.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 s3transfer-0.13.1 tensorboardX-2.6.4 types-python-dateutil-2.9.0.20250822 uri-template-1.3.0 uvloop-0.21.0 watchfiles-1.1.0 websocket-client-1.6.1 xmltodict-0.14.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "asyncio",
                  "google",
                  "requests",
                  "site"
                ]
              },
              "id": "dd40b307fd414007a43dfb10e0c09d8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Installing visualization libraries...\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (7.7.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n",
            "Requirement already satisfied: choreographer>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from kaleido) (1.0.10)\n",
            "Requirement already satisfied: logistro>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from kaleido) (1.1.0)\n",
            "Requirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.12/dist-packages (from kaleido) (3.11.2)\n",
            "Requirement already satisfied: simplejson>=3.19.3 in /usr/local/lib/python3.12/dist-packages (from choreographer>=1.0.5->kaleido) (3.20.1)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.27.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.12/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.13.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.10.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.3)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.7.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.0.2)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.11.1)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.2)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.12/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.9.0.20250822)\n",
            "âœ… All dependencies installed successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Detect runtime environment\n",
        "def is_colab():\n",
        "    return \"COLAB_GPU\" in os.environ\n",
        "\n",
        "def is_kaggle():\n",
        "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
        "\n",
        "def check_tpu_availability():\n",
        "    try:\n",
        "        import requests\n",
        "        response = requests.get(\n",
        "            'http://metadata.google.internal/computeMetadata/v1/instance/attributes/accelerator-type',\n",
        "            headers={'Metadata-Flavor': 'Google'},\n",
        "            timeout=5\n",
        "        )\n",
        "        return 'tpu' in response.text.lower()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "in_cloud_env = is_colab() or is_kaggle()\n",
        "is_tpu_environment = check_tpu_availability() if in_cloud_env else False\n",
        "\n",
        "print(f\"ğŸ” Running in Colab: {is_colab()} | Kaggle: {is_kaggle()} | TPU: {is_tpu_environment}\")\n",
        "\n",
        "# Install only if in cloud (Colab/Kaggle/TPU)\n",
        "if in_cloud_env or is_tpu_environment:\n",
        "    print(\"ğŸ“¦ Installing base requirements...\")\n",
        "    !pip install -r requirements.txt\n",
        "\n",
        "    if is_tpu_environment:\n",
        "        print(\"ğŸš€ Installing TPU support...\")\n",
        "        !pip install torch_xla cloud-tpu-client\n",
        "\n",
        "    print(\"ğŸ“Š Installing visualization libraries...\")\n",
        "    !pip install ipywidgets plotly kaleido\n",
        "\n",
        "    print(\"âœ… All dependencies installed successfully!\")\n",
        "else:\n",
        "    print(\"â­ï¸ Local environment detected. Skipping installation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tafK7hNo67Oa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.NaN = np.nan  # Fix missing alias\n",
        "import pandas_ta as ta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V4RRkrVr67Ob",
        "outputId": "30f78b41-011b-44cb-f5fd-e1b1f4f0b6d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â˜ï¸ Cloud environment detected\n",
            "ğŸ“ Project path set to: /content/AlgoTrading\n",
            "ğŸ“‚ Project exists: True\n",
            "âœ… Changed working directory to: /content/AlgoTrading\n",
            "âœ… Added to Python path: /content/AlgoTrading\n",
            "âœ… Found directory: src/\n",
            "âœ… Found directory: config/\n",
            "âœ… Found directory: data/\n",
            "\n",
            "ğŸ“‹ Project contents:\n",
            "  ğŸ“ .bmad-core/\n",
            "  ğŸ“ .claude/\n",
            "  ğŸ“ .gemini/\n",
            "  ğŸ“ .git/\n",
            "  ğŸ“„ .gitignore\n",
            "  ğŸ“„ .roomodes\n",
            "  ğŸ“„ HRM_Training_Notebook.ipynb\n",
            "  ğŸ“ __pycache__/\n",
            "  ğŸ“ config/\n",
            "  ğŸ“ data/\n",
            "  ğŸ“„ data_processing_pipeline.log\n",
            "  ğŸ“ docs/\n",
            "  ğŸ“„ fyers_docs.txt\n",
            "  ğŸ“„ hrm-research-paper.txt\n",
            "  ğŸ“ logs/\n",
            "  ğŸ“„ qwen-cli-claude-delegation.md\n",
            "  ğŸ“ reports/\n",
            "  ğŸ“„ requirements.txt\n",
            "  ğŸ“„ run_training.py\n",
            "  ğŸ“ src/\n",
            "  ğŸ“ temp/\n",
            "  ğŸ“ tests/\n",
            "  ğŸ“ web-bundles/\n",
            "\n",
            "ğŸ¯ PROJECT_PATH variable set: /content/AlgoTrading\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment and set project path\n",
        "def is_colab():\n",
        "    return \"COLAB_GPU\" in os.environ\n",
        "\n",
        "def is_kaggle():\n",
        "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
        "\n",
        "# Determine project path based on environment\n",
        "in_cloud_env = is_colab() or is_kaggle()\n",
        "\n",
        "if in_cloud_env:\n",
        "    # Cloud environment: Use cloned repository path\n",
        "    PROJECT_PATH = \"/content/AlgoTrading\"\n",
        "    print(f\"â˜ï¸ Cloud environment detected\")\n",
        "else:\n",
        "    # Local environment: Use current directory or set custom path\n",
        "    # You can modify this path if your project is in a different location\n",
        "    PROJECT_PATH = os.getcwd()\n",
        "    print(f\"ğŸ’» Local environment detected\")\n",
        "\n",
        "# Convert to Path object for easier manipulation\n",
        "project_path = Path(PROJECT_PATH)\n",
        "\n",
        "print(f\"ğŸ“ Project path set to: {project_path}\")\n",
        "print(f\"ğŸ“‚ Project exists: {project_path.exists()}\")\n",
        "\n",
        "# Change to project directory if it exists\n",
        "if project_path.exists():\n",
        "    os.chdir(str(project_path))\n",
        "    print(f\"âœ… Changed working directory to: {os.getcwd()}\")\n",
        "\n",
        "    # Add project path to Python path for imports\n",
        "    if str(project_path) not in sys.path:\n",
        "        sys.path.insert(0, str(project_path))\n",
        "        print(f\"âœ… Added to Python path: {project_path}\")\n",
        "\n",
        "    # Verify key directories exist\n",
        "    key_dirs = ['src', 'config', 'data']\n",
        "    for dir_name in key_dirs:\n",
        "        dir_path = project_path / dir_name\n",
        "        if dir_path.exists():\n",
        "            print(f\"âœ… Found directory: {dir_name}/\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ Directory not found: {dir_name}/\")\n",
        "\n",
        "    # List contents to verify\n",
        "    print(f\"\\nğŸ“‹ Project contents:\")\n",
        "    for item in sorted(project_path.iterdir()):\n",
        "        if item.is_dir():\n",
        "            print(f\"  ğŸ“ {item.name}/\")\n",
        "        else:\n",
        "            print(f\"  ğŸ“„ {item.name}\")\n",
        "else:\n",
        "    print(f\"âŒ Project path does not exist: {project_path}\")\n",
        "    print(\"âš ï¸ You may need to modify PROJECT_PATH variable above\")\n",
        "\n",
        "# Store project path for use in other cells\n",
        "globals()['PROJECT_PATH'] = str(project_path)\n",
        "print(f\"\\nğŸ¯ PROJECT_PATH variable set: {PROJECT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7csSXZW67Ob"
      },
      "source": [
        "## 3. Device Detection and Setup\n",
        "\n",
        "Automatically detect and configure the best available device (TPU > GPU > CPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sjFR2Tk767Ob",
        "outputId": "a21e2102-0bd2-4948-e6c1-f17567cf55d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ”¥ HIGH-PERFORMANCE DEVICE CONFIGURATION\n",
            "================================================================================\n",
            "Selected Device: cuda\n",
            "Device Type: GPU\n",
            "GPU Name: Tesla T4\n",
            "GPU Memory: 14.7 GB total\n",
            "Available Memory: 14.7 GB\n",
            "GPU Count: 1\n",
            "Compute Capability: 7\n",
            "Tensor Cores: âœ… Available\n",
            "Mixed Precision: âœ… Enabled\n",
            "\n",
            "ğŸš€ PERFORMANCE OPTIMIZATIONS:\n",
            "Recommended Batch Size: 256\n",
            "Gradient Accumulation: 4x\n",
            "DataLoader Workers: 7\n",
            "Pin Memory: âœ… Enabled\n",
            "Mixed Precision: âœ… FP16 Enabled (Tensor Cores)\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸš€ HIGH-PERFORMANCE TRAINING CONFIGURATION\n",
            "================================================================================\n",
            "Target Hardware: 15GB VRAM + 12GB RAM (Colab/Kaggle)\n",
            "Device: cuda\n",
            "Batch Size: 256 (8x larger for 15GB VRAM)\n",
            "Gradient Accumulation: 4x\n",
            "Effective Batch Size: 1024 (massive training batches)\n",
            "Mixed Precision: âœ… FP16 Enabled\n",
            "DataLoader Workers: 7\n",
            "Pin Memory: âœ… Enabled\n",
            "Model Compilation: âœ… Enabled\n",
            "\n",
            "âš¡ EXPECTED PERFORMANCE IMPROVEMENTS:\n",
            "â€¢ 8x larger batch sizes = Better gradient estimates\n",
            "â€¢ Mixed precision = 1.5-2x speed boost with Tensor Cores\n",
            "â€¢ Optimized data loading = 2-3x faster data pipeline\n",
            "â€¢ Memory management = Stable training with full VRAM usage\n",
            "â€¢ Total expected speedup: 5-10x faster than default settings\n",
            "\n",
            "ğŸ“Š CURRENT MEMORY USAGE:\n",
            "GPU Memory: 0.0GB allocated, 0.0GB reserved\n",
            "================================================================================\n",
            "\n",
            "ğŸ¯ HIGH-PERFORMANCE TRAINING CONFIGURATION:\n",
            "  Device: cuda\n",
            "  Batch Size: 256 (optimized for 15GB VRAM)\n",
            "  Gradient Accumulation: 4x\n",
            "  Effective Batch Size: 1024\n",
            "  Mixed Precision: âœ… Enabled\n",
            "  DataLoader Workers: 7\n",
            "  Expected Speedup: 5-10x faster than default settings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/AlgoTrading/src/utils/training_optimizer.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler() if self.use_mixed_precision else None\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Import optimized components\n",
        "from src.utils.device_manager import get_device_manager\n",
        "from src.utils.training_optimizer import get_training_optimizer\n",
        "\n",
        "# Initialize device manager\n",
        "device_manager = get_device_manager()\n",
        "device = device_manager.get_device()\n",
        "device_type = device_manager.get_device_type()\n",
        "\n",
        "# Print detailed device information\n",
        "device_manager.print_device_summary()\n",
        "\n",
        "# Initialize high-performance training optimizer\n",
        "training_optimizer = get_training_optimizer()\n",
        "performance_config = training_optimizer.get_optimized_training_config()\n",
        "\n",
        "# Print performance optimization summary\n",
        "training_optimizer.print_performance_summary()\n",
        "\n",
        "# Store optimized configuration for training\n",
        "DEVICE = device\n",
        "DEVICE_TYPE = device_type\n",
        "BATCH_SIZE = performance_config['batch_size']  # Optimized for 15GB VRAM\n",
        "GRADIENT_ACCUMULATION = performance_config['gradient_accumulation_steps']\n",
        "EFFECTIVE_BATCH_SIZE = performance_config['effective_batch_size']\n",
        "MIXED_PRECISION = performance_config['mixed_precision']\n",
        "DATALOADER_CONFIG = performance_config['dataloader_config']\n",
        "\n",
        "print(f\"\\nğŸ¯ HIGH-PERFORMANCE TRAINING CONFIGURATION:\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE} (optimized for 15GB VRAM)\")\n",
        "print(f\"  Gradient Accumulation: {GRADIENT_ACCUMULATION}x\")\n",
        "print(f\"  Effective Batch Size: {EFFECTIVE_BATCH_SIZE}\")\n",
        "print(f\"  Mixed Precision: {'âœ… Enabled' if MIXED_PRECISION else 'âŒ Disabled'}\")\n",
        "print(f\"  DataLoader Workers: {DATALOADER_CONFIG['num_workers']}\")\n",
        "print(f\"  Expected Speedup: 5-10x faster than default settings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPq4rb3U67Oc"
      },
      "source": [
        "## 4. Data Processing Pipeline\n",
        "\n",
        "Run the data processing pipeline to convert raw data into training-ready features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b7HAiB8L67Oc",
        "outputId": "aef79683-938d-412e-db27-ed0b5cb9162e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Initializing Data Processing Pipeline...\n",
            "================================================================================\n",
            "ğŸ“‚ Raw data path: data/raw\n",
            "ğŸ“‚ Final data path: data/final\n",
            "ğŸ“„ Found 60 raw data files:\n",
            "  - Sensex_5.csv\n",
            "  - Bankex_45.csv\n",
            "  - Finnifty_120.csv\n",
            "  - Bankex_15.csv\n",
            "  - Bankex_240.csv\n",
            "  ... and 55 more files\n",
            "\n",
            "ğŸš€ Running Feature Generation Pipeline...\n",
            "================================================================================\n",
            "\n",
            "âœ… Data Processing Pipeline Completed Successfully!\n",
            "================================================================================\n",
            "ğŸ“Š Summary:\n",
            "  â€¢ Total files processed: 60\n",
            "  â€¢ Total rows processed: 664,905\n",
            "  â€¢ Processing time: 9.3 minutes\n",
            "  â€¢ Output directory: data/final\n",
            "\n",
            "ğŸ“ˆ Generated 60 feature files:\n",
            "  - features_Sensex_2.csv\n",
            "  - features_Bankex_180.csv\n",
            "  - features_Finnifty_2.csv\n",
            "  - features_Finnifty_45.csv\n",
            "  - features_Bankex_2.csv\n",
            "  - features_Nifty_180.csv\n",
            "  - features_Bank_Nifty_3.csv\n",
            "  - features_Bankex_5.csv\n",
            "  - features_Finnifty_3.csv\n",
            "  - features_Sensex_240.csv\n",
            "  - features_Bank_Nifty_60.csv\n",
            "  - features_Finnifty_120.csv\n",
            "  - features_Bankex_20.csv\n",
            "  - features_Nifty_20.csv\n",
            "  - features_Nifty_2.csv\n",
            "  - features_Sensex_15.csv\n",
            "  - features_Nifty_3.csv\n",
            "  - features_Nifty_10.csv\n",
            "  - features_Sensex_180.csv\n",
            "  - features_Sensex_120.csv\n",
            "  - features_Bankex_45.csv\n",
            "  - features_Bank_Nifty_180.csv\n",
            "  - features_Finnifty_240.csv\n",
            "  - features_Nifty_240.csv\n",
            "  - features_Sensex_60.csv\n",
            "  - features_Bank_Nifty_15.csv\n",
            "  - features_Bank_Nifty_2.csv\n",
            "  - features_Bank_Nifty_120.csv\n",
            "  - features_Bank_Nifty_30.csv\n",
            "  - features_Finnifty_60.csv\n",
            "  - features_Sensex_5.csv\n",
            "  - features_Bank_Nifty_20.csv\n",
            "  - features_Finnifty_10.csv\n",
            "  - features_Finnifty_30.csv\n",
            "  - features_Bank_Nifty_45.csv\n",
            "  - features_Sensex_30.csv\n",
            "  - features_Bankex_240.csv\n",
            "  - features_Bankex_120.csv\n",
            "  - features_Finnifty_20.csv\n",
            "  - features_Bankex_10.csv\n",
            "  - features_Bankex_15.csv\n",
            "  - features_Nifty_60.csv\n",
            "  - features_Finnifty_5.csv\n",
            "  - features_Sensex_3.csv\n",
            "  - features_Nifty_15.csv\n",
            "  - features_Bankex_3.csv\n",
            "  - features_Bank_Nifty_10.csv\n",
            "  - features_Nifty_45.csv\n",
            "  - features_Bank_Nifty_240.csv\n",
            "  - features_Sensex_45.csv\n",
            "  - features_Nifty_30.csv\n",
            "  - features_Sensex_10.csv\n",
            "  - features_Finnifty_15.csv\n",
            "  - features_Bankex_60.csv\n",
            "  - features_Bank_Nifty_5.csv\n",
            "  - features_Nifty_5.csv\n",
            "  - features_Nifty_120.csv\n",
            "  - features_Bankex_30.csv\n",
            "  - features_Sensex_20.csv\n",
            "  - features_Finnifty_180.csv\n",
            "\n",
            "ğŸ¯ Data processing complete. Ready for training!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Import data processing pipeline\n",
        "from src.data_processing.pipeline import DataProcessingPipeline\n",
        "\n",
        "print(\"ğŸ”„ Initializing Data Processing Pipeline...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Initialize pipeline\n",
        "pipeline = DataProcessingPipeline()\n",
        "\n",
        "# Check if raw data exists\n",
        "raw_data_path = Path('data/raw')\n",
        "final_data_path = Path('data/final')\n",
        "\n",
        "print(f\"ğŸ“‚ Raw data path: {raw_data_path}\")\n",
        "print(f\"ğŸ“‚ Final data path: {final_data_path}\")\n",
        "\n",
        "# List raw data files\n",
        "if raw_data_path.exists():\n",
        "    raw_files = list(raw_data_path.glob(\"*.csv\"))\n",
        "    print(f\"ğŸ“„ Found {len(raw_files)} raw data files:\")\n",
        "    for file in raw_files[:5]:  # Show first 5 files\n",
        "        print(f\"  - {file.name}\")\n",
        "    if len(raw_files) > 5:\n",
        "        print(f\"  ... and {len(raw_files) - 5} more files\")\n",
        "else:\n",
        "    print(\"âš ï¸ No raw data directory found\")\n",
        "\n",
        "# Run feature generation pipeline\n",
        "print(\"\\nğŸš€ Running Feature Generation Pipeline...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    result = pipeline.run_complete_pipeline(\n",
        "        input_dir=str(raw_data_path),\n",
        "        output_dir=str(final_data_path)\n",
        "    )\n",
        "\n",
        "    if result['success']:\n",
        "        print(\"\\nâœ… Data Processing Pipeline Completed Successfully!\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"ğŸ“Š Summary:\")\n",
        "        print(f\"  â€¢ Total files processed: {result.get('total_files_processed', 'Unknown')}\")\n",
        "        print(f\"  â€¢ Total rows processed: {result.get('total_rows_processed', 0):,}\")\n",
        "        print(f\"  â€¢ Processing time: {result.get('total_time_formatted', 'Unknown')}\")\n",
        "        print(f\"  â€¢ Output directory: {result.get('output_directory', 'Unknown')}\")\n",
        "\n",
        "        # List final data files\n",
        "        if final_data_path.exists():\n",
        "            final_files = list(final_data_path.glob(\"features_*.csv\"))\n",
        "            print(f\"\\nğŸ“ˆ Generated {len(final_files)} feature files:\")\n",
        "            for file in final_files:\n",
        "                print(f\"  - {file.name}\")\n",
        "    else:\n",
        "        print(f\"âŒ Data processing failed: {result.get('error', 'Unknown error')}\")\n",
        "        raise Exception(\"Data processing pipeline failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Pipeline execution failed: {str(e)}\")\n",
        "    # Continue anyway if some data exists\n",
        "    if final_data_path.exists() and list(final_data_path.glob(\"features_*.csv\")):\n",
        "        print(\"âš ï¸ Using existing processed data files\")\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "print(\"\\nğŸ¯ Data processing complete. Ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drO9dpfp67Oc"
      },
      "source": [
        "## 5. HRM Training Configuration\n",
        "\n",
        "Configure the HRM training parameters and initialize the trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MI_L3BLG67Od",
        "outputId": "247f7cbb-73e1-43b8-eed4-0911d6c4ae1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸš€ HIGH-PERFORMANCE Training Parameters (Colab/Kaggle Optimized):\n",
            "  â€¢ epochs: 100\n",
            "  â€¢ save_frequency: 20\n",
            "  â€¢ log_frequency: 5\n",
            "  â€¢ validation_frequency: 10\n",
            "  â€¢ debug_mode: True\n",
            "  â€¢ config_path: config/hrm_config.yaml\n",
            "  â€¢ data_path: data/final\n",
            "  â€¢ memory_efficient: False\n",
            "  â€¢ high_performance: True\n",
            "  â€¢ mixed_precision: True\n",
            "  â€¢ gradient_accumulation_steps: 4\n",
            "  â€¢ dataloader_config:\n",
            "    - num_workers: 7\n",
            "    - pin_memory: True\n",
            "    - prefetch_factor: 2\n",
            "    - persistent_workers: True\n",
            "    - batch_size: 256\n",
            "  â€¢ compile_model: True\n",
            "  â€¢ cloud_optimized: True\n",
            "  â€¢ vram_target: 15GB\n",
            "  â€¢ ram_target: 12GB\n",
            "\n",
            "ğŸ“± Device Configuration:\n",
            "  â€¢ Device: cuda\n",
            "  â€¢ Device Type: gpu\n",
            "  â€¢ Batch Size: 256 (8x larger for 15GB VRAM)\n",
            "  â€¢ Effective Batch Size: 1024 (with gradient accumulation)\n",
            "\n",
            "ğŸ—ï¸ HIGH-PERFORMANCE Training Architecture:\n",
            "  â€¢ Multi-Data per Epoch: All instruments trained in each epoch\n",
            "  â€¢ Memory Strategy: Full VRAM utilization (15GB)\n",
            "  â€¢ Mixed Precision: FP16 Enabled\n",
            "  â€¢ DataLoader Workers: 7 (optimized for 12GB RAM)\n",
            "  â€¢ Expected Performance: 5-10x faster than default settings\n",
            "  â€¢ Flow: Large Batches â†’ Mixed Precision â†’ Gradient Accumulation â†’ Maximum Speed\n"
          ]
        }
      ],
      "source": [
        "# HIGH-PERFORMANCE Training parameters for Colab/Kaggle (15GB VRAM + 12GB RAM)\n",
        "TRAINING_PARAMS = {\n",
        "    'epochs': 100,  # More epochs due to faster training\n",
        "    'save_frequency': 20,  # Save checkpoint every 20 epochs\n",
        "    'log_frequency': 5,   # Log progress every 5 epochs\n",
        "    'validation_frequency': 10,  # Validate every 10 epochs\n",
        "    'debug_mode': True,   # Disable debug mode for speed\n",
        "    'config_path': 'config/hrm_config.yaml',\n",
        "    'data_path': 'data/final',\n",
        "    'memory_efficient': False,  # Disabled - we have plenty of VRAM now\n",
        "\n",
        "    # High-performance optimizations\n",
        "    'high_performance': True,\n",
        "    'mixed_precision': MIXED_PRECISION,\n",
        "    'gradient_accumulation_steps': GRADIENT_ACCUMULATION,\n",
        "    'dataloader_config': DATALOADER_CONFIG,\n",
        "    'compile_model': True,  # PyTorch 2.0+ optimization\n",
        "\n",
        "    # Optimized for cloud environments\n",
        "    'cloud_optimized': True,\n",
        "    'vram_target': '15GB',\n",
        "    'ram_target': '12GB'\n",
        "}\n",
        "\n",
        "print(f\"\\nğŸš€ HIGH-PERFORMANCE Training Parameters (Colab/Kaggle Optimized):\")\n",
        "for key, value in TRAINING_PARAMS.items():\n",
        "    if isinstance(value, dict):\n",
        "        print(f\"  â€¢ {key}:\")\n",
        "        for subkey, subvalue in value.items():\n",
        "            print(f\"    - {subkey}: {subvalue}\")\n",
        "    else:\n",
        "        print(f\"  â€¢ {key}: {value}\")\n",
        "\n",
        "print(f\"\\nğŸ“± Device Configuration:\")\n",
        "print(f\"  â€¢ Device: {DEVICE}\")\n",
        "print(f\"  â€¢ Device Type: {DEVICE_TYPE}\")\n",
        "print(f\"  â€¢ Batch Size: {BATCH_SIZE} (8x larger for 15GB VRAM)\")\n",
        "print(f\"  â€¢ Effective Batch Size: {EFFECTIVE_BATCH_SIZE} (with gradient accumulation)\")\n",
        "\n",
        "print(f\"\\nğŸ—ï¸ HIGH-PERFORMANCE Training Architecture:\")\n",
        "print(f\"  â€¢ Multi-Data per Epoch: All instruments trained in each epoch\")\n",
        "print(f\"  â€¢ Memory Strategy: Full VRAM utilization (15GB)\")\n",
        "print(f\"  â€¢ Mixed Precision: {'FP16 Enabled' if MIXED_PRECISION else 'FP32'}\")\n",
        "print(f\"  â€¢ DataLoader Workers: {DATALOADER_CONFIG['num_workers']} (optimized for 12GB RAM)\")\n",
        "print(f\"  â€¢ Expected Performance: 5-10x faster than default settings\")\n",
        "print(f\"  â€¢ Flow: Large Batches â†’ Mixed Precision â†’ Gradient Accumulation â†’ Maximum Speed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjd5iQWo67Od"
      },
      "source": [
        "## 6. Initialize HRM Training Pipeline\n",
        "\n",
        "Initialize the HRM training pipeline with automatic device detection and parallel training support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wjYYrGZy67Od",
        "outputId": "3bd681cd-1517-4964-a0ce-217c37e027ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Initializing HRM Training Pipeline (Multi-Data per Epoch)...\n",
            "================================================================================\n",
            "âœ… HRM Trainer initialized successfully!\n",
            "ğŸ“Š Available instruments: 60\n",
            "ğŸ’¾ Memory-efficient training: Loading one data file at a time\n",
            "\n",
            "ğŸ“ˆ Available Training Instruments (All will be used per epoch):\n",
            "  1. Sensex_2\n",
            "  2. Bankex_180\n",
            "  3. Finnifty_2\n",
            "  4. Finnifty_45\n",
            "  5. Bankex_2\n",
            "  6. Nifty_180\n",
            "  7. Bank_Nifty_3\n",
            "  8. Bankex_5\n",
            "  9. Finnifty_3\n",
            "  10. Sensex_240\n",
            "  11. Bank_Nifty_60\n",
            "  12. Finnifty_120\n",
            "  13. Bankex_20\n",
            "  14. Nifty_20\n",
            "  15. Nifty_2\n",
            "  16. Sensex_15\n",
            "  17. Nifty_3\n",
            "  18. Nifty_10\n",
            "  19. Sensex_180\n",
            "  20. Sensex_120\n",
            "  21. Bankex_45\n",
            "  22. Bank_Nifty_180\n",
            "  23. Finnifty_240\n",
            "  24. Nifty_240\n",
            "  25. Sensex_60\n",
            "  26. Bank_Nifty_15\n",
            "  27. Bank_Nifty_2\n",
            "  28. Bank_Nifty_120\n",
            "  29. Bank_Nifty_30\n",
            "  30. Finnifty_60\n",
            "  31. Sensex_5\n",
            "  32. Bank_Nifty_20\n",
            "  33. Finnifty_10\n",
            "  34. Finnifty_30\n",
            "  35. Bank_Nifty_45\n",
            "  36. Sensex_30\n",
            "  37. Bankex_240\n",
            "  38. Bankex_120\n",
            "  39. Finnifty_20\n",
            "  40. Bankex_10\n",
            "  41. Bankex_15\n",
            "  42. Nifty_60\n",
            "  43. Finnifty_5\n",
            "  44. Sensex_3\n",
            "  45. Nifty_15\n",
            "  46. Bankex_3\n",
            "  47. Bank_Nifty_10\n",
            "  48. Nifty_45\n",
            "  49. Bank_Nifty_240\n",
            "  50. Sensex_45\n",
            "  51. Nifty_30\n",
            "  52. Sensex_10\n",
            "  53. Finnifty_15\n",
            "  54. Bankex_60\n",
            "  55. Bank_Nifty_5\n",
            "  56. Nifty_5\n",
            "  57. Nifty_120\n",
            "  58. Bankex_30\n",
            "  59. Sensex_20\n",
            "  60. Finnifty_180\n",
            "\n",
            "ğŸ¯ Training Architecture:\n",
            "  â€¢ Per Epoch: All 60 instruments\n",
            "  â€¢ Per Instrument: Multiple episodes (1500 rows each)\n",
            "  â€¢ Memory Usage: One data file loaded at a time\n",
            "  â€¢ Training Flow: Instrument 1 â†’ All episodes â†’ Instrument 2 â†’ All episodes â†’ ... â†’ Epoch Complete\n",
            "\n",
            "ğŸ”¥ Ready to start multi-data HRM training!\n"
          ]
        }
      ],
      "source": [
        "# Import the HRM trainer directly instead of using run_training pipeline\n",
        "from src.models.hrm_trainer import HRMTrainer\n",
        "import logging\n",
        "\n",
        "# Setup enhanced logging for notebook\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler()  # Only console output for notebook\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"ğŸš€ Initializing HRM Training Pipeline (Multi-Data per Epoch)...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    # Initialize HRM trainer with memory-efficient multi-data training\n",
        "    trainer = HRMTrainer(\n",
        "        config_path=TRAINING_PARAMS['config_path'],\n",
        "        data_path=TRAINING_PARAMS['data_path'],\n",
        "        device=str(DEVICE),\n",
        "        debug_mode=TRAINING_PARAMS['debug_mode']  # Now False for multi-data\n",
        "    )\n",
        "\n",
        "    print(\"âœ… HRM Trainer initialized successfully!\")\n",
        "\n",
        "    # Check available instruments for multi-data training\n",
        "    from pathlib import Path\n",
        "    data_files = list(Path(TRAINING_PARAMS['data_path']).glob(\"features_*.csv\"))\n",
        "    available_instruments = [f.stem.replace('features_', '') for f in data_files]\n",
        "\n",
        "    print(f\"ğŸ“Š Available instruments: {len(available_instruments)}\")\n",
        "    print(\"ğŸ’¾ Memory-efficient training: Loading one data file at a time\")\n",
        "\n",
        "    # List available instruments\n",
        "    if available_instruments:\n",
        "        print(\"\\nğŸ“ˆ Available Training Instruments (All will be used per epoch):\")\n",
        "        for i, instrument in enumerate(available_instruments, 1):\n",
        "            print(f\"  {i}. {instrument}\")\n",
        "\n",
        "        print(f\"\\nğŸ¯ Training Architecture:\")\n",
        "        print(f\"  â€¢ Per Epoch: All {len(available_instruments)} instruments\")\n",
        "        print(f\"  â€¢ Per Instrument: Multiple episodes (1500 rows each)\")\n",
        "        print(f\"  â€¢ Memory Usage: One data file loaded at a time\")\n",
        "        print(f\"  â€¢ Training Flow: Instrument 1 â†’ All episodes â†’ Instrument 2 â†’ All episodes â†’ ... â†’ Epoch Complete\")\n",
        "    else:\n",
        "        raise ValueError(\"No instruments available for training\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Failed to initialize HRM trainer: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "print(\"\\nğŸ”¥ Ready to start multi-data HRM training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL9sgXV267Oe"
      },
      "source": [
        "## 7. Training Loss Visualization Setup\n",
        "\n",
        "Setup real-time visualization for training metrics and loss curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZCeLUwHP67Oe",
        "outputId": "dbbc0ae8-d0c7-49d5-fc0e-1dde5ab0ba0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Training visualizer initialized!\n",
            "ğŸ“ˆ Real-time plots will be updated during training\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "from IPython.display import display, clear_output\n",
        "import threading\n",
        "import time\n",
        "\n",
        "class HRMTrainingVisualizer:\n",
        "    \"\"\"Real-time training visualization for Jupyter notebooks\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.training_history = []\n",
        "        self.loss_history = []\n",
        "        self.reward_history = []\n",
        "        self.best_reward = float('-inf')\n",
        "\n",
        "        # Setup plots\n",
        "        self.fig = None\n",
        "        self.setup_plots()\n",
        "\n",
        "    def setup_plots(self):\n",
        "        \"\"\"Initialize the plotting framework\"\"\"\n",
        "        plt.style.use('default')\n",
        "        plt.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "    def update_metrics(self, episode, metrics):\n",
        "        \"\"\"Update training metrics\"\"\"\n",
        "        self.training_history.append({\n",
        "            'episode': episode,\n",
        "            'total_reward': metrics.get('total_reward', 0),\n",
        "            'avg_reward': metrics.get('avg_reward', 0),\n",
        "            'total_loss': metrics.get('total_loss', 0),\n",
        "            'avg_loss': metrics.get('avg_loss', 0),\n",
        "            'steps': metrics.get('steps', 0)\n",
        "        })\n",
        "\n",
        "        self.reward_history.append(metrics.get('avg_reward', 0))\n",
        "        self.loss_history.append(metrics.get('avg_loss', 0))\n",
        "\n",
        "        if metrics.get('avg_reward', 0) > self.best_reward:\n",
        "            self.best_reward = metrics.get('avg_reward', 0)\n",
        "\n",
        "    def create_training_plots(self):\n",
        "        \"\"\"Create comprehensive training plots\"\"\"\n",
        "        if len(self.training_history) < 2:\n",
        "            return\n",
        "\n",
        "        # Create subplots\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('Average Reward per Episode', 'Average Loss per Episode',\n",
        "                          'Total Reward Trend', 'Training Progress Summary'),\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "        )\n",
        "\n",
        "        episodes = [h['episode'] for h in self.training_history]\n",
        "\n",
        "        # Reward plot\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=episodes, y=self.reward_history,\n",
        "                      mode='lines+markers', name='Avg Reward',\n",
        "                      line=dict(color='green', width=2)),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # Loss plot\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=episodes, y=self.loss_history,\n",
        "                      mode='lines+markers', name='Avg Loss',\n",
        "                      line=dict(color='red', width=2)),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # Total reward trend\n",
        "        total_rewards = [h['total_reward'] for h in self.training_history]\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=episodes, y=total_rewards,\n",
        "                      mode='lines+markers', name='Total Reward',\n",
        "                      line=dict(color='blue', width=2)),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # Summary metrics\n",
        "        if len(self.reward_history) >= 10:\n",
        "            recent_avg_reward = np.mean(self.reward_history[-10:])\n",
        "            recent_avg_loss = np.mean(self.loss_history[-10:])\n",
        "        else:\n",
        "            recent_avg_reward = np.mean(self.reward_history)\n",
        "            recent_avg_loss = np.mean(self.loss_history)\n",
        "\n",
        "        # Create summary text\n",
        "        summary_text = f\"\"\"\n",
        "        ğŸ“Š Training Summary (Last 10 Episodes)\n",
        "        â€¢ Best Reward: {self.best_reward:.4f}\n",
        "        â€¢ Recent Avg Reward: {recent_avg_reward:.4f}\n",
        "        â€¢ Recent Avg Loss: {recent_avg_loss:.4f}\n",
        "        â€¢ Total Episodes: {len(self.training_history)}\n",
        "        \"\"\"\n",
        "\n",
        "        fig.add_annotation(\n",
        "            text=summary_text,\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0.75, y=0.3, xanchor='left', yanchor='top',\n",
        "            showarrow=False,\n",
        "            font=dict(size=12, family=\"monospace\"),\n",
        "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
        "            bordercolor=\"black\",\n",
        "            borderwidth=1\n",
        "        )\n",
        "\n",
        "        fig.update_layout(\n",
        "            height=800,\n",
        "            title_text=\"HRM Training Progress - Real-time Monitoring\",\n",
        "            showlegend=True\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def display_current_metrics(self):\n",
        "        \"\"\"Display current training metrics\"\"\"\n",
        "        if not self.training_history:\n",
        "            return\n",
        "\n",
        "        latest = self.training_history[-1]\n",
        "\n",
        "        print(f\"\\nğŸ“ˆ Episode {latest['episode']} Results:\")\n",
        "        print(f\"  ğŸ¯ Average Reward: {latest['avg_reward']:.4f}\")\n",
        "        print(f\"  ğŸ“‰ Average Loss: {latest['avg_loss']:.4f}\")\n",
        "        print(f\"  ğŸ® Steps Completed: {latest['steps']}\")\n",
        "        print(f\"  ğŸ† Best Reward So Far: {self.best_reward:.4f}\")\n",
        "\n",
        "        if latest['avg_reward'] == self.best_reward:\n",
        "            print(\"  ğŸŒŸ NEW BEST PERFORMANCE! ğŸŒŸ\")\n",
        "\n",
        "# Initialize visualizer\n",
        "visualizer = HRMTrainingVisualizer()\n",
        "print(\"ğŸ“Š Training visualizer initialized!\")\n",
        "print(\"ğŸ“ˆ Real-time plots will be updated during training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFUhslqh67Oe"
      },
      "source": [
        "## 8. Start HRM Training\n",
        "\n",
        "Begin the HRM training process with real-time monitoring and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVFhQlD167Oe",
        "outputId": "2f3aa3c8-1ba0-40ea-d2e5-d8e1753a8b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626,
          "referenced_widgets": [
            "175501aabf314945bb3f13c4dd99f91d",
            "0c1dd137baba4988bcfcb01cd862ea01",
            "22646573d04e4e5d862b09ce9695f0d3",
            "2e285078e3a84456958ec6d5e96f2174",
            "7b4cbd69439e4cf6afe49d374339aa97",
            "6b913efc14964d0aba6053a40efa4187",
            "121d893bdf974d0facf05c783f226d04",
            "83f621d082474f04bb008877dbd0a515",
            "f6ab6938593b4ceaaa5c48797a07c8cb",
            "836be5d8885041afb7e4a888dc0bbcd9",
            "4d94dce272af4e40b29bdb9c090e572f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting HRM Multi-Data Training Process...\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(IntProgress(value=0, bar_style='info', description='Training:', style=ProgressStyle(bar_color='â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "175501aabf314945bb3f13c4dd99f91d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Setting up multi-data training for 60 instruments...\n",
            "ğŸ“‹ Instruments: ['Sensex_2', 'Bankex_180', 'Finnifty_2', 'Finnifty_45', 'Bankex_2', 'Nifty_180', 'Bank_Nifty_3', 'Bankex_5', 'Finnifty_3', 'Sensex_240', 'Bank_Nifty_60', 'Finnifty_120', 'Bankex_20', 'Nifty_20', 'Nifty_2', 'Sensex_15', 'Nifty_3', 'Nifty_10', 'Sensex_180', 'Sensex_120', 'Bankex_45', 'Bank_Nifty_180', 'Finnifty_240', 'Nifty_240', 'Sensex_60', 'Bank_Nifty_15', 'Bank_Nifty_2', 'Bank_Nifty_120', 'Bank_Nifty_30', 'Finnifty_60', 'Sensex_5', 'Bank_Nifty_20', 'Finnifty_10', 'Finnifty_30', 'Bank_Nifty_45', 'Sensex_30', 'Bankex_240', 'Bankex_120', 'Finnifty_20', 'Bankex_10', 'Bankex_15', 'Nifty_60', 'Finnifty_5', 'Sensex_3', 'Nifty_15', 'Bankex_3', 'Bank_Nifty_10', 'Nifty_45', 'Bank_Nifty_240', 'Sensex_45', 'Nifty_30', 'Sensex_10', 'Finnifty_15', 'Bankex_60', 'Bank_Nifty_5', 'Nifty_5', 'Nifty_120', 'Bankex_30', 'Sensex_20', 'Finnifty_180']\n",
            "ğŸ“Š Calculating training steps (memory efficient)...\n",
            "ğŸ“ˆ Training Progress: 100 epochs Ã— 440 total episodes = 44000 steps\n",
            "ğŸ’¾ Memory-efficient: One data file loaded at a time\n",
            "ğŸš€ Starting multi-data training...\n",
            "\n",
            "EPOCH 1 PARALLEL TRAINING: 10 instruments\n",
            "Instruments: Sensex, Bankex, Finnifty, Finnifty, Bankex, Nifty, Bank, Bankex, Finnifty, Sensex\n",
            "Parallel training in progress - detailed step logs not shown to avoid output conflicts\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Warning: Invalid data segment in data/final/parquet/Sensex_2.parquet, Segment: 8474-10024\n",
            "Warning: Failed to load data segment for Sensex_2, falling back to full FINAL data loading. Episode: 8474-10024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Insufficient data for streaming mode, loading all FINAL data for Bankex_180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Invalid data segment in data/final/parquet/Finnifty_2.parquet, Segment: 6224-7774\n",
            "Warning: Failed to load data segment for Finnifty_2, falling back to full FINAL data loading. Episode: 6224-7774\n",
            "Warning: Invalid data segment in data/final/parquet/Finnifty_45.parquet, Segment: 227-1777\n",
            "Warning: Failed to load data segment for Finnifty_45, falling back to full FINAL data loading. Episode: 227-1777\n",
            "Warning: Invalid data segment in data/final/parquet/Bankex_2.parquet, Segment: 41255-42805\n",
            "Warning: Failed to load data segment for Bankex_2, falling back to full FINAL data loading. Episode: 41255-42805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Insufficient data for streaming mode, loading all FINAL data for Nifty_180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Invalid data segment in data/final/parquet/Bank_Nifty_3.parquet, Segment: 4658-6208\n",
            "Warning: Failed to load data segment for Bank_Nifty_3, falling back to full FINAL data loading. Episode: 4658-6208\n",
            "Warning: Invalid data segment in data/final/parquet/Bankex_5.parquet, Segment: 16416-17966\n",
            "Warning: Failed to load data segment for Bankex_5, falling back to full FINAL data loading. Episode: 16416-17966\n",
            "Warning: Invalid data segment in data/final/parquet/Finnifty_3.parquet, Segment: 16629-18179\n",
            "Warning: Failed to load data segment for Finnifty_3, falling back to full FINAL data loading. Episode: 16629-18179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Insufficient data for streaming mode, loading all FINAL data for Sensex_240\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "print(\"ğŸš€ Starting HRM Multi-Data Training Process...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create training progress widgets for multi-data training\n",
        "progress_bar = widgets.IntProgress(\n",
        "    value=0,\n",
        "    min=0,\n",
        "    max=100,  # Will be updated dynamically\n",
        "    description='Training:',\n",
        "    bar_style='info',\n",
        "    style={'bar_color': 'green'},\n",
        "    orientation='horizontal'\n",
        ")\n",
        "\n",
        "status_text = widgets.HTML(value=\"<b>Initializing multi-data training...</b>\")\n",
        "metrics_text = widgets.HTML(value=\"\")\n",
        "\n",
        "display(widgets.VBox([progress_bar, status_text, metrics_text]))\n",
        "\n",
        "# Custom training loop with visualization for multi-data\n",
        "class HRMTrainingMonitor:\n",
        "    def __init__(self, trainer, visualizer):\n",
        "        self.trainer = trainer\n",
        "        self.visualizer = visualizer\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def run_training_with_monitoring(self, epochs, available_instruments):\n",
        "        \"\"\"Run multi-data training with real-time monitoring\"\"\"\n",
        "\n",
        "        # Setup training with complete initialization for multi-data\n",
        "        print(f\"ğŸ”§ Setting up multi-data training for {len(available_instruments)} instruments...\")\n",
        "        print(f\"ğŸ“‹ Instruments: {available_instruments}\")\n",
        "\n",
        "        status_text.value = f\"<b>ğŸ¯ Multi-Data Training: {len(available_instruments)} instruments Ã— {epochs} epochs</b>\"\n",
        "\n",
        "        # Calculate total steps for progress bar (memory efficient)\n",
        "        total_steps = 0\n",
        "        print(\"ğŸ“Š Calculating training steps (memory efficient)...\")\n",
        "        for symbol in available_instruments:\n",
        "            # Quick data length check (memory efficient)\n",
        "            from src.utils.data_loader import DataLoader\n",
        "            temp_loader = DataLoader(final_data_dir=TRAINING_PARAMS['data_path'])\n",
        "            temp_data = temp_loader.load_final_data_for_symbol(symbol)\n",
        "            episodes_per_file = max(1, (len(temp_data) - 1) // 1500)  # 1500 = episode_length\n",
        "            total_steps += episodes_per_file\n",
        "            del temp_data  # Free memory immediately\n",
        "\n",
        "        total_steps *= epochs\n",
        "        progress_bar.max = total_steps\n",
        "\n",
        "        print(f\"ğŸ“ˆ Training Progress: {epochs} epochs Ã— {total_steps // epochs} total episodes = {total_steps} steps\")\n",
        "        print(\"ğŸ’¾ Memory-efficient: One data file loaded at a time\")\n",
        "\n",
        "        # Use the trainer's built-in multi-data training with progress monitoring\n",
        "        class ProgressCallback:\n",
        "            def __init__(self, progress_bar, status_text, metrics_text, visualizer):\n",
        "                self.progress_bar = progress_bar\n",
        "                self.status_text = status_text\n",
        "                self.metrics_text = metrics_text\n",
        "                self.visualizer = visualizer\n",
        "                self.step_count = 0\n",
        "\n",
        "            def update(self, epoch, instrument_idx, total_instruments, episode_metrics=None):\n",
        "                self.step_count += 1\n",
        "                self.progress_bar.value = self.step_count\n",
        "\n",
        "                current_instrument = available_instruments[instrument_idx] if instrument_idx < len(available_instruments) else \"Completing\"\n",
        "\n",
        "                self.status_text.value = f\"\"\"\n",
        "                <b>ğŸ“ˆ Epoch {epoch + 1}/{epochs} - Processing: {current_instrument}</b><br>\n",
        "                ğŸ“Š Instrument {instrument_idx + 1}/{total_instruments}<br>\n",
        "                ğŸ’¾ Memory: One data file loaded at a time\n",
        "                \"\"\"\n",
        "\n",
        "                if episode_metrics:\n",
        "                    self.visualizer.update_metrics(self.step_count, episode_metrics)\n",
        "\n",
        "                    self.metrics_text.value = f\"\"\"\n",
        "                    <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
        "                    <b>ğŸ“Š Current Metrics:</b><br>\n",
        "                    ğŸ¯ Avg Reward: <span style=\"color: green;\"><b>{episode_metrics.get('avg_reward', 0):.4f}</b></span><br>\n",
        "                    ğŸ“‰ Avg Loss: <span style=\"color: red;\"><b>{episode_metrics.get('avg_loss', 0):.4f}</b></span><br>\n",
        "                    ğŸ® Steps: {episode_metrics.get('steps', 0)}<br>\n",
        "                    ğŸ† Best Reward: <span style=\"color: blue;\"><b>{self.visualizer.best_reward:.4f}</b></span>\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "\n",
        "        # Custom progress callback (simplified for now)\n",
        "        try:\n",
        "            print(\"ğŸš€ Starting multi-data training...\")\n",
        "            training_history = self.trainer.train(\n",
        "                epochs=epochs,\n",
        "                available_instruments=available_instruments,\n",
        "                save_frequency=TRAINING_PARAMS['save_frequency'],\n",
        "                log_frequency=TRAINING_PARAMS['log_frequency']\n",
        "            )\n",
        "\n",
        "            # Update visualizer with final results\n",
        "            for i, metrics in enumerate(training_history):\n",
        "                self.visualizer.update_metrics(i + 1, metrics)\n",
        "\n",
        "            # Final status update\n",
        "            status_text.value = \"<b>âœ… Multi-data training completed successfully!</b>\"\n",
        "            progress_bar.bar_style = 'success'\n",
        "\n",
        "            return training_history\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nâš ï¸ Training interrupted by user\")\n",
        "            status_text.value = \"<b>âš ï¸ Training interrupted by user</b>\"\n",
        "            progress_bar.bar_style = 'warning'\n",
        "            return self.trainer.training_history\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Training error: {str(e)}\")\n",
        "            status_text.value = f\"<b>âŒ Training error: {str(e)}</b>\"\n",
        "            progress_bar.bar_style = 'danger'\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return self.trainer.training_history\n",
        "\n",
        "# Initialize training monitor\n",
        "monitor = HRMTrainingMonitor(trainer, visualizer)\n",
        "\n",
        "# Start multi-data training\n",
        "try:\n",
        "    training_results = monitor.run_training_with_monitoring(\n",
        "        epochs=TRAINING_PARAMS['epochs'],\n",
        "        available_instruments=available_instruments  # Train on ALL instruments per epoch\n",
        "    )\n",
        "\n",
        "    print(\"\\nâœ… Multi-data training completed successfully!\")\n",
        "    print(f\"ğŸ¯ Trained on {len(available_instruments)} instruments per epoch\")\n",
        "    print(\"ğŸ’¾ Memory-efficient: One data file loaded at a time\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Training failed: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip6OZSy167Of"
      },
      "source": [
        "## 9. Training Results Analysis\n",
        "\n",
        "Analyze the training results and evaluate model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rWPrdXk67Of"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"ğŸ“Š Analyzing Training Results...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Final training visualization\n",
        "if visualizer.training_history:\n",
        "    print(\"ğŸ“ˆ Generating Final Training Report...\")\n",
        "\n",
        "    # Create comprehensive plots\n",
        "    final_fig = visualizer.create_training_plots()\n",
        "    if final_fig:\n",
        "        final_fig.update_layout(title_text=\"HRM Training Results - Final Report\")\n",
        "        final_fig.show()\n",
        "\n",
        "    # Statistical analysis\n",
        "    rewards = visualizer.reward_history\n",
        "    losses = visualizer.loss_history\n",
        "\n",
        "    print(f\"\\nğŸ“ˆ Training Statistics:\")\n",
        "    print(f\"  ğŸ¯ Total Episodes Completed: {len(rewards)}\")\n",
        "    print(f\"  ğŸ† Best Reward Achieved: {visualizer.best_reward:.4f}\")\n",
        "    print(f\"  ğŸ“Š Final Average Reward: {rewards[-1]:.4f}\")\n",
        "    print(f\"  ğŸ“‰ Final Average Loss: {losses[-1]:.4f}\")\n",
        "\n",
        "    if len(rewards) >= 20:\n",
        "        print(f\"\\nğŸ“Š Performance Trends:\")\n",
        "        early_reward = np.mean(rewards[:10])\n",
        "        late_reward = np.mean(rewards[-10:])\n",
        "        reward_improvement = late_reward - early_reward\n",
        "\n",
        "        early_loss = np.mean(losses[:10])\n",
        "        late_loss = np.mean(losses[-10:])\n",
        "        loss_improvement = early_loss - late_loss\n",
        "\n",
        "        print(f\"  ğŸ“ˆ Reward Improvement: {reward_improvement:.4f} ({reward_improvement/early_reward*100:+.1f}%)\")\n",
        "        print(f\"  ğŸ“‰ Loss Improvement: {loss_improvement:.4f} ({loss_improvement/early_loss*100:+.1f}%)\")\n",
        "\n",
        "    # Model performance analysis\n",
        "    print(f\"\\nğŸ§  Model Performance Analysis:\")\n",
        "    if len(rewards) > 0:\n",
        "        reward_variance = np.var(rewards)\n",
        "        reward_stability = 1.0 / (1.0 + reward_variance) if reward_variance > 0 else 1.0\n",
        "        print(f\"  ğŸ“Š Reward Variance: {reward_variance:.4f}\")\n",
        "        print(f\"  ğŸ¯ Training Stability: {reward_stability:.3f}\")\n",
        "\n",
        "        # Performance rating\n",
        "        if visualizer.best_reward > 0.1:\n",
        "            performance_rating = \"ğŸŒŸ Excellent\"\n",
        "        elif visualizer.best_reward > 0.05:\n",
        "            performance_rating = \"âœ… Good\"\n",
        "        elif visualizer.best_reward > 0.0:\n",
        "            performance_rating = \"âš ï¸ Fair\"\n",
        "        else:\n",
        "            performance_rating = \"âŒ Poor\"\n",
        "\n",
        "        print(f\"  ğŸ† Overall Performance: {performance_rating}\")\n",
        "\n",
        "    # Save training summary\n",
        "    summary_data = {\n",
        "        'episode': list(range(1, len(rewards) + 1)),\n",
        "        'avg_reward': rewards,\n",
        "        'avg_loss': losses,\n",
        "        'best_reward_so_far': [max(rewards[:i+1]) for i in range(len(rewards))]\n",
        "    }\n",
        "\n",
        "    df_summary = pd.DataFrame(summary_data)\n",
        "\n",
        "    # Save to CSV\n",
        "    results_dir = Path(\"training_results\")\n",
        "    results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = results_dir / f\"hrm_training_summary_{timestamp}.csv\"\n",
        "    df_summary.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(f\"\\nğŸ’¾ Training summary saved to: {csv_path}\")\n",
        "\n",
        "    # Display final data sample\n",
        "    print(f\"\\nğŸ“‹ Training Summary (Last 10 Episodes):\")\n",
        "    print(df_summary.tail(10).to_string(index=False))\n",
        "\n",
        "else:\n",
        "    print(\"âš ï¸ No training history available\")\n",
        "\n",
        "print(\"\\nğŸ‰ Training analysis complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PomEXTd267Og"
      },
      "source": [
        "## 10. Model Evaluation and Testing\n",
        "\n",
        "Evaluate the trained HRM model on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mWoDgJC67Og"
      },
      "outputs": [],
      "source": [
        "print(\"ğŸ”¬ Evaluating Trained HRM Model...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    # Run model evaluation\n",
        "    if hasattr(trainer, 'model') and trainer.model is not None:\n",
        "        print(\"ğŸ§ª Running model evaluation on test episodes...\")\n",
        "        # Use first available instrument for evaluation\n",
        "        eval_instrument = available_instruments[0] if available_instruments else \"Bank_Nifty_5\"\n",
        "        print(f\"ğŸ“Š Evaluating on instrument: {eval_instrument}\")\n",
        "\n",
        "        eval_results = trainer.evaluate(\n",
        "            episodes=20,  # Evaluate on 20 test episodes\n",
        "            symbol=eval_instrument\n",
        "        )\n",
        "\n",
        "        print(\"\\nğŸ“Š Model Evaluation Results:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        for metric, value in eval_results.items():\n",
        "            if isinstance(value, float):\n",
        "                print(f\"  ğŸ“ˆ {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"  ğŸ“ˆ {metric.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "        # Model summary\n",
        "        print(\"\\nğŸ§  HRM Model Architecture Summary:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        model_summary = trainer.model.get_model_summary()\n",
        "\n",
        "        print(f\"  ğŸ”¢ Total Parameters: {model_summary['total_parameters']:,}\")\n",
        "        print(f\"  ğŸ“ Trainable Parameters: {model_summary['trainable_parameters']:,}\")\n",
        "        print(f\"  ğŸ§  H-Module Parameters: {model_summary['h_module_parameters']:,}\")\n",
        "        print(f\"  âš¡ L-Module Parameters: {model_summary['l_module_parameters']:,}\")\n",
        "        print(f\"  ğŸ”„ ACT Module Parameters: {model_summary['act_module_parameters']:,}\")\n",
        "        print(f\"  ğŸ“š Deep Supervision Parameters: {model_summary['deep_supervision_parameters']:,}\")\n",
        "        print(f\"  ğŸ“Š Hidden Dimension: {model_summary['hidden_dimension']}\")\n",
        "        print(f\"  ğŸ”„ H-Cycles: {model_summary['H_cycles']} | L-Cycles: {model_summary['L_cycles']}\")\n",
        "        print(f\"  ğŸ‘ï¸ H-Lookback: {model_summary['h_lookback_window']} | L-Lookback: {model_summary['l_lookback_window']}\")\n",
        "        print(f\"  ğŸ’» Device: {model_summary['device']}\")\n",
        "\n",
        "        # Checkpoint validation\n",
        "        print(\"\\nğŸ” Validating Model Checkpoints...\")\n",
        "        model_dir = Path(\"models/hrm\")\n",
        "        checkpoint_dir = Path(\"checkpoints/hrm\")\n",
        "\n",
        "        checkpoints_valid = False\n",
        "        if model_dir.exists() or checkpoint_dir.exists():\n",
        "            checkpoints_valid = True\n",
        "            print(\"âœ… Model checkpoints are valid and ready for deployment\")\n",
        "\n",
        "            # List saved models\n",
        "            if model_dir.exists():\n",
        "                model_files = list(model_dir.glob(\"*.pt\"))\n",
        "                print(f\"\\nğŸ“ Saved Models ({len(model_files)}):\")\n",
        "                for model_file in model_files:\n",
        "                    print(f\"  ğŸ’¾ {model_file.name}\")\n",
        "\n",
        "            if checkpoint_dir.exists():\n",
        "                checkpoint_files = list(checkpoint_dir.glob(\"*.pt\"))\n",
        "                print(f\"\\nğŸ“ Training Checkpoints ({len(checkpoint_files)}):\")\n",
        "                for checkpoint_file in checkpoint_files:\n",
        "                    print(f\"  ğŸ’¾ {checkpoint_file.name}\")\n",
        "        else:\n",
        "            print(\"âš ï¸ No checkpoint directories found yet\")\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ No trained model available for evaluation\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Evaluation failed: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\nğŸ¯ Model evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iINYDIkS67Og"
      },
      "source": [
        "## 11. Export Results and Model\n",
        "\n",
        "Export training results, model files, and create deployment package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlDj6a0r67Og"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"ğŸ“¦ Creating Export Package...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create export directory\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "export_dir = Path(f\"hrm_export_{timestamp}\")\n",
        "export_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"ğŸ“ Export directory: {export_dir}\")\n",
        "\n",
        "try:\n",
        "    # 1. Copy model files\n",
        "    model_dir = Path(\"models/hrm\")\n",
        "    if model_dir.exists():\n",
        "        export_models_dir = export_dir / \"models\"\n",
        "        shutil.copytree(model_dir, export_models_dir)\n",
        "        print(f\"âœ… Models exported to: {export_models_dir}\")\n",
        "\n",
        "    # 2. Copy checkpoints\n",
        "    checkpoint_dir = Path(\"checkpoints/hrm\")\n",
        "    if checkpoint_dir.exists():\n",
        "        export_checkpoints_dir = export_dir / \"checkpoints\"\n",
        "        shutil.copytree(checkpoint_dir, export_checkpoints_dir)\n",
        "        print(f\"âœ… Checkpoints exported to: {export_checkpoints_dir}\")\n",
        "\n",
        "    # 3. Copy training results\n",
        "    results_dir = Path(\"training_results\")\n",
        "    if results_dir.exists():\n",
        "        export_results_dir = export_dir / \"training_results\"\n",
        "        shutil.copytree(results_dir, export_results_dir)\n",
        "        print(f\"âœ… Training results exported to: {export_results_dir}\")\n",
        "\n",
        "    # 4. Copy configuration\n",
        "    config_files = ['config/hrm_config.yaml']\n",
        "    export_config_dir = export_dir / \"config\"\n",
        "    export_config_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    for config_file in config_files:\n",
        "        if Path(config_file).exists():\n",
        "            shutil.copy2(config_file, export_config_dir)\n",
        "            print(f\"âœ… Config exported: {config_file}\")\n",
        "\n",
        "    # 5. Create training summary report\n",
        "    summary_report = {\n",
        "        \"training_info\": {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"device_used\": str(DEVICE),\n",
        "            \"device_type\": DEVICE_TYPE,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"epochs_completed\": len(visualizer.training_history) if visualizer.training_history else 0,\n",
        "            \"selected_instrument\": available_instruments[0] if available_instruments else \"Unknown\"\n",
        "        },\n",
        "        \"training_parameters\": TRAINING_PARAMS,\n",
        "        \"performance_summary\": {\n",
        "            \"best_reward\": float(visualizer.best_reward) if visualizer.training_history else 0.0,\n",
        "            \"final_reward\": float(visualizer.reward_history[-1]) if visualizer.reward_history else 0.0,\n",
        "            \"final_loss\": float(visualizer.loss_history[-1]) if visualizer.loss_history else 0.0,\n",
        "            \"total_episodes\": len(visualizer.training_history) if visualizer.training_history else 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Add evaluation results if available\n",
        "    if 'eval_results' in locals():\n",
        "        summary_report[\"evaluation_results\"] = {\n",
        "            k: float(v) if isinstance(v, (int, float)) else v\n",
        "            for k, v in eval_results.items()\n",
        "        }\n",
        "\n",
        "    # Save summary report\n",
        "    summary_file = export_dir / \"training_summary.json\"\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(summary_report, f, indent=2)\n",
        "    print(f\"âœ… Summary report saved: {summary_file}\")\n",
        "\n",
        "    # 6. Create README for export\n",
        "    readme_content = f\"\"\"\n",
        "# HRM Trading Model Export Package\n",
        "\n",
        "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "**Training Device:** {DEVICE_TYPE.upper()}\n",
        "**Instrument:** {available_instruments[0] if available_instruments else 'Unknown'}\n",
        "\n",
        "## Contents\n",
        "\n",
        "- `models/` - Trained HRM models (best model: hrm_best_model.pt)\n",
        "- `checkpoints/` - Training checkpoints for resuming\n",
        "- `training_results/` - Training logs and metrics\n",
        "- `config/` - Model configuration files\n",
        "- `training_summary.json` - Complete training summary\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "- **Total Parameters:** {model_summary['total_parameters']:,} (â‰ˆ27M)\n",
        "- **Architecture:** Hierarchical Reasoning Model (HRM)\n",
        "- **H-Module Lookback:** {model_summary['h_lookback_window']} candles\n",
        "- **L-Module Lookback:** {model_summary['l_lookback_window']} candles\n",
        "- **Hidden Dimension:** {model_summary['hidden_dimension']}\n",
        "\n",
        "## Performance\n",
        "\n",
        "- **Best Reward:** {visualizer.best_reward:.4f}\n",
        "- **Episodes Completed:** {len(visualizer.training_history) if visualizer.training_history else 0}\n",
        "- **Training Status:** {'Completed' if len(visualizer.training_history) >= TRAINING_PARAMS['epochs'] else 'Partial'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2y74wzv67Og"
      },
      "source": [
        "## 12. Training Summary and Next Steps\n",
        "\n",
        "Final summary of the training process and recommendations for next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNgNiqzi67Oh"
      },
      "outputs": [],
      "source": [
        "print(\"ğŸŠ HRM TRAINING COMPLETED!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Final summary\n",
        "total_time = time.time() - monitor.start_time if 'monitor' in locals() else 0\n",
        "\n",
        "print(f\"\\nğŸ“Š FINAL TRAINING SUMMARY:\")\n",
        "print(f\"=\" * 50)\n",
        "print(f\"ğŸ§  Model: Hierarchical Reasoning Model (HRM)\")\n",
        "print(f\"ğŸ“ˆ Instrument: {selected_instrument if 'selected_instrument' in locals() else 'Unknown'}\")\n",
        "print(f\"ğŸ’» Device: {DEVICE_TYPE.upper()} ({DEVICE})\")\n",
        "print(f\"â±ï¸ Total Training Time: {total_time/3600:.1f} hours\")\n",
        "print(f\"ğŸ¯ Episodes Completed: {len(visualizer.training_history) if visualizer.training_history else 0}/{TRAINING_PARAMS['epochs']}\")\n",
        "\n",
        "if visualizer.training_history:\n",
        "    print(f\"ğŸ† Best Performance: {visualizer.best_reward:.4f}\")\n",
        "    print(f\"ğŸ“ˆ Final Performance: {visualizer.reward_history[-1]:.4f}\")\n",
        "    print(f\"ğŸ“‰ Final Loss: {visualizer.loss_history[-1]:.4f}\")\n",
        "\n",
        "# Model specifications\n",
        "if 'model_summary' in locals():\n",
        "    print(f\"\\nğŸ§  MODEL SPECIFICATIONS:\")\n",
        "    print(f\"=\" * 50)\n",
        "    print(f\"ğŸ“Š Total Parameters: {model_summary['total_parameters']:,}\")\n",
        "    print(f\"ğŸ”„ H-Cycles: {model_summary['H_cycles']} | L-Cycles: {model_summary['L_cycles']}\")\n",
        "    print(f\"ğŸ‘ï¸ H-Lookback: {model_summary['h_lookback_window']} | L-Lookback: {model_summary['l_lookback_window']}\")\n",
        "    print(f\"ğŸ”¢ Hidden Dimension: {model_summary['hidden_dimension']}\")\n",
        "\n",
        "# Performance evaluation\n",
        "if visualizer.training_history:\n",
        "    performance_level = \"ğŸŒŸ Excellent\" if visualizer.best_reward > 0.1 else \\\n",
        "                       \"âœ… Good\" if visualizer.best_reward > 0.05 else \\\n",
        "                       \"âš ï¸ Fair\" if visualizer.best_reward > 0.0 else \\\n",
        "                       \"âŒ Needs Improvement\"\n",
        "\n",
        "    print(f\"\\nğŸ† PERFORMANCE EVALUATION: {performance_level}\")\n",
        "\n",
        "# Next steps recommendations\n",
        "print(f\"\\nğŸš€ RECOMMENDED NEXT STEPS:\")\n",
        "print(f\"=\" * 50)\n",
        "\n",
        "if visualizer.best_reward > 0.05:\n",
        "    print(f\"âœ… Model shows good performance:\")\n",
        "    print(f\"  â€¢ Deploy model for live testing\")\n",
        "    print(f\"  â€¢ Run extended evaluation on more instruments\")\n",
        "    print(f\"  â€¢ Consider ensemble with other models\")\n",
        "elif visualizer.best_reward > 0.0:\n",
        "    print(f\"âš ï¸ Model shows moderate performance:\")\n",
        "    print(f\"  â€¢ Continue training for more epochs\")\n",
        "    print(f\"  â€¢ Experiment with different hyperparameters\")\n",
        "    print(f\"  â€¢ Try different instruments or timeframes\")\n",
        "else:\n",
        "    print(f\"âŒ Model needs improvement:\")\n",
        "    print(f\"  â€¢ Check data quality and preprocessing\")\n",
        "    print(f\"  â€¢ Adjust learning rates or architecture\")\n",
        "    print(f\"  â€¢ Consider curriculum learning approach\")\n",
        "\n",
        "print(f\"\\nğŸ“ Export package available at: {export_dir if 'export_dir' in locals() else 'Not created'}\")\n",
        "\n",
        "# Additional recommendations\n",
        "print(f\"\\nğŸ’¡ ADDITIONAL RECOMMENDATIONS:\")\n",
        "print(f\"=\" * 50)\n",
        "print(f\"ğŸ“Š â€¢ Monitor model performance on different market conditions\")\n",
        "print(f\"ğŸ”„ â€¢ Implement continuous learning for market adaptation\")\n",
        "print(f\"âš ï¸ â€¢ Add robust risk management and position sizing\")\n",
        "print(f\"ğŸ“ˆ â€¢ Backtest on historical data before live deployment\")\n",
        "print(f\"ğŸ­ â€¢ Consider distributed training for larger datasets\")\n",
        "\n",
        "print(f\"\\nğŸ¯ Training session completed successfully!\")\n",
        "print(f\"ğŸ“š Refer to the HRM research paper for implementation details\")\n",
        "print(f\"ğŸ”— Paper: 'Hierarchical Reasoning Model' by Guan Wang et al.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ‰ Thank you for using the HRM Training Notebook!\")\n",
        "print(\"=\" * 80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "175501aabf314945bb3f13c4dd99f91d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c1dd137baba4988bcfcb01cd862ea01",
              "IPY_MODEL_22646573d04e4e5d862b09ce9695f0d3",
              "IPY_MODEL_2e285078e3a84456958ec6d5e96f2174"
            ],
            "layout": "IPY_MODEL_7b4cbd69439e4cf6afe49d374339aa97"
          }
        },
        "0c1dd137baba4988bcfcb01cd862ea01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "Training:",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b913efc14964d0aba6053a40efa4187",
            "max": 44000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_121d893bdf974d0facf05c783f226d04",
            "value": 0
          }
        },
        "22646573d04e4e5d862b09ce9695f0d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83f621d082474f04bb008877dbd0a515",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f6ab6938593b4ceaaa5c48797a07c8cb",
            "value": "<b>ğŸ¯ Multi-Data Training: 60 instruments Ã— 100 epochs</b>"
          }
        },
        "2e285078e3a84456958ec6d5e96f2174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_836be5d8885041afb7e4a888dc0bbcd9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4d94dce272af4e40b29bdb9c090e572f",
            "value": ""
          }
        },
        "7b4cbd69439e4cf6afe49d374339aa97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b913efc14964d0aba6053a40efa4187": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "121d893bdf974d0facf05c783f226d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "green",
            "description_width": ""
          }
        },
        "83f621d082474f04bb008877dbd0a515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6ab6938593b4ceaaa5c48797a07c8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "836be5d8885041afb7e4a888dc0bbcd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d94dce272af4e40b29bdb9c090e572f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}