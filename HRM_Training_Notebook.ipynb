{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRM (Hierarchical Reasoning Model) Trading Training on Colab/Kaggle\n",
    "\n",
    "This notebook converts the HRM training pipeline to run on Google Colab or Kaggle notebooks with automatic TPU/GPU/CPU detection.\n",
    "\n",
    "## Overview\n",
    "- **HRM Architecture**: Brain-inspired hierarchical model with 27M parameters\n",
    "- **Training Approach**: Deep supervision with adaptive computation time\n",
    "- **Data Processing**: Automated pipeline from raw to training-ready data\n",
    "- **Hardware Support**: Automatic TPU/GPU/CPU detection with parallel training\n",
    "\n",
    "Based on the research paper: *Hierarchical Reasoning Model* by Guan Wang et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Clone Repository\n",
    "\n",
    "Clone the private GitHub repository using the service token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running in Colab: False | Kaggle: False | TPU: False\n",
      "⏭️ Local environment detected. Skipping repository cloning.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Detect runtime environment\n",
    "def is_colab():\n",
    "    return \"COLAB_GPU\" in os.environ\n",
    "\n",
    "def is_kaggle():\n",
    "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "\n",
    "def check_tpu_availability():\n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.get(\n",
    "            'http://metadata.google.internal/computeMetadata/v1/instance/attributes/accelerator-type',\n",
    "            headers={'Metadata-Flavor': 'Google'},\n",
    "            timeout=5\n",
    "        )\n",
    "        return 'tpu' in response.text.lower()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "in_cloud_env = is_colab() or is_kaggle()\n",
    "is_tpu_environment = check_tpu_availability() if in_cloud_env else False\n",
    "\n",
    "print(f\"🔍 Running in Colab: {is_colab()} | Kaggle: {is_kaggle()} | TPU: {is_tpu_environment}\")\n",
    "\n",
    "# Repo details\n",
    "repo_url = \"https://{personal_access_token}@github.com/marshaltudu14/AlgoTrading.git\"\n",
    "repo_path = \"/content/AlgoTrading\"\n",
    "\n",
    "if in_cloud_env or is_tpu_environment:\n",
    "    # Clone repository only if running in cloud\n",
    "    if not os.path.exists(repo_path):\n",
    "        print(\"📥 Cloning AlgoTrading repository...\")\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"clone\", repo_url, repo_path],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ Repository cloned successfully!\")\n",
    "        else:\n",
    "            print(f\"❌ Failed to clone repository: {result.stderr}\")\n",
    "            raise Exception(\"Repository clone failed\")\n",
    "    else:\n",
    "        print(\"✅ Repository already exists\")\n",
    "\n",
    "    # Change to repository directory\n",
    "    os.chdir(repo_path)\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "    print(f\"📁 Current directory: {os.getcwd()}\")\n",
    "    print(f\"📋 Repository contents:\")\n",
    "    for item in sorted(os.listdir('.')):\n",
    "        print(f\"  {item}\")\n",
    "else:\n",
    "    print(\"⏭️ Local environment detected. Skipping repository cloning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "Install all required packages including TPU support for Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running in Colab: False | Kaggle: False | TPU: False\n",
      "⏭️ Local environment detected. Skipping installation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect runtime environment\n",
    "def is_colab():\n",
    "    return \"COLAB_GPU\" in os.environ\n",
    "\n",
    "def is_kaggle():\n",
    "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "\n",
    "def check_tpu_availability():\n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.get(\n",
    "            'http://metadata.google.internal/computeMetadata/v1/instance/attributes/accelerator-type',\n",
    "            headers={'Metadata-Flavor': 'Google'},\n",
    "            timeout=5\n",
    "        )\n",
    "        return 'tpu' in response.text.lower()\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "in_cloud_env = is_colab() or is_kaggle()\n",
    "is_tpu_environment = check_tpu_availability() if in_cloud_env else False\n",
    "\n",
    "print(f\"🔍 Running in Colab: {is_colab()} | Kaggle: {is_kaggle()} | TPU: {is_tpu_environment}\")\n",
    "\n",
    "# Install only if in cloud (Colab/Kaggle/TPU)\n",
    "if in_cloud_env or is_tpu_environment:\n",
    "    print(\"📦 Installing base requirements...\")\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "    if is_tpu_environment:\n",
    "        print(\"🚀 Installing TPU support...\")\n",
    "        !pip install torch_xla cloud-tpu-client\n",
    "\n",
    "    print(\"📊 Installing visualization libraries...\")\n",
    "    !pip install ipywidgets plotly kaleido\n",
    "\n",
    "    print(\"✅ All dependencies installed successfully!\")\n",
    "else:\n",
    "    print(\"⏭️ Local environment detected. Skipping installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.NaN = np.nan  # Fix missing alias\n",
    "import pandas_ta as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment and set project path\n",
    "def is_colab():\n",
    "    return \"COLAB_GPU\" in os.environ\n",
    "\n",
    "def is_kaggle():\n",
    "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "\n",
    "# Determine project path based on environment\n",
    "in_cloud_env = is_colab() or is_kaggle()\n",
    "\n",
    "if in_cloud_env:\n",
    "    # Cloud environment: Use cloned repository path\n",
    "    PROJECT_PATH = \"/content/AlgoTrading\"\n",
    "    print(f\"☁️ Cloud environment detected\")\n",
    "else:\n",
    "    # Local environment: Use current directory or set custom path\n",
    "    # You can modify this path if your project is in a different location\n",
    "    PROJECT_PATH = os.getcwd()\n",
    "    print(f\"💻 Local environment detected\")\n",
    "\n",
    "# Convert to Path object for easier manipulation\n",
    "project_path = Path(PROJECT_PATH)\n",
    "\n",
    "print(f\"📁 Project path set to: {project_path}\")\n",
    "print(f\"📂 Project exists: {project_path.exists()}\")\n",
    "\n",
    "# Change to project directory if it exists\n",
    "if project_path.exists():\n",
    "    os.chdir(str(project_path))\n",
    "    print(f\"✅ Changed working directory to: {os.getcwd()}\")\n",
    "    \n",
    "    # Add project path to Python path for imports\n",
    "    if str(project_path) not in sys.path:\n",
    "        sys.path.insert(0, str(project_path))\n",
    "        print(f\"✅ Added to Python path: {project_path}\")\n",
    "    \n",
    "    # Verify key directories exist\n",
    "    key_dirs = ['src', 'config', 'data']\n",
    "    for dir_name in key_dirs:\n",
    "        dir_path = project_path / dir_name\n",
    "        if dir_path.exists():\n",
    "            print(f\"✅ Found directory: {dir_name}/\")\n",
    "        else:\n",
    "            print(f\"⚠️ Directory not found: {dir_name}/\")\n",
    "    \n",
    "    # List contents to verify\n",
    "    print(f\"\\n📋 Project contents:\")\n",
    "    for item in sorted(project_path.iterdir()):\n",
    "        if item.is_dir():\n",
    "            print(f\"  📁 {item.name}/\")\n",
    "        else:\n",
    "            print(f\"  📄 {item.name}\")\n",
    "else:\n",
    "    print(f\"❌ Project path does not exist: {project_path}\")\n",
    "    print(\"⚠️ You may need to modify PROJECT_PATH variable above\")\n",
    "\n",
    "# Store project path for use in other cells\n",
    "globals()['PROJECT_PATH'] = str(project_path)\n",
    "print(f\"\\n🎯 PROJECT_PATH variable set: {PROJECT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Device Detection and Setup\n",
    "\n",
    "Automatically detect and configure the best available device (TPU > GPU > CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import optimized components\n",
    "from src.utils.device_manager import get_device_manager\n",
    "from src.utils.training_optimizer import get_training_optimizer\n",
    "\n",
    "# Initialize device manager\n",
    "device_manager = get_device_manager()\n",
    "device = device_manager.get_device()\n",
    "device_type = device_manager.get_device_type()\n",
    "\n",
    "# Print detailed device information\n",
    "device_manager.print_device_summary()\n",
    "\n",
    "# Initialize high-performance training optimizer\n",
    "training_optimizer = get_training_optimizer()\n",
    "performance_config = training_optimizer.get_optimized_training_config()\n",
    "\n",
    "# Print performance optimization summary\n",
    "training_optimizer.print_performance_summary()\n",
    "\n",
    "# Store optimized configuration for training\n",
    "DEVICE = device\n",
    "DEVICE_TYPE = device_type\n",
    "BATCH_SIZE = performance_config['batch_size']  # Optimized for 15GB VRAM\n",
    "GRADIENT_ACCUMULATION = performance_config['gradient_accumulation_steps']\n",
    "EFFECTIVE_BATCH_SIZE = performance_config['effective_batch_size']\n",
    "MIXED_PRECISION = performance_config['mixed_precision']\n",
    "DATALOADER_CONFIG = performance_config['dataloader_config']\n",
    "\n",
    "print(f\"\\n🎯 HIGH-PERFORMANCE TRAINING CONFIGURATION:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE} (optimized for 15GB VRAM)\")\n",
    "print(f\"  Gradient Accumulation: {GRADIENT_ACCUMULATION}x\")\n",
    "print(f\"  Effective Batch Size: {EFFECTIVE_BATCH_SIZE}\")\n",
    "print(f\"  Mixed Precision: {'✅ Enabled' if MIXED_PRECISION else '❌ Disabled'}\")\n",
    "print(f\"  DataLoader Workers: {DATALOADER_CONFIG['num_workers']}\")\n",
    "print(f\"  Expected Speedup: 5-10x faster than default settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Processing Pipeline\n",
    "\n",
    "Run the data processing pipeline to convert raw data into training-ready features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 18:10:31,284 - INFO - DataProcessingPipeline initialized\n",
      "2025-08-31 18:10:31,292 - INFO - ================================================================================\n",
      "2025-08-31 18:10:31,292 - INFO - STARTING COMPLETE DATA PROCESSING PIPELINE\n",
      "2025-08-31 18:10:31,294 - INFO - ================================================================================\n",
      "2025-08-31 18:10:31,296 - INFO - STEP 1: FEATURE GENERATION\n",
      "2025-08-31 18:10:31,297 - INFO - ----------------------------------------\n",
      "2025-08-31 18:10:31,303 - INFO - Running feature generator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Initializing Data Processing Pipeline...\n",
      "================================================================================\n",
      "📂 Raw data path: data\\raw\n",
      "📂 Final data path: data\\final\n",
      "📄 Found 60 raw data files:\n",
      "  - Bankex_10.csv\n",
      "  - Bankex_120.csv\n",
      "  - Bankex_15.csv\n",
      "  - Bankex_180.csv\n",
      "  - Bankex_2.csv\n",
      "  ... and 55 more files\n",
      "\n",
      "🚀 Running Feature Generation Pipeline...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas_ta\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n",
      "2025-08-31 18:10:32,157 - INFO - Found 60 CSV files to process\n",
      "2025-08-31 18:10:32,188 - INFO - Dropped 'volume' column from Bankex_10.csv\n",
      "2025-08-31 18:10:32,197 - INFO - Loaded 10127 rows from Bankex_10.csv\n",
      "2025-08-31 18:10:32,197 - INFO - Processing in-memory DataFrame with 10127 rows...\n",
      "2025-08-31 18:10:32,462 - INFO - ✅ Enhanced datetime processing: epoch as feature, readable as index\n",
      "2025-08-31 18:10:48,523 - INFO - Removed 199 rows with NaN values. Final dataset: 9928 rows\n",
      "2025-08-31 18:10:48,527 - INFO - Generated 48 features for DataFrame\n",
      "2025-08-31 18:10:49,078 - INFO - Processed Bankex_10.csv: 9928 rows, 53 features\n",
      "2025-08-31 18:10:49,078 - INFO - Replaced existing file: data\\final\\features_Bankex_10.csv\n",
      "2025-08-31 18:10:49,083 - INFO - Dropped 'volume' column from Bankex_120.csv\n",
      "2025-08-31 18:10:49,091 - INFO - Loaded 1067 rows from Bankex_120.csv\n",
      "2025-08-31 18:10:49,093 - INFO - Processing in-memory DataFrame with 1067 rows...\n",
      "2025-08-31 18:10:49,098 - INFO - ✅ Enhanced datetime processing: epoch as feature, readable as index\n",
      "2025-08-31 18:10:50,177 - INFO - Removed 199 rows with NaN values. Final dataset: 868 rows\n",
      "2025-08-31 18:10:50,177 - INFO - Generated 48 features for DataFrame\n",
      "2025-08-31 18:10:50,230 - INFO - Processed Bankex_120.csv: 868 rows, 53 features\n",
      "2025-08-31 18:10:50,230 - INFO - Replaced existing file: data\\final\\features_Bankex_120.csv\n",
      "2025-08-31 18:10:50,249 - INFO - Dropped 'volume' column from Bankex_15.csv\n",
      "2025-08-31 18:10:50,254 - INFO - Loaded 6661 rows from Bankex_15.csv\n",
      "2025-08-31 18:10:50,255 - INFO - Processing in-memory DataFrame with 6661 rows...\n",
      "2025-08-31 18:10:50,260 - INFO - ✅ Enhanced datetime processing: epoch as feature, readable as index\n",
      "2025-08-31 18:10:58,093 - INFO - Removed 199 rows with NaN values. Final dataset: 6462 rows\n",
      "2025-08-31 18:10:58,098 - INFO - Generated 48 features for DataFrame\n",
      "2025-08-31 18:10:58,804 - INFO - Processed Bankex_15.csv: 6462 rows, 53 features\n",
      "2025-08-31 18:10:58,804 - INFO - Replaced existing file: data\\final\\features_Bankex_15.csv\n",
      "2025-08-31 18:10:58,812 - INFO - Dropped 'volume' column from Bankex_180.csv\n",
      "2025-08-31 18:10:58,817 - INFO - Loaded 802 rows from Bankex_180.csv\n",
      "2025-08-31 18:10:58,817 - INFO - Processing in-memory DataFrame with 802 rows...\n",
      "2025-08-31 18:10:58,829 - INFO - ✅ Enhanced datetime processing: epoch as feature, readable as index\n",
      "2025-08-31 18:11:00,168 - INFO - Removed 199 rows with NaN values. Final dataset: 603 rows\n",
      "2025-08-31 18:11:00,175 - INFO - Generated 48 features for DataFrame\n",
      "2025-08-31 18:11:00,319 - INFO - Processed Bankex_180.csv: 603 rows, 53 features\n",
      "2025-08-31 18:11:00,319 - INFO - Replaced existing file: data\\final\\features_Bankex_180.csv\n",
      "2025-08-31 18:11:00,417 - INFO - Dropped 'volume' column from Bankex_2.csv\n",
      "2025-08-31 18:11:00,446 - INFO - Loaded 50093 rows from Bankex_2.csv\n",
      "2025-08-31 18:11:00,450 - INFO - Processing in-memory DataFrame with 50093 rows...\n",
      "2025-08-31 18:11:00,485 - INFO - ✅ Enhanced datetime processing: epoch as feature, readable as index\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     result = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_complete_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_data_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     43\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Data Processing Pipeline Completed Successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src\\data_processing\\pipeline.py:123\u001b[39m, in \u001b[36mDataProcessingPipeline.run_complete_pipeline\u001b[39m\u001b[34m(self, input_dir, output_dir)\u001b[39m\n\u001b[32m    120\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mSTEP 1: FEATURE GENERATION\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    121\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m feature_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_feature_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feature_results[\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    127\u001b[39m         \u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    128\u001b[39m         \u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeature generation failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_results[\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    129\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mstep_failed\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mfeature_generation\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    130\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src\\data_processing\\pipeline.py:187\u001b[39m, in \u001b[36mDataProcessingPipeline.run_feature_generation\u001b[39m\u001b[34m(self, input_dir, output_dir)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# Get the processor class and run it\u001b[39;00m\n\u001b[32m    186\u001b[39m processor = feature_module.DynamicFileProcessor()\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m results = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_all_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[32m    190\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mFeature generator completed successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src/data_processing/feature_generator.py:454\u001b[39m, in \u001b[36mDynamicFileProcessor.process_all_files\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    451\u001b[39m df = \u001b[38;5;28mself\u001b[39m.load_and_validate_data(file_path)\n\u001b[32m    453\u001b[39m \u001b[38;5;66;03m# Process the dataframe\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m processed_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;66;03m# Save processed file (replace existing)\u001b[39;00m\n\u001b[32m    457\u001b[39m output_path = \u001b[38;5;28mself\u001b[39m.processed_folder / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfeatures_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src/data_processing/feature_generator.py:426\u001b[39m, in \u001b[36mDynamicFileProcessor.process_dataframe\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    423\u001b[39m close_prices = df[\u001b[33m'\u001b[39m\u001b[33mclose\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    425\u001b[39m \u001b[38;5;66;03m# Generate all features\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m features_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_all_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopen_prices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhigh_prices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_prices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_prices\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Combine with original data\u001b[39;00m\n\u001b[32m    431\u001b[39m result_df = df.join(features_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src/data_processing/feature_generator.py:264\u001b[39m, in \u001b[36mDynamicFileProcessor.generate_all_features\u001b[39m\u001b[34m(self, open_prices, high_prices, low_prices, close_prices)\u001b[39m\n\u001b[32m    254\u001b[39m     features.update({\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mbb_upper\u001b[39m\u001b[33m'\u001b[39m: bb_data[bb_upper_col],\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mbb_middle\u001b[39m\u001b[33m'\u001b[39m: bb_data[bb_middle_col],\n\u001b[32m   (...)\u001b[39m\u001b[32m    259\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mbb_position\u001b[39m\u001b[33m'\u001b[39m: (close_prices - bb_data[bb_lower_col]) / (bb_data[bb_upper_col] - bb_data[bb_lower_col]) * \u001b[32m100\u001b[39m\n\u001b[32m    260\u001b[39m     })\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# === MARKET STRUCTURE ===\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# Trend Analysis\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m trend_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmarket_structure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrend_strength\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclose_prices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m features.update(trend_data)\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# === PRICE ACTION FEATURES ===\u001b[39;00m\n\u001b[32m    268\u001b[39m \u001b[38;5;66;03m# Price changes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src/data_processing/feature_generator.py:51\u001b[39m, in \u001b[36mMarketStructureAnalyzer.trend_strength\u001b[39m\u001b[34m(close, period)\u001b[39m\n\u001b[32m     48\u001b[39m     slope = np.polyfit(x, series, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m slope\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m trend_slope = \u001b[43mclose\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrolling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalculate_slope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Trend strength based on R-squared\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_r_squared\u001b[39m(series):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:2049\u001b[39m, in \u001b[36mRolling.apply\u001b[39m\u001b[34m(self, func, raw, engine, engine_kwargs, args, kwargs)\u001b[39m\n\u001b[32m   2016\u001b[39m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[32m   2017\u001b[39m     template_header,\n\u001b[32m   2018\u001b[39m     create_section_header(\u001b[33m\"\u001b[39m\u001b[33mParameters\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2047\u001b[39m     kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2048\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m2049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:1508\u001b[39m, in \u001b[36mRollingAndExpandingMixin.apply\u001b[39m\u001b[34m(self, func, raw, engine, engine_kwargs, args, kwargs)\u001b[39m\n\u001b[32m   1505\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1506\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mengine must be either \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnumba\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcython\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1508\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapply_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapply\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnumba_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumba_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1512\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:619\u001b[39m, in \u001b[36mBaseWindow._apply\u001b[39m\u001b[34m(self, func, name, numeric_only, numba_args, **kwargs)\u001b[39m\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.method == \u001b[33m\"\u001b[39m\u001b[33msingle\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_columnwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomogeneous_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply_tablewise(homogeneous_func, name, numeric_only)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:472\u001b[39m, in \u001b[36mBaseWindow._apply_columnwise\u001b[39m\u001b[34m(self, homogeneous_func, name, numeric_only)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_numeric_only(name, numeric_only)\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomogeneous_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m obj = \u001b[38;5;28mself\u001b[39m._create_data(\u001b[38;5;28mself\u001b[39m._selected_obj, numeric_only)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# GH 12541: Special case for count where we support date-like types\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:456\u001b[39m, in \u001b[36mBaseWindow._apply_series\u001b[39m\u001b[34m(self, homogeneous_func, name)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DataError(\u001b[33m\"\u001b[39m\u001b[33mNo numeric types to aggregate\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m result = \u001b[43mhomogeneous_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m index = \u001b[38;5;28mself\u001b[39m._slice_axis_for_step(obj.index, result)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor(result, index=index, name=obj.name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:614\u001b[39m, in \u001b[36mBaseWindow._apply.<locals>.homogeneous_func\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(x, start, end, min_periods, *numba_args)\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(\u001b[38;5;28mall\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m     result = \u001b[43mcalc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:611\u001b[39m, in \u001b[36mBaseWindow._apply.<locals>.homogeneous_func.<locals>.calc\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    602\u001b[39m start, end = window_indexer.get_window_bounds(\n\u001b[32m    603\u001b[39m     num_values=\u001b[38;5;28mlen\u001b[39m(x),\n\u001b[32m    604\u001b[39m     min_periods=min_periods,\n\u001b[32m   (...)\u001b[39m\u001b[32m    607\u001b[39m     step=\u001b[38;5;28mself\u001b[39m.step,\n\u001b[32m    608\u001b[39m )\n\u001b[32m    609\u001b[39m \u001b[38;5;28mself\u001b[39m._check_window_bounds(start, end, \u001b[38;5;28mlen\u001b[39m(x))\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mnumba_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iamma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:1535\u001b[39m, in \u001b[36mRollingAndExpandingMixin._generate_cython_apply_func.<locals>.apply_func\u001b[39m\u001b[34m(values, begin, end, min_periods, raw)\u001b[39m\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw:\n\u001b[32m   1533\u001b[39m     \u001b[38;5;66;03m# GH 45912\u001b[39;00m\n\u001b[32m   1534\u001b[39m     values = Series(values, index=\u001b[38;5;28mself\u001b[39m._on, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1535\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwindow_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/window/aggregations.pyx:1422\u001b[39m, in \u001b[36mpandas._libs.window.aggregations.roll_apply\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\AlgoTrading\\src/data_processing/feature_generator.py:47\u001b[39m, in \u001b[36mMarketStructureAnalyzer.trend_strength.<locals>.calculate_slope\u001b[39m\u001b[34m(series)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(series) < \u001b[32m2\u001b[39m:\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m x = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m slope = np.polyfit(x, series, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m slope\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Import data processing pipeline\n",
    "from src.data_processing.pipeline import DataProcessingPipeline\n",
    "\n",
    "print(\"🔄 Initializing Data Processing Pipeline...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = DataProcessingPipeline()\n",
    "\n",
    "# Check if raw data exists\n",
    "raw_data_path = Path('data/raw')\n",
    "final_data_path = Path('data/final')\n",
    "\n",
    "print(f\"📂 Raw data path: {raw_data_path}\")\n",
    "print(f\"📂 Final data path: {final_data_path}\")\n",
    "\n",
    "# List raw data files\n",
    "if raw_data_path.exists():\n",
    "    raw_files = list(raw_data_path.glob(\"*.csv\"))\n",
    "    print(f\"📄 Found {len(raw_files)} raw data files:\")\n",
    "    for file in raw_files[:5]:  # Show first 5 files\n",
    "        print(f\"  - {file.name}\")\n",
    "    if len(raw_files) > 5:\n",
    "        print(f\"  ... and {len(raw_files) - 5} more files\")\n",
    "else:\n",
    "    print(\"⚠️ No raw data directory found\")\n",
    "\n",
    "# Run feature generation pipeline\n",
    "print(\"\\n🚀 Running Feature Generation Pipeline...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    result = pipeline.run_complete_pipeline(\n",
    "        input_dir=str(raw_data_path),\n",
    "        output_dir=str(final_data_path)\n",
    "    )\n",
    "    \n",
    "    if result['success']:\n",
    "        print(\"\\n✅ Data Processing Pipeline Completed Successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"📊 Summary:\")\n",
    "        print(f\"  • Total files processed: {result.get('total_files_processed', 'Unknown')}\")\n",
    "        print(f\"  • Total rows processed: {result.get('total_rows_processed', 0):,}\")\n",
    "        print(f\"  • Processing time: {result.get('total_time_formatted', 'Unknown')}\")\n",
    "        print(f\"  • Output directory: {result.get('output_directory', 'Unknown')}\")\n",
    "        \n",
    "        # List final data files\n",
    "        if final_data_path.exists():\n",
    "            final_files = list(final_data_path.glob(\"features_*.csv\"))\n",
    "            print(f\"\\n📈 Generated {len(final_files)} feature files:\")\n",
    "            for file in final_files:\n",
    "                print(f\"  - {file.name}\")\n",
    "    else:\n",
    "        print(f\"❌ Data processing failed: {result.get('error', 'Unknown error')}\")\n",
    "        raise Exception(\"Data processing pipeline failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Pipeline execution failed: {str(e)}\")\n",
    "    # Continue anyway if some data exists\n",
    "    if final_data_path.exists() and list(final_data_path.glob(\"features_*.csv\")):\n",
    "        print(\"⚠️ Using existing processed data files\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(\"\\n🎯 Data processing complete. Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HRM Training Configuration\n",
    "\n",
    "Configure the HRM training parameters and initialize the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIGH-PERFORMANCE Training parameters for Colab/Kaggle (15GB VRAM + 12GB RAM)\n",
    "TRAINING_PARAMS = {\n",
    "    'epochs': 100,  # More epochs due to faster training\n",
    "    'save_frequency': 20,  # Save checkpoint every 20 epochs\n",
    "    'log_frequency': 5,   # Log progress every 5 epochs\n",
    "    'validation_frequency': 10,  # Validate every 10 epochs\n",
    "    'debug_mode': True,   # Disable debug mode for speed\n",
    "    'config_path': 'config/hrm_config.yaml',\n",
    "    'data_path': 'data/final',\n",
    "    'memory_efficient': False,  # Disabled - we have plenty of VRAM now\n",
    "    \n",
    "    # High-performance optimizations\n",
    "    'high_performance': True,\n",
    "    'mixed_precision': MIXED_PRECISION,\n",
    "    'gradient_accumulation_steps': GRADIENT_ACCUMULATION,\n",
    "    'dataloader_config': DATALOADER_CONFIG,\n",
    "    'compile_model': True,  # PyTorch 2.0+ optimization\n",
    "    \n",
    "    # Optimized for cloud environments\n",
    "    'cloud_optimized': True,\n",
    "    'vram_target': '15GB',\n",
    "    'ram_target': '12GB'\n",
    "}\n",
    "\n",
    "print(f\"\\n🚀 HIGH-PERFORMANCE Training Parameters (Colab/Kaggle Optimized):\")\n",
    "for key, value in TRAINING_PARAMS.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  • {key}:\")\n",
    "        for subkey, subvalue in value.items():\n",
    "            print(f\"    - {subkey}: {subvalue}\")\n",
    "    else:\n",
    "        print(f\"  • {key}: {value}\")\n",
    "\n",
    "print(f\"\\n📱 Device Configuration:\")\n",
    "print(f\"  • Device: {DEVICE}\")\n",
    "print(f\"  • Device Type: {DEVICE_TYPE}\")\n",
    "print(f\"  • Batch Size: {BATCH_SIZE} (8x larger for 15GB VRAM)\")\n",
    "print(f\"  • Effective Batch Size: {EFFECTIVE_BATCH_SIZE} (with gradient accumulation)\")\n",
    "\n",
    "print(f\"\\n🏗️ HIGH-PERFORMANCE Training Architecture:\")\n",
    "print(f\"  • Multi-Data per Epoch: All instruments trained in each epoch\")\n",
    "print(f\"  • Memory Strategy: Full VRAM utilization (15GB)\")\n",
    "print(f\"  • Mixed Precision: {'FP16 Enabled' if MIXED_PRECISION else 'FP32'}\")\n",
    "print(f\"  • DataLoader Workers: {DATALOADER_CONFIG['num_workers']} (optimized for 12GB RAM)\")\n",
    "print(f\"  • Expected Performance: 5-10x faster than default settings\")\n",
    "print(f\"  • Flow: Large Batches → Mixed Precision → Gradient Accumulation → Maximum Speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize HRM Training Pipeline\n",
    "\n",
    "Initialize the HRM training pipeline with automatic device detection and parallel training support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing HRM Training Pipeline (Multi-Data per Epoch)...\n",
      "================================================================================\n",
      "✅ HRM Trainer initialized successfully!\n",
      "📊 Available instruments: 6\n",
      "💾 Memory-efficient training: Loading one data file at a time\n",
      "\n",
      "📈 Available Training Instruments (All will be used per epoch):\n",
      "  1. Bankex_10\n",
      "  2. Bankex_120\n",
      "  3. Bankex_15\n",
      "  4. Bankex_180\n",
      "  5. Bank_Nifty_5\n",
      "  6. Bank_Nifty_60\n",
      "\n",
      "🎯 Training Architecture:\n",
      "  • Per Epoch: All 6 instruments\n",
      "  • Per Instrument: Multiple episodes (1500 rows each)\n",
      "  • Memory Usage: One data file loaded at a time\n",
      "  • Training Flow: Instrument 1 → All episodes → Instrument 2 → All episodes → ... → Epoch Complete\n",
      "\n",
      "🔥 Ready to start multi-data HRM training!\n"
     ]
    }
   ],
   "source": [
    "# Import the HRM trainer directly instead of using run_training pipeline\n",
    "from src.models.hrm_trainer import HRMTrainer\n",
    "import logging\n",
    "\n",
    "# Setup enhanced logging for notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Only console output for notebook\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"🚀 Initializing HRM Training Pipeline (Multi-Data per Epoch)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Initialize HRM trainer with memory-efficient multi-data training\n",
    "    trainer = HRMTrainer(\n",
    "        config_path=TRAINING_PARAMS['config_path'],\n",
    "        data_path=TRAINING_PARAMS['data_path'],\n",
    "        device=str(DEVICE),\n",
    "        debug_mode=TRAINING_PARAMS['debug_mode']  # Now False for multi-data\n",
    "    )\n",
    "    \n",
    "    print(\"✅ HRM Trainer initialized successfully!\")\n",
    "    \n",
    "    # Check available instruments for multi-data training\n",
    "    from pathlib import Path\n",
    "    data_files = list(Path(TRAINING_PARAMS['data_path']).glob(\"features_*.csv\"))\n",
    "    available_instruments = [f.stem.replace('features_', '') for f in data_files]\n",
    "    \n",
    "    print(f\"📊 Available instruments: {len(available_instruments)}\")\n",
    "    print(\"💾 Memory-efficient training: Loading one data file at a time\")\n",
    "    \n",
    "    # List available instruments\n",
    "    if available_instruments:\n",
    "        print(\"\\n📈 Available Training Instruments (All will be used per epoch):\")\n",
    "        for i, instrument in enumerate(available_instruments, 1):\n",
    "            print(f\"  {i}. {instrument}\")\n",
    "        \n",
    "        print(f\"\\n🎯 Training Architecture:\")\n",
    "        print(f\"  • Per Epoch: All {len(available_instruments)} instruments\")\n",
    "        print(f\"  • Per Instrument: Multiple episodes (1500 rows each)\")\n",
    "        print(f\"  • Memory Usage: One data file loaded at a time\")\n",
    "        print(f\"  • Training Flow: Instrument 1 → All episodes → Instrument 2 → All episodes → ... → Epoch Complete\")\n",
    "    else:\n",
    "        raise ValueError(\"No instruments available for training\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to initialize HRM trainer: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"\\n🔥 Ready to start multi-data HRM training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loss Visualization Setup\n",
    "\n",
    "Setup real-time visualization for training metrics and loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Training visualizer initialized!\n",
      "📈 Real-time plots will be updated during training\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class HRMTrainingVisualizer:\n",
    "    \"\"\"Real-time training visualization for Jupyter notebooks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_history = []\n",
    "        self.loss_history = []\n",
    "        self.reward_history = []\n",
    "        self.best_reward = float('-inf')\n",
    "        \n",
    "        # Setup plots\n",
    "        self.fig = None\n",
    "        self.setup_plots()\n",
    "    \n",
    "    def setup_plots(self):\n",
    "        \"\"\"Initialize the plotting framework\"\"\"\n",
    "        plt.style.use('default')\n",
    "        plt.rcParams['figure.figsize'] = (15, 10)\n",
    "        \n",
    "    def update_metrics(self, episode, metrics):\n",
    "        \"\"\"Update training metrics\"\"\"\n",
    "        self.training_history.append({\n",
    "            'episode': episode,\n",
    "            'total_reward': metrics.get('total_reward', 0),\n",
    "            'avg_reward': metrics.get('avg_reward', 0),\n",
    "            'total_loss': metrics.get('total_loss', 0),\n",
    "            'avg_loss': metrics.get('avg_loss', 0),\n",
    "            'steps': metrics.get('steps', 0)\n",
    "        })\n",
    "        \n",
    "        self.reward_history.append(metrics.get('avg_reward', 0))\n",
    "        self.loss_history.append(metrics.get('avg_loss', 0))\n",
    "        \n",
    "        if metrics.get('avg_reward', 0) > self.best_reward:\n",
    "            self.best_reward = metrics.get('avg_reward', 0)\n",
    "    \n",
    "    def create_training_plots(self):\n",
    "        \"\"\"Create comprehensive training plots\"\"\"\n",
    "        if len(self.training_history) < 2:\n",
    "            return\n",
    "        \n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Average Reward per Episode', 'Average Loss per Episode', \n",
    "                          'Total Reward Trend', 'Training Progress Summary'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        episodes = [h['episode'] for h in self.training_history]\n",
    "        \n",
    "        # Reward plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=episodes, y=self.reward_history, \n",
    "                      mode='lines+markers', name='Avg Reward',\n",
    "                      line=dict(color='green', width=2)),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Loss plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=episodes, y=self.loss_history,\n",
    "                      mode='lines+markers', name='Avg Loss',\n",
    "                      line=dict(color='red', width=2)),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Total reward trend\n",
    "        total_rewards = [h['total_reward'] for h in self.training_history]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=episodes, y=total_rewards,\n",
    "                      mode='lines+markers', name='Total Reward',\n",
    "                      line=dict(color='blue', width=2)),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Summary metrics\n",
    "        if len(self.reward_history) >= 10:\n",
    "            recent_avg_reward = np.mean(self.reward_history[-10:])\n",
    "            recent_avg_loss = np.mean(self.loss_history[-10:])\n",
    "        else:\n",
    "            recent_avg_reward = np.mean(self.reward_history)\n",
    "            recent_avg_loss = np.mean(self.loss_history)\n",
    "        \n",
    "        # Create summary text\n",
    "        summary_text = f\"\"\"\n",
    "        📊 Training Summary (Last 10 Episodes)\n",
    "        • Best Reward: {self.best_reward:.4f}\n",
    "        • Recent Avg Reward: {recent_avg_reward:.4f}\n",
    "        • Recent Avg Loss: {recent_avg_loss:.4f}\n",
    "        • Total Episodes: {len(self.training_history)}\n",
    "        \"\"\"\n",
    "        \n",
    "        fig.add_annotation(\n",
    "            text=summary_text,\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.75, y=0.3, xanchor='left', yanchor='top',\n",
    "            showarrow=False,\n",
    "            font=dict(size=12, family=\"monospace\"),\n",
    "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "            bordercolor=\"black\",\n",
    "            borderwidth=1\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title_text=\"HRM Training Progress - Real-time Monitoring\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def display_current_metrics(self):\n",
    "        \"\"\"Display current training metrics\"\"\"\n",
    "        if not self.training_history:\n",
    "            return\n",
    "            \n",
    "        latest = self.training_history[-1]\n",
    "        \n",
    "        print(f\"\\n📈 Episode {latest['episode']} Results:\")\n",
    "        print(f\"  🎯 Average Reward: {latest['avg_reward']:.4f}\")\n",
    "        print(f\"  📉 Average Loss: {latest['avg_loss']:.4f}\")\n",
    "        print(f\"  🎮 Steps Completed: {latest['steps']}\")\n",
    "        print(f\"  🏆 Best Reward So Far: {self.best_reward:.4f}\")\n",
    "        \n",
    "        if latest['avg_reward'] == self.best_reward:\n",
    "            print(\"  🌟 NEW BEST PERFORMANCE! 🌟\")\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = HRMTrainingVisualizer()\n",
    "print(\"📊 Training visualizer initialized!\")\n",
    "print(\"📈 Real-time plots will be updated during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start HRM Training\n",
    "\n",
    "Begin the HRM training process with real-time monitoring and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting HRM Multi-Data Training Process...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6057b59b1047d7a7abfab4eb507ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, bar_style='info', description='Training:', style=ProgressStyle(bar_color='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 18:14:21,019 - INFO - Looking for data for symbol: Bankex_10 in directory: data/final\n",
      "2025-08-31 18:14:21,020 - INFO - Checking file: data/final\\features_Bankex_10.csv\n",
      "2025-08-31 18:14:21,021 - INFO - File exists: data/final\\features_Bankex_10.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setting up multi-data training for 6 instruments...\n",
      "📋 Instruments: ['Bankex_10', 'Bankex_120', 'Bankex_15', 'Bankex_180', 'Bank_Nifty_5', 'Bank_Nifty_60']\n",
      "📊 Calculating training steps (memory efficient)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 18:14:21,338 - INFO - Loaded final data for Bankex_10: 9928 rows from features_Bankex_10.csv\n",
      "2025-08-31 18:14:21,350 - INFO - Looking for data for symbol: Bankex_120 in directory: data/final\n",
      "2025-08-31 18:14:21,350 - INFO - Checking file: data/final\\features_Bankex_120.csv\n",
      "2025-08-31 18:14:21,350 - INFO - File exists: data/final\\features_Bankex_120.csv\n",
      "2025-08-31 18:14:21,384 - INFO - Loaded final data for Bankex_120: 868 rows from features_Bankex_120.csv\n",
      "2025-08-31 18:14:21,396 - INFO - Looking for data for symbol: Bankex_15 in directory: data/final\n",
      "2025-08-31 18:14:21,397 - INFO - Checking file: data/final\\features_Bankex_15.csv\n",
      "2025-08-31 18:14:21,400 - INFO - File exists: data/final\\features_Bankex_15.csv\n",
      "2025-08-31 18:14:21,540 - INFO - Loaded final data for Bankex_15: 6462 rows from features_Bankex_15.csv\n",
      "2025-08-31 18:14:21,550 - INFO - Looking for data for symbol: Bankex_180 in directory: data/final\n",
      "2025-08-31 18:14:21,550 - INFO - Checking file: data/final\\features_Bankex_180.csv\n",
      "2025-08-31 18:14:21,555 - INFO - File exists: data/final\\features_Bankex_180.csv\n",
      "2025-08-31 18:14:21,581 - INFO - Loaded final data for Bankex_180: 603 rows from features_Bankex_180.csv\n",
      "2025-08-31 18:14:21,592 - INFO - Looking for data for symbol: Bank_Nifty_5 in directory: data/final\n",
      "2025-08-31 18:14:21,592 - INFO - Checking file: data/final\\features_Bank_Nifty_5.csv\n",
      "2025-08-31 18:14:21,592 - INFO - File exists: data/final\\features_Bank_Nifty_5.csv\n",
      "2025-08-31 18:14:21,627 - INFO - Loaded final data for Bank_Nifty_5: 729 rows from features_Bank_Nifty_5.csv\n",
      "2025-08-31 18:14:21,640 - INFO - Looking for data for symbol: Bank_Nifty_60 in directory: data/final\n",
      "2025-08-31 18:14:21,642 - INFO - Checking file: data/final\\features_Bank_Nifty_60.csv\n",
      "2025-08-31 18:14:21,644 - INFO - File exists: data/final\\features_Bank_Nifty_60.csv\n",
      "2025-08-31 18:14:21,667 - INFO - Loaded final data for Bank_Nifty_60: 536 rows from features_Bank_Nifty_60.csv\n",
      "2025-08-31 18:14:21,678 - INFO - ✅ Configuration loaded from config/settings.yaml\n",
      "2025-08-31 18:14:21,683 - INFO - Looking for data for symbol: Bankex_10 in directory: data/final\n",
      "2025-08-31 18:14:21,683 - INFO - Checking file: data/final\\features_Bankex_10.csv\n",
      "2025-08-31 18:14:21,683 - INFO - File exists: data/final\\features_Bankex_10.csv\n",
      "2025-08-31 18:14:21,797 - INFO - Loaded final data for Bankex_10: 9928 rows from features_Bankex_10.csv\n",
      "2025-08-31 18:14:21,809 - INFO - Loaded HRM configuration from config/hrm_config.yaml\n",
      "2025-08-31 18:14:21,826 - INFO - ✅ Configuration loaded from config/settings.yaml\n",
      "2025-08-31 18:14:21,837 - INFO - ✅ Configuration loaded from config/settings.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Training Progress: 50 epochs × 14 total episodes = 700 steps\n",
      "💾 Memory-efficient: One data file loaded at a time\n",
      "🚀 Starting multi-data training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 18:14:21,865 - INFO - ✅ Configuration loaded from config/settings.yaml\n",
      "2025-08-31 18:14:21,897 - INFO - Parsed symbol: Bankex, timeframe: 10\n",
      "2025-08-31 18:14:21,897 - INFO - Resolved market context: Bankex (ID: 2), 10 (ID: 3)\n",
      "2025-08-31 18:14:21,902 - INFO - Episode data: rows 0 to 1499 (1500 rows)\n",
      "2025-08-31 18:14:22,490 - INFO - ✅ Configuration loaded from config/settings.yaml\n",
      "2025-08-31 18:14:22,526 - INFO - HRM Agent initialized: H-lookback=100, L-lookback=15, Hidden=512, H-cycles=2, L-cycles=6\n",
      "2025-08-31 18:14:22,530 - INFO - HRM Trading Agent initialized with 47787052 parameters\n",
      "2025-08-31 18:14:22,534 - INFO - Training setup completed for 6 instruments\n",
      "2025-08-31 18:14:22,536 - INFO - Memory-efficient mode: Data files loaded one at a time during training\n",
      "2025-08-31 18:14:22,543 - INFO - Starting HRM training for 50 epochs\n",
      "2025-08-31 18:14:22,544 - INFO - Training on 6 instruments per epoch\n",
      "2025-08-31 18:14:22,544 - INFO - Debug mode: ON\n",
      "2025-08-31 18:14:22,544 - INFO - Device: cpu\n",
      "2025-08-31 18:14:22,547 - INFO - Calculating total episodes (loading data headers only)...\n",
      "2025-08-31 18:14:22,548 - INFO - Looking for data for symbol: Bankex_10 in directory: data/final\n",
      "2025-08-31 18:14:22,554 - INFO - Checking file: data/final\\features_Bankex_10.csv\n",
      "2025-08-31 18:14:22,557 - INFO - File exists: data/final\\features_Bankex_10.csv\n",
      "2025-08-31 18:14:22,712 - INFO - Loaded final data for Bankex_10: 9928 rows from features_Bankex_10.csv\n",
      "2025-08-31 18:14:22,714 - INFO - Looking for data for symbol: Bankex_120 in directory: data/final\n",
      "2025-08-31 18:14:22,717 - INFO - Checking file: data/final\\features_Bankex_120.csv\n",
      "2025-08-31 18:14:22,718 - INFO - File exists: data/final\\features_Bankex_120.csv\n",
      "2025-08-31 18:14:22,747 - INFO - Loaded final data for Bankex_120: 868 rows from features_Bankex_120.csv\n",
      "2025-08-31 18:14:22,750 - INFO - Looking for data for symbol: Bankex_15 in directory: data/final\n",
      "2025-08-31 18:14:22,751 - INFO - Checking file: data/final\\features_Bankex_15.csv\n",
      "2025-08-31 18:14:22,756 - INFO - File exists: data/final\\features_Bankex_15.csv\n",
      "2025-08-31 18:14:22,863 - INFO - Loaded final data for Bankex_15: 6462 rows from features_Bankex_15.csv\n",
      "2025-08-31 18:14:22,864 - INFO - Looking for data for symbol: Bankex_180 in directory: data/final\n",
      "2025-08-31 18:14:22,865 - INFO - Checking file: data/final\\features_Bankex_180.csv\n",
      "2025-08-31 18:14:22,871 - INFO - File exists: data/final\\features_Bankex_180.csv\n",
      "2025-08-31 18:14:22,902 - INFO - Loaded final data for Bankex_180: 603 rows from features_Bankex_180.csv\n",
      "2025-08-31 18:14:22,902 - INFO - Looking for data for symbol: Bank_Nifty_5 in directory: data/final\n",
      "2025-08-31 18:14:22,907 - INFO - Checking file: data/final\\features_Bank_Nifty_5.csv\n",
      "2025-08-31 18:14:22,911 - INFO - File exists: data/final\\features_Bank_Nifty_5.csv\n",
      "2025-08-31 18:14:22,942 - INFO - Loaded final data for Bank_Nifty_5: 729 rows from features_Bank_Nifty_5.csv\n",
      "2025-08-31 18:14:22,944 - INFO - Looking for data for symbol: Bank_Nifty_60 in directory: data/final\n",
      "2025-08-31 18:14:22,944 - INFO - Checking file: data/final\\features_Bank_Nifty_60.csv\n",
      "2025-08-31 18:14:22,947 - INFO - File exists: data/final\\features_Bank_Nifty_60.csv\n",
      "2025-08-31 18:14:22,976 - INFO - Loaded final data for Bank_Nifty_60: 536 rows from features_Bank_Nifty_60.csv\n",
      "2025-08-31 18:14:22,977 - INFO - Total training steps: 50 epochs × 14 episodes = 700 steps\n",
      "2025-08-31 18:14:22,982 - INFO - Memory-efficient training: Loading only one data file at a time\n",
      "2025-08-31 18:14:22,982 - INFO - Starting Epoch 1/50\n",
      "2025-08-31 18:14:22,982 - INFO - Loading data file: Bankex_10\n",
      "2025-08-31 18:14:23,033 - INFO - Looking for data for symbol: Bankex_10 in directory: data/final\n",
      "2025-08-31 18:14:23,039 - INFO - Checking file: data/final\\features_Bankex_10.csv\n",
      "2025-08-31 18:14:23,041 - INFO - File exists: data/final\\features_Bankex_10.csv\n",
      "2025-08-31 18:14:23,352 - INFO - Loaded final data for Bankex_10: 9928 rows from features_Bankex_10.csv\n",
      "2025-08-31 18:14:23,353 - INFO - Processing Bankex_10: 6 episodes (9928 rows)\n",
      "2025-08-31 18:14:23,364 - INFO - Memory usage: One data file loaded (2.7 MB)\n",
      "2025-08-31 18:14:23,365 - INFO - Parsed symbol: Bankex, timeframe: 10\n",
      "2025-08-31 18:14:23,367 - INFO - Resolved market context: Bankex (ID: 2), 10 (ID: 3)\n",
      "2025-08-31 18:14:23,372 - INFO - Episode data: rows 0 to 1499 (1500 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 STEP-BY-STEP: Bankex (10min)\n",
      "Step | Instrument     | Timeframe | DateTime           | Initial Cap | Current Cap | Action   | Win%  | P&L      | Current Price | Entry   | Target Price(Pts) | SL Price(Pts)   | Reward | Reason\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "   1 | Bankex         | 10min     | 2024-03-27 12:45:00 | Rs100,000 | Rs100,000 | HOLD     |   0.0% | Rs      - | Rs53023.83 |      -      |         -        |         -        | +0.000 | -\n",
      "   2 | Bankex         | 10min     | 2024-03-27 12:55:00 | Rs100,000 | Rs100,000 | HOLD     |   0.0% | Rs      - | Rs53066.58 |      -      |         -        |         -        | +0.000 | -\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "print(\"🚀 Starting HRM Multi-Data Training Process...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create training progress widgets for multi-data training\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,  # Will be updated dynamically\n",
    "    description='Training:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': 'green'},\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "status_text = widgets.HTML(value=\"<b>Initializing multi-data training...</b>\")\n",
    "metrics_text = widgets.HTML(value=\"\")\n",
    "\n",
    "display(widgets.VBox([progress_bar, status_text, metrics_text]))\n",
    "\n",
    "# Custom training loop with visualization for multi-data\n",
    "class HRMTrainingMonitor:\n",
    "    def __init__(self, trainer, visualizer):\n",
    "        self.trainer = trainer\n",
    "        self.visualizer = visualizer\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def run_training_with_monitoring(self, epochs, available_instruments):\n",
    "        \"\"\"Run multi-data training with real-time monitoring\"\"\"\n",
    "        \n",
    "        # Setup training with complete initialization for multi-data\n",
    "        print(f\"🔧 Setting up multi-data training for {len(available_instruments)} instruments...\")\n",
    "        print(f\"📋 Instruments: {available_instruments}\")\n",
    "        \n",
    "        status_text.value = f\"<b>🎯 Multi-Data Training: {len(available_instruments)} instruments × {epochs} epochs</b>\"\n",
    "        \n",
    "        # Calculate total steps for progress bar (memory efficient)\n",
    "        total_steps = 0\n",
    "        print(\"📊 Calculating training steps (memory efficient)...\")\n",
    "        for symbol in available_instruments:\n",
    "            # Quick data length check (memory efficient)\n",
    "            from src.utils.data_loader import DataLoader\n",
    "            temp_loader = DataLoader(final_data_dir=TRAINING_PARAMS['data_path'])\n",
    "            temp_data = temp_loader.load_final_data_for_symbol(symbol)\n",
    "            episodes_per_file = max(1, (len(temp_data) - 1) // 1500)  # 1500 = episode_length\n",
    "            total_steps += episodes_per_file\n",
    "            del temp_data  # Free memory immediately\n",
    "        \n",
    "        total_steps *= epochs\n",
    "        progress_bar.max = total_steps\n",
    "        \n",
    "        print(f\"📈 Training Progress: {epochs} epochs × {total_steps // epochs} total episodes = {total_steps} steps\")\n",
    "        print(\"💾 Memory-efficient: One data file loaded at a time\")\n",
    "        \n",
    "        # Use the trainer's built-in multi-data training with progress monitoring\n",
    "        class ProgressCallback:\n",
    "            def __init__(self, progress_bar, status_text, metrics_text, visualizer):\n",
    "                self.progress_bar = progress_bar\n",
    "                self.status_text = status_text\n",
    "                self.metrics_text = metrics_text\n",
    "                self.visualizer = visualizer\n",
    "                self.step_count = 0\n",
    "                \n",
    "            def update(self, epoch, instrument_idx, total_instruments, episode_metrics=None):\n",
    "                self.step_count += 1\n",
    "                self.progress_bar.value = self.step_count\n",
    "                \n",
    "                current_instrument = available_instruments[instrument_idx] if instrument_idx < len(available_instruments) else \"Completing\"\n",
    "                \n",
    "                self.status_text.value = f\"\"\"\n",
    "                <b>📈 Epoch {epoch + 1}/{epochs} - Processing: {current_instrument}</b><br>\n",
    "                📊 Instrument {instrument_idx + 1}/{total_instruments}<br>\n",
    "                💾 Memory: One data file loaded at a time\n",
    "                \"\"\"\n",
    "                \n",
    "                if episode_metrics:\n",
    "                    self.visualizer.update_metrics(self.step_count, episode_metrics)\n",
    "                    \n",
    "                    self.metrics_text.value = f\"\"\"\n",
    "                    <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
    "                    <b>📊 Current Metrics:</b><br>\n",
    "                    🎯 Avg Reward: <span style=\"color: green;\"><b>{episode_metrics.get('avg_reward', 0):.4f}</b></span><br>\n",
    "                    📉 Avg Loss: <span style=\"color: red;\"><b>{episode_metrics.get('avg_loss', 0):.4f}</b></span><br>\n",
    "                    🎮 Steps: {episode_metrics.get('steps', 0)}<br>\n",
    "                    🏆 Best Reward: <span style=\"color: blue;\"><b>{self.visualizer.best_reward:.4f}</b></span>\n",
    "                    </div>\n",
    "                    \"\"\"\n",
    "        \n",
    "        # Custom progress callback (simplified for now)\n",
    "        try:\n",
    "            print(\"🚀 Starting multi-data training...\")\n",
    "            training_history = self.trainer.train(\n",
    "                epochs=epochs,\n",
    "                available_instruments=available_instruments,\n",
    "                save_frequency=TRAINING_PARAMS['save_frequency'],\n",
    "                log_frequency=TRAINING_PARAMS['log_frequency']\n",
    "            )\n",
    "            \n",
    "            # Update visualizer with final results\n",
    "            for i, metrics in enumerate(training_history):\n",
    "                self.visualizer.update_metrics(i + 1, metrics)\n",
    "            \n",
    "            # Final status update\n",
    "            status_text.value = \"<b>✅ Multi-data training completed successfully!</b>\"\n",
    "            progress_bar.bar_style = 'success'\n",
    "                \n",
    "            return training_history\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n⚠️ Training interrupted by user\")\n",
    "            status_text.value = \"<b>⚠️ Training interrupted by user</b>\"\n",
    "            progress_bar.bar_style = 'warning'\n",
    "            return self.trainer.training_history\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Training error: {str(e)}\")\n",
    "            status_text.value = f\"<b>❌ Training error: {str(e)}</b>\"\n",
    "            progress_bar.bar_style = 'danger'\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return self.trainer.training_history\n",
    "\n",
    "# Initialize training monitor\n",
    "monitor = HRMTrainingMonitor(trainer, visualizer)\n",
    "\n",
    "# Start multi-data training\n",
    "try:\n",
    "    training_results = monitor.run_training_with_monitoring(\n",
    "        epochs=TRAINING_PARAMS['epochs'],\n",
    "        available_instruments=available_instruments  # Train on ALL instruments per epoch\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ Multi-data training completed successfully!\")\n",
    "    print(f\"🎯 Trained on {len(available_instruments)} instruments per epoch\")\n",
    "    print(\"💾 Memory-efficient: One data file loaded at a time\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Results Analysis\n",
    "\n",
    "Analyze the training results and evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"📊 Analyzing Training Results...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Final training visualization\n",
    "if visualizer.training_history:\n",
    "    print(\"📈 Generating Final Training Report...\")\n",
    "    \n",
    "    # Create comprehensive plots\n",
    "    final_fig = visualizer.create_training_plots()\n",
    "    if final_fig:\n",
    "        final_fig.update_layout(title_text=\"HRM Training Results - Final Report\")\n",
    "        final_fig.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    rewards = visualizer.reward_history\n",
    "    losses = visualizer.loss_history\n",
    "    \n",
    "    print(f\"\\n📈 Training Statistics:\")\n",
    "    print(f\"  🎯 Total Episodes Completed: {len(rewards)}\")\n",
    "    print(f\"  🏆 Best Reward Achieved: {visualizer.best_reward:.4f}\")\n",
    "    print(f\"  📊 Final Average Reward: {rewards[-1]:.4f}\")\n",
    "    print(f\"  📉 Final Average Loss: {losses[-1]:.4f}\")\n",
    "    \n",
    "    if len(rewards) >= 20:\n",
    "        print(f\"\\n📊 Performance Trends:\")\n",
    "        early_reward = np.mean(rewards[:10])\n",
    "        late_reward = np.mean(rewards[-10:])\n",
    "        reward_improvement = late_reward - early_reward\n",
    "        \n",
    "        early_loss = np.mean(losses[:10])\n",
    "        late_loss = np.mean(losses[-10:])\n",
    "        loss_improvement = early_loss - late_loss\n",
    "        \n",
    "        print(f\"  📈 Reward Improvement: {reward_improvement:.4f} ({reward_improvement/early_reward*100:+.1f}%)\")\n",
    "        print(f\"  📉 Loss Improvement: {loss_improvement:.4f} ({loss_improvement/early_loss*100:+.1f}%)\")\n",
    "    \n",
    "    # Model performance analysis\n",
    "    print(f\"\\n🧠 Model Performance Analysis:\")\n",
    "    if len(rewards) > 0:\n",
    "        reward_variance = np.var(rewards)\n",
    "        reward_stability = 1.0 / (1.0 + reward_variance) if reward_variance > 0 else 1.0\n",
    "        print(f\"  📊 Reward Variance: {reward_variance:.4f}\")\n",
    "        print(f\"  🎯 Training Stability: {reward_stability:.3f}\")\n",
    "        \n",
    "        # Performance rating\n",
    "        if visualizer.best_reward > 0.1:\n",
    "            performance_rating = \"🌟 Excellent\"\n",
    "        elif visualizer.best_reward > 0.05:\n",
    "            performance_rating = \"✅ Good\"\n",
    "        elif visualizer.best_reward > 0.0:\n",
    "            performance_rating = \"⚠️ Fair\"\n",
    "        else:\n",
    "            performance_rating = \"❌ Poor\"\n",
    "            \n",
    "        print(f\"  🏆 Overall Performance: {performance_rating}\")\n",
    "    \n",
    "    # Save training summary\n",
    "    summary_data = {\n",
    "        'episode': list(range(1, len(rewards) + 1)),\n",
    "        'avg_reward': rewards,\n",
    "        'avg_loss': losses,\n",
    "        'best_reward_so_far': [max(rewards[:i+1]) for i in range(len(rewards))]\n",
    "    }\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_dir = Path(\"training_results\")\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_path = results_dir / f\"hrm_training_summary_{timestamp}.csv\"\n",
    "    df_summary.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\n💾 Training summary saved to: {csv_path}\")\n",
    "    \n",
    "    # Display final data sample\n",
    "    print(f\"\\n📋 Training Summary (Last 10 Episodes):\")\n",
    "    print(df_summary.tail(10).to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ No training history available\")\n",
    "\n",
    "print(\"\\n🎉 Training analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Testing\n",
    "\n",
    "Evaluate the trained HRM model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔬 Evaluating Trained HRM Model...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Run model evaluation\n",
    "    if hasattr(trainer, 'model') and trainer.model is not None:\n",
    "        print(\"🧪 Running model evaluation on test episodes...\")\n",
    "        \n",
    "        eval_results = trainer.evaluate(\n",
    "            episodes=20,  # Evaluate on 20 test episodes\n",
    "            symbol=selected_instrument\n",
    "        )\n",
    "        \n",
    "        print(\"\\n📊 Model Evaluation Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for metric, value in eval_results.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  📈 {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  📈 {metric.replace('_', ' ').title()}: {value}\")\n",
    "        \n",
    "        # Model summary\n",
    "        print(\"\\n🧠 HRM Model Architecture Summary:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        model_summary = trainer.model.get_model_summary()\n",
    "        \n",
    "        print(f\"  🔢 Total Parameters: {model_summary['total_parameters']:,}\")\n",
    "        print(f\"  🎓 Trainable Parameters: {model_summary['trainable_parameters']:,}\")\n",
    "        print(f\"  🧠 H-Module Parameters: {model_summary['h_module_parameters']:,}\")\n",
    "        print(f\"  ⚡ L-Module Parameters: {model_summary['l_module_parameters']:,}\")\n",
    "        print(f\"  🔄 ACT Module Parameters: {model_summary['act_module_parameters']:,}\")\n",
    "        print(f\"  📚 Deep Supervision Parameters: {model_summary['deep_supervision_parameters']:,}\")\n",
    "        print(f\"  📊 Hidden Dimension: {model_summary['hidden_dimension']}\")\n",
    "        print(f\"  🔄 H-Cycles: {model_summary['H_cycles']} | L-Cycles: {model_summary['L_cycles']}\")\n",
    "        print(f\"  👁️ H-Lookback: {model_summary['h_lookback_window']} | L-Lookback: {model_summary['l_lookback_window']}\")\n",
    "        print(f\"  💻 Device: {model_summary['device']}\")\n",
    "        \n",
    "        # Checkpoint validation\n",
    "        print(\"\\n🔍 Validating Model Checkpoints...\")\n",
    "        model_dir = Path(\"models/hrm\")\n",
    "        checkpoint_dir = Path(\"checkpoints/hrm\")\n",
    "        \n",
    "        checkpoints_valid = False\n",
    "        if model_dir.exists() or checkpoint_dir.exists():\n",
    "            checkpoints_valid = True\n",
    "            print(\"✅ Model checkpoints are valid and ready for deployment\")\n",
    "            \n",
    "            # List saved models\n",
    "            if model_dir.exists():\n",
    "                model_files = list(model_dir.glob(\"*.pt\"))\n",
    "                print(f\"\\n📁 Saved Models ({len(model_files)}):\")\n",
    "                for model_file in model_files:\n",
    "                    print(f\"  💾 {model_file.name}\")\n",
    "            \n",
    "            if checkpoint_dir.exists():\n",
    "                checkpoint_files = list(checkpoint_dir.glob(\"*.pt\"))\n",
    "                print(f\"\\n📁 Training Checkpoints ({len(checkpoint_files)}):\")\n",
    "                for checkpoint_file in checkpoint_files:\n",
    "                    print(f\"  💾 {checkpoint_file.name}\")\n",
    "        else:\n",
    "            print(\"⚠️ No checkpoint directories found yet\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️ No trained model available for evaluation\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Evaluation failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n🎯 Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results and Model\n",
    "\n",
    "Export training results, model files, and create deployment package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"📦 Creating Export Package...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create export directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "export_dir = Path(f\"hrm_export_{timestamp}\")\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"📁 Export directory: {export_dir}\")\n",
    "\n",
    "try:\n",
    "    # 1. Copy model files\n",
    "    model_dir = Path(\"models/hrm\")\n",
    "    if model_dir.exists():\n",
    "        export_models_dir = export_dir / \"models\"\n",
    "        shutil.copytree(model_dir, export_models_dir)\n",
    "        print(f\"✅ Models exported to: {export_models_dir}\")\n",
    "    \n",
    "    # 2. Copy checkpoints\n",
    "    checkpoint_dir = Path(\"checkpoints/hrm\")\n",
    "    if checkpoint_dir.exists():\n",
    "        export_checkpoints_dir = export_dir / \"checkpoints\"\n",
    "        shutil.copytree(checkpoint_dir, export_checkpoints_dir)\n",
    "        print(f\"✅ Checkpoints exported to: {export_checkpoints_dir}\")\n",
    "    \n",
    "    # 3. Copy training results\n",
    "    results_dir = Path(\"training_results\")\n",
    "    if results_dir.exists():\n",
    "        export_results_dir = export_dir / \"training_results\"\n",
    "        shutil.copytree(results_dir, export_results_dir)\n",
    "        print(f\"✅ Training results exported to: {export_results_dir}\")\n",
    "    \n",
    "    # 4. Copy configuration\n",
    "    config_files = ['config/hrm_config.yaml']\n",
    "    export_config_dir = export_dir / \"config\"\n",
    "    export_config_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for config_file in config_files:\n",
    "        if Path(config_file).exists():\n",
    "            shutil.copy2(config_file, export_config_dir)\n",
    "            print(f\"✅ Config exported: {config_file}\")\n",
    "    \n",
    "    # 5. Create training summary report\n",
    "    summary_report = {\n",
    "        \"training_info\": {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"device_used\": str(DEVICE),\n",
    "            \"device_type\": DEVICE_TYPE,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epochs_completed\": len(visualizer.training_history) if visualizer.training_history else 0,\n",
    "            \"selected_instrument\": selected_instrument if 'selected_instrument' in locals() else \"Unknown\"\n",
    "        },\n",
    "        \"training_parameters\": TRAINING_PARAMS,\n",
    "        \"performance_summary\": {\n",
    "            \"best_reward\": float(visualizer.best_reward) if visualizer.training_history else 0.0,\n",
    "            \"final_reward\": float(visualizer.reward_history[-1]) if visualizer.reward_history else 0.0,\n",
    "            \"final_loss\": float(visualizer.loss_history[-1]) if visualizer.loss_history else 0.0,\n",
    "            \"total_episodes\": len(visualizer.training_history) if visualizer.training_history else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add evaluation results if available\n",
    "    if 'eval_results' in locals():\n",
    "        summary_report[\"evaluation_results\"] = {\n",
    "            k: float(v) if isinstance(v, (int, float)) else v \n",
    "            for k, v in eval_results.items()\n",
    "        }\n",
    "    \n",
    "    # Save summary report\n",
    "    summary_file = export_dir / \"training_summary.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2)\n",
    "    print(f\"✅ Summary report saved: {summary_file}\")\n",
    "    \n",
    "    # 6. Create README for export\n",
    "    readme_content = f\"\"\"\n",
    "# HRM Trading Model Export Package\n",
    "\n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Training Device:** {DEVICE_TYPE.upper()}\n",
    "**Instrument:** {selected_instrument if 'selected_instrument' in locals() else 'Unknown'}\n",
    "\n",
    "## Contents\n",
    "\n",
    "- `models/` - Trained HRM models (best model: hrm_best_model.pt)\n",
    "- `checkpoints/` - Training checkpoints for resuming\n",
    "- `training_results/` - Training logs and metrics\n",
    "- `config/` - Model configuration files\n",
    "- `training_summary.json` - Complete training summary\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "- **Total Parameters:** {model_summary['total_parameters']:,} (≈27M)\n",
    "- **Architecture:** Hierarchical Reasoning Model (HRM)\n",
    "- **H-Module Lookback:** {model_summary['h_lookback_window']} candles\n",
    "- **L-Module Lookback:** {model_summary['l_lookback_window']} candles\n",
    "- **Hidden Dimension:** {model_summary['hidden_dimension']}\n",
    "\n",
    "## Performance\n",
    "\n",
    "- **Best Reward:** {visualizer.best_reward:.4f}\n",
    "- **Episodes Completed:** {len(visualizer.training_history) if visualizer.training_history else 0}\n",
    "- **Training Status:** {'Completed' if len(visualizer.training_history) >= TRAINING_PARAMS['epochs'] else 'Partial'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Summary and Next Steps\n",
    "\n",
    "Final summary of the training process and recommendations for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎊 HRM TRAINING COMPLETED!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Final summary\n",
    "total_time = time.time() - monitor.start_time if 'monitor' in locals() else 0\n",
    "\n",
    "print(f\"\\n📊 FINAL TRAINING SUMMARY:\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"🧠 Model: Hierarchical Reasoning Model (HRM)\")\n",
    "print(f\"📈 Instrument: {selected_instrument if 'selected_instrument' in locals() else 'Unknown'}\")\n",
    "print(f\"💻 Device: {DEVICE_TYPE.upper()} ({DEVICE})\")\n",
    "print(f\"⏱️ Total Training Time: {total_time/3600:.1f} hours\")\n",
    "print(f\"🎯 Episodes Completed: {len(visualizer.training_history) if visualizer.training_history else 0}/{TRAINING_PARAMS['epochs']}\")\n",
    "\n",
    "if visualizer.training_history:\n",
    "    print(f\"🏆 Best Performance: {visualizer.best_reward:.4f}\")\n",
    "    print(f\"📈 Final Performance: {visualizer.reward_history[-1]:.4f}\")\n",
    "    print(f\"📉 Final Loss: {visualizer.loss_history[-1]:.4f}\")\n",
    "\n",
    "# Model specifications\n",
    "if 'model_summary' in locals():\n",
    "    print(f\"\\n🧠 MODEL SPECIFICATIONS:\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"📊 Total Parameters: {model_summary['total_parameters']:,}\")\n",
    "    print(f\"🔄 H-Cycles: {model_summary['H_cycles']} | L-Cycles: {model_summary['L_cycles']}\")\n",
    "    print(f\"👁️ H-Lookback: {model_summary['h_lookback_window']} | L-Lookback: {model_summary['l_lookback_window']}\")\n",
    "    print(f\"🔢 Hidden Dimension: {model_summary['hidden_dimension']}\")\n",
    "\n",
    "# Performance evaluation\n",
    "if visualizer.training_history:\n",
    "    performance_level = \"🌟 Excellent\" if visualizer.best_reward > 0.1 else \\\n",
    "                       \"✅ Good\" if visualizer.best_reward > 0.05 else \\\n",
    "                       \"⚠️ Fair\" if visualizer.best_reward > 0.0 else \\\n",
    "                       \"❌ Needs Improvement\"\n",
    "    \n",
    "    print(f\"\\n🏆 PERFORMANCE EVALUATION: {performance_level}\")\n",
    "\n",
    "# Next steps recommendations\n",
    "print(f\"\\n🚀 RECOMMENDED NEXT STEPS:\")\n",
    "print(f\"=\" * 50)\n",
    "\n",
    "if visualizer.best_reward > 0.05:\n",
    "    print(f\"✅ Model shows good performance:\")\n",
    "    print(f\"  • Deploy model for live testing\")\n",
    "    print(f\"  • Run extended evaluation on more instruments\")\n",
    "    print(f\"  • Consider ensemble with other models\")\n",
    "elif visualizer.best_reward > 0.0:\n",
    "    print(f\"⚠️ Model shows moderate performance:\")\n",
    "    print(f\"  • Continue training for more epochs\")\n",
    "    print(f\"  • Experiment with different hyperparameters\")\n",
    "    print(f\"  • Try different instruments or timeframes\")\n",
    "else:\n",
    "    print(f\"❌ Model needs improvement:\")\n",
    "    print(f\"  • Check data quality and preprocessing\")\n",
    "    print(f\"  • Adjust learning rates or architecture\")\n",
    "    print(f\"  • Consider curriculum learning approach\")\n",
    "\n",
    "print(f\"\\n📁 Export package available at: {export_dir if 'export_dir' in locals() else 'Not created'}\")\n",
    "\n",
    "# Additional recommendations\n",
    "print(f\"\\n💡 ADDITIONAL RECOMMENDATIONS:\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"📊 • Monitor model performance on different market conditions\")\n",
    "print(f\"🔄 • Implement continuous learning for market adaptation\")\n",
    "print(f\"⚠️ • Add robust risk management and position sizing\")\n",
    "print(f\"📈 • Backtest on historical data before live deployment\")\n",
    "print(f\"🏭 • Consider distributed training for larger datasets\")\n",
    "\n",
    "print(f\"\\n🎯 Training session completed successfully!\")\n",
    "print(f\"📚 Refer to the HRM research paper for implementation details\")\n",
    "print(f\"🔗 Paper: 'Hierarchical Reasoning Model' by Guan Wang et al.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🎉 Thank you for using the HRM Training Notebook!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "accelerator": "GPU",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
