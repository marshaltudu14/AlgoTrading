# HRM (Hierarchical Reasoning Model) Configuration for Algorithmic Trading

# Model Architecture Configuration
model_architecture:
  name: "HRMTradingAgent"
  
  # Core HRM Parameters
  hidden_dim: 256
  H_cycles: 2          # High-level strategic cycles per forward pass
  L_cycles: 5          # Low-level tactical cycles per H-cycle
  H_layers: 4          # Number of transformer layers in H-module
  L_layers: 3          # Number of transformer layers in L-module
  num_heads: 8         # Multi-head attention heads
  dropout: 0.1
  
  # Adaptive Computation Time (ACT)
  halt_max_steps: 8
  halt_exploration_prob: 0.1
  
  # Deep Supervision
  deep_supervision_segments: 4
  segment_weights: [0.4, 0.3, 0.2, 0.1]  # Weights for different supervision segments

# Hierarchical Processing Configuration - SIMPLIFIED
hierarchical_processing:
  # Lookback windows - key difference between modules
  high_level_lookback: 100        # H-module sees 100 candles for strategic context
  low_level_lookback: 15          # L-module sees 15 candles for tactical decisions
  
  # Both modules see ALL available features - no hardcoded column filtering
  # The HRM will naturally learn which features are relevant for each module
  # Market regime detection emerges from the data and lookback windows

# Trading Strategy Configuration - Use existing settings.yaml values
# These will be loaded from settings.yaml to avoid duplication

# Training Configuration
training:
  # Curriculum Learning
  curriculum_phases:
    phase_1:
      name: "basic_trading"
      episodes: 1000
      H_cycles: 1
      L_cycles: 3
      tasks: ["trend_following"]
      
    phase_2:
      name: "advanced_strategies"
      episodes: 2000
      H_cycles: 2
      L_cycles: 5
      tasks: ["trend_following", "mean_reversion"]
      
    phase_3:
      name: "full_hrm_trading"
      episodes: 3000
      H_cycles: 2
      L_cycles: 5
      tasks: ["trend_following", "mean_reversion", "breakout_trading"]
  
  # Multi-Task Learning
  task_distribution:
    trend_following: 0.4
    mean_reversion: 0.3
    breakout_trading: 0.2
    risk_management_scenarios: 0.1
  
  # Loss Function Weights
  loss_weights:
    strategic_loss: 0.3       # H-module strategic decisions
    tactical_loss: 0.4        # L-module tactical decisions
    act_loss: 0.1            # Adaptive computation time
    consistency_loss: 0.1     # H-L coordination
    performance_loss: 0.1     # Overall trading performance
  
  # Learning Rates
  learning_rates:
    base_lr: 0.0001
    strategic_lr: 0.00005     # Slower learning for strategic decisions
    tactical_lr: 0.0001       # Faster learning for tactical decisions
    act_lr: 0.0002           # Fast adaptation for ACT
  
  # Optimization
  optimizer: "AdamW"
  weight_decay: 0.01
  gradient_clip: 1.0
  
  # Training Schedule
  warmup_episodes: 100
  lr_schedule: "cosine"
  min_lr_ratio: 0.1

# Environment Integration
environment:
  # HRM-specific environment settings
  enable_hierarchical_observations: true
  strategic_update_frequency: 10   # Update strategic context every N steps
  
  # Observation Processing - Dynamic feature handling
  observation_preprocessing:
    normalize_features: true
    include_temporal_features: true
    # No hardcoded feature exclusions - let HRM see all data

# Performance Monitoring
monitoring:
  # HRM-specific metrics
  hierarchical_metrics:
    h_module_accuracy: true
    l_module_precision: true
    h_l_coordination: true
    act_efficiency: true
    
  # Traditional trading metrics
  trading_metrics:
    sharpe_ratio: true
    maximum_drawdown: true
    win_rate: true
    profit_factor: true
    calmar_ratio: true
    
  # Real-time monitoring
  log_frequency: 100           # Log every N episodes
  save_frequency: 1000         # Save model every N episodes
  
  # Performance thresholds
  performance_thresholds:
    min_sharpe_ratio: 1.0
    max_drawdown: 0.15
    min_win_rate: 0.45
    min_profit_factor: 1.2

# Model Checkpointing and Deployment
deployment:
  # Model saving
  save_best_model: true
  save_intermediate_checkpoints: true
  checkpoint_frequency: 500
  
  # Model versioning
  model_version: "1.0.0"
  experiment_name: "HRM_Trading_v1"
  
  # Production deployment
  production_config:
    enable_act: true             # Use adaptive computation time in production
    max_computation_time: 100    # Maximum computation time (ms)
    fallback_to_simple: true     # Fallback to simple model if HRM fails
    
  # Risk controls
  production_risk_controls:
    max_single_trade_risk: 0.02  # Maximum risk per single trade
    max_daily_trades: 10         # Maximum trades per day
    emergency_stop_loss: 0.05    # Emergency stop if losses exceed this threshold

# Debugging and Development
debug:
  enable_detailed_logging: false
  save_attention_weights: false
  save_intermediate_states: false
  visualize_regime_classification: true
  track_hierarchical_convergence: true

# Hardware Configuration
hardware:
  device: "cuda"               # cuda, cpu, mps
  mixed_precision: true        # Use mixed precision training
  compile_model: false         # Use torch.compile (requires PyTorch 2.0+)
  
# Data Configuration
data:
  # HRM-specific data requirements
  min_sequence_length: 200     # Minimum sequence length for hierarchical reasoning
  strategic_sequence_length: 100
  tactical_sequence_length: 50
  
  # Feature engineering for HRM
  feature_engineering:
    hierarchical_features: true  # Generate features for different levels
    regime_indicators: true      # Include regime classification features
    multi_timeframe: true        # Include multiple timeframe analysis
    
  # Data validation
  validation:
    check_hierarchical_consistency: true
    validate_temporal_ordering: true
    ensure_sufficient_lookback: true