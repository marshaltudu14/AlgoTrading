---
id: story-template-v2
name: Story Document
version: 2.0
---

# Story 2.3: PPO Agent Implementation

## Status
Draft

## Story
**As an** RL engineer,
**I want** a baseline Proximal Policy Optimization (PPO) agent,
**so that** I can establish a performance benchmark and begin training the RL model.

## Acceptance Criteria
1. `src/agents/ppo_agent.py` contains a class `PPOAgent` that inherits from `BaseAgent`.
2. `PPOAgent` initializes with an `LSTMModel` for its policy and value networks.
3. It implements the `select_action` method to choose actions based on the current policy.
4. It implements the `learn` method to update the policy and value networks using the PPO algorithm.
5. The agent can be initialized and interact with the `TradingEnv` (from Epic 1) by selecting actions and receiving experiences.

## Tasks / Subtasks
- [ ] Task 1 (AC: 1, 2): Implement the `PPOAgent` class structure.
    - [ ] Subtask 1.1: Create the file `src/agents/ppo_agent.py`.
    - [ ] Subtask 1.2: Define the `PPOAgent` class, inheriting from `BaseAgent`.
    - [ ] Subtask 1.3: Implement the `__init__` method to accept hyperparameters (e.g., `lr`, `gamma`, `epsilon_clip`) and initialize actor and critic networks using `LSTMModel`. [Source: `docs/architecture/rl-model-architecture-detailed.md#3-srcagentsppo_agentpy---ppo-agent-implementation`]
- [ ] Task 2 (AC: 3): Implement the `select_action` method.
    - [ ] Subtask 2.1: Implement the `select_action` method to take an observation, pass it through the actor network, and sample an action from the resulting probability distribution.
- [ ] Task 3 (AC: 4): Implement the `learn` method.
    - [ ] Subtask 3.1: Implement the `learn` method to process a batch of experiences, calculate advantages (e.g., using GAE), and perform the PPO update for a specified number of epochs.
- [ ] Task 4 (AC: 5): Write unit tests for the `PPOAgent`.
    - [ ] Subtask 4.1: Add test cases to `tests/test_agents.py`.
    - [ ] Subtask 4.2: Write a test case to verify that the agent can be initialized correctly.
    - [ ] Subtask 4.3: Write a test case to ensure `select_action` returns a valid action.
    - [ ] Subtask 4.4: Write a test case for the `learn` method to ensure it runs without errors and updates the network weights.

## Dev Notes
- **File Location**: `src/agents/ppo_agent.py` [Source: `docs/architecture/source-tree.md#6-source-directory-src`]
- **Class**: `PPOAgent`
- **Base Class**: `BaseAgent`
- **Responsibilities**: This class provides a concrete implementation of the PPO algorithm. It will be the first agent used for training and benchmarking. [Source: `docs/architecture/rl-model-architecture-detailed.md#3-srcagentsppo_agentpy---ppo-agent-implementation`]

### Testing
- Unit tests should be added to the existing `tests/test_agents.py` file.
- Tests should mock the `TradingEnv` and focus on the agent's internal logic.

## QA Results
