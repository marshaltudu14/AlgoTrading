---
id: story-template-v2
name: Story Document
version: 2.0
---

# Story 5.1: Standard Trading Performance Metrics

## Status
Draft

## Story
**As an** RL engineer,
**I want** to calculate standard trading performance metrics,
**so that** I can rigorously evaluate the profitability and risk of the RL agent's strategies.

## Acceptance Criteria
1. `src/utils/metrics.py` contains functions to calculate:
    - Sharpe Ratio
    - Total P&L (absolute profit generated)
    - Profit Factor (ratio of gross profit to gross loss)
    - Maximum Drawdown
    - Win Rate (percentage of profitable trades)
    - Average P&L per Trade
    - Number of Trades
2. These functions take a trade history (e.g., a list of trade objects or a DataFrame from the `BacktestingEngine`) as input.
3. All metrics are calculated correctly and handle edge cases (e.g., no trades, all losing trades).

## Tasks / Subtasks
- [ ] Task 1 (AC: 1, 2, 3): Implement the performance metrics functions.
    - [ ] Subtask 1.1: Create the file `src/utils/metrics.py`.
    - [ ] Subtask 1.2: Implement functions for each of the required metrics. [Source: `docs/architecture/rl-model-architecture-detailed.md#1-srcutilsmetricspy---standard-trading-performance-metrics`]
- [ ] Task 2: Write unit tests for the metrics functions.
    - [ ] Subtask 2.1: Add test cases to `tests/test_utils.py`.
    - [ ] Subtask 2.2: For each metric, write test cases with known inputs and expected outputs.
    - [ ] Subtask 2.3: Write test cases to handle edge cases like no trades or all losing trades.

## Dev Notes
- **File Location**: `src/utils/metrics.py` [Source: `docs/architecture/source-tree.md#6-source-directory-src`]
- **Responsibilities**: This module provides the tools to evaluate the performance of the trading agents. [Source: `docs/architecture/rl-model-architecture-detailed.md#1-srcutilsmetricspy---standard-trading-performance-metrics`]

### Testing
- Unit tests should be added to the existing `tests/test_utils.py` file.

## QA Results
