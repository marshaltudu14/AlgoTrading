---
id: story-template-v2
name: Story Document
version: 2.0
---

# Story 4.2: Inner Loop (Adaptation) Implementation

## Status
Draft

## Story
**As an** RL engineer,
**I want** to implement the inner loop of the MAML algorithm,
**so that** the RL agent can quickly adapt its policy to a specific sampled task.

## Acceptance Criteria
1. The `BaseAgent` (or a new MAML-specific base class) is updated to support temporary, task-specific parameter updates without affecting the global meta-parameters.
2. The `learn` method of the RL agents (PPO and MoE) can perform a specified number of gradient steps on a given task's data, effectively adapting the agent's policy to that task.
3. This adaptation process is isolated for each task within the inner loop.

## Tasks / Subtasks
- [ ] Task 1 (AC: 1, 2, 3): Implement the inner loop adaptation.
    - [ ] Subtask 1.1: Enhance `BaseAgent` to support creating a temporary, differentiable copy of its parameters.
    - [ ] Subtask 1.2: Modify the `learn` method of the agents to perform adaptation on the temporary parameters. [Source: `docs/architecture/rl-model-architecture-detailed.md#2-srcagentsbase_agentpy---inner-loop-adaptation-implementation`]
- [ ] Task 2: Write unit tests for the inner loop adaptation.
    - [ ] Subtask 2.1: Add test cases to `tests/test_agents.py`.
    - [ ] Subtask 2.2: Write a test case to verify that the adaptation process updates the temporary parameters but not the global meta-parameters.

## Dev Notes
- **File Location**: `src/agents/base_agent.py`, `src/agents/ppo_agent.py`, `src/agents/moe_agent.py` [Source: `docs/architecture/source-tree.md#6-source-directory-src`]
- **Responsibilities**: This is the core of the MAML algorithm, where the agent learns how to learn. [Source: `docs/architecture/rl-model-architecture-detailed.md#2-srcagentsbase_agentpy---inner-loop-adaptation-implementation`]

### Testing
- Unit tests should be added to the existing `tests/test_agents.py` file.

## QA Results
