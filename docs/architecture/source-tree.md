# Project Source Tree

This document outlines the standard directory structure for the Autonomous Trading Bot project. Adhering to this structure ensures consistency, maintainability, and ease of navigation for all team members and automated processes.

```
C:/AlgoTrading/
├───.git/                       # Git version control metadata
├───.gemini/                    # Gemini-specific configuration and memory
├───.bmad-core/                 # BMad-Method core agent definitions, tasks, templates, etc.
├───data/
│   ├───raw/                    # Raw, unprocessed historical market data (CSV files)
│   ├───processed/              # Intermediate processed data (e.g., features, reasoning data)
│   │   └───embeddings/         # Directory for storing pre-computed text embeddings
│   └───final/                  # Final, training-ready datasets (features + reasoning)
├───docs/
│   ├───mainIdea.md             # High-level project vision and technical specification
│   ├───architecture/           # Architectural documentation
│   │   ├───coding-standards.md # Project-wide coding guidelines
│   │   ├───tech-stack.md       # Overview of technologies used
│   │   └───source-tree.md      # This document: project directory structure
│   └───stories/                # User stories and epic breakdowns
├───logs/                       # Application and pipeline execution logs
├───models/
│   ├───hf_tokenizer/           # Hugging Face tokenizer files
│   ├───hf_model/               # Hugging Face model files
│   ├───label_encoder.joblib    # Saved LabelEncoder for target variable
│   ├───scaler.joblib           # Saved StandardScaler for numerical features
│   └───supervised_model.joblib # Saved supervised machine learning model
├───reports/
│   ├───pipeline/               # Reports generated by the data processing pipeline
│   └───quality/                # Quality reports for reasoning data and model performance
├───scripts/
│   ├───data_processing/        # Scripts related to data ingestion and transformation
│   ├───testing/                # Scripts for running tests (unit, integration, end-to-end)
│   └───organize_project.py     # Utility script for project setup/organization
│   └───live_predictor.py       # Script for live market prediction using trained model (to be created)
├───src/
│   ├───__init__.py             # Python package initializer
│   ├───auth/                   # Authentication related modules (if applicable)
│   ├───config/                 # Configuration files (e.g., config.py)
│   ├───data_processing/        # Modules for data cleaning, feature engineering, and pipeline orchestration
│   │   ├───feature_generator.py# Generates technical indicators and market features
│   │   ├───pipeline.py         # Orchestrates data processing steps
│   │   └───reasoning_processor.py # Processes features to add reasoning
│   ├───reasoning_system/       # Core modules for the AI reasoning engine
│   │   ├───core/               # Core logic and orchestrators
│   │   ├───engines/            # Different reasoning engines
│   │   ├───generators/         # Natural language generation for reasoning
│   │   ├───managers/           # Manages reasoning components
│   │   ├───templates/          # Templates for reasoning narratives
│   │   └───validators/         # Validates reasoning quality
│   └───supervised_model/       # Modules for supervised learning models
│       ├───generate_embeddings.py # Generates embeddings for text data
│       └───model_pipeline.py   # Trains, evaluates, and saves supervised models
├───temp/                       # Temporary files or backups
├───web-bundles/                # Web-compatible bundles for agents (if applicable)
├───.gitignore                  # Specifies intentionally untracked files to ignore
├───data_processing_pipeline.log # Log file for data processing pipeline
├───DEVELOPER_TASK_PROCESS_SUMMARY.md # Summary of developer tasks
├───DEVELOPER_TASK_PROCESS.md   # Detailed developer task process
├───fyers_docs.txt              # Documentation related to Fyers API
├───read-data.ipynb             # Jupyter notebook for data exploration
├───reasoning_processor.log     # Log file for reasoning processor
└───requirements.txt            # Python dependencies
```

## 1. Top-Level Directories

*   `.git/`: Standard Git repository directory.
*   `.gemini/`: Configuration and memory specific to the Gemini environment.
*   `.bmad-core/`: Contains the core definitions for the BMad-Method framework, including agents, tasks, templates, and checklists.
*   `data/`: Stores all data related to the project, categorized by processing stage.
*   `docs/`: Contains all project documentation, including architectural specifications, user stories, and high-level ideas.
*   `logs/`: Centralized location for all application and pipeline logs.
*   `models/`: Stores trained machine learning models, preprocessors, and Hugging Face model assets.
*   `reports/`: Contains various reports generated by the system, such as pipeline summaries and quality assessments.
*   `scripts/`: Houses standalone executable scripts for various operations (e.g., data processing, testing, utility functions).
*   `src/`: Contains the core source code of the trading bot, organized into logical modules.
*   `temp/`: A temporary directory for transient files or backups.
*   `web-bundles/`: Contains web-compatible bundles for agents, used in web UI environments.

## 2. Data Directory (`data/`)

*   `raw/`: For raw, untouched historical market data, typically in CSV format.
*   `processed/`: For intermediate data generated during the data processing pipeline, such as feature sets before reasoning is applied.
    *   `embeddings/`: Specifically for storing pre-computed numerical embeddings of text data (e.g., from the reasoning engine).
*   `final/`: For the final, cleaned, and enriched datasets ready for model training.

## 3. Docs Directory (`docs/`)

*   `mainIdea.md`: The overarching vision and high-level technical specification for the autonomous trading bot.
*   `architecture/`: Dedicated to architectural documentation.
    *   `coding-standards.md`: Defines the coding conventions and quality guidelines.
    *   `tech-stack.md`: Lists and describes the technologies and frameworks used.
    *   `source-tree.md`: This document, detailing the project's directory structure.
*   `stories/`: Contains individual user stories and epic breakdowns, guiding development tasks.

## 4. Models Directory (`models/`)

*   `hf_tokenizer/`: Stores the files for the Hugging Face tokenizer used for text processing.
*   `hf_model/`: Stores the files for the Hugging Face model used for generating text embeddings.
*   `label_encoder.joblib`: The serialized `LabelEncoder` object used for transforming target variables.
*   `scaler.joblib`: The serialized `StandardScaler` object used for scaling numerical features.
*   `supervised_model.joblib`: The serialized trained supervised machine learning model.

## 5. Reports Directory (`reports/`)

*   `pipeline/`: Contains reports generated by the data processing pipeline, summarizing its execution and outcomes.
*   `quality/`: Stores reports related to the quality of generated reasoning data and the performance of trained models.

## 6. Scripts Directory (`scripts/`)

*   `data_processing/`: Scripts specifically for data ingestion, transformation, or one-off data tasks.
*   `testing/`: Contains scripts for running various types of tests (unit, integration, end-to-end).
*   `organize_project.py`: A utility script for initial project setup, directory creation, or file organization.
*   `live_predictor.py`: (To be created) A script responsible for loading the trained model and making predictions on live market data.

## 7. Source Directory (`src/`)

*   `__init__.py`: Marks `src` as a Python package.
*   `auth/`: (Optional) Modules related to authentication and authorization.
*   `config/`: Contains configuration files and settings for the application.
*   `data_processing/`: Core modules for handling data-related operations.
    *   `feature_generator.py`: Implements the logic for generating technical indicators and other market features.
    *   `pipeline.py`: Orchestrates the entire data processing workflow.
    *   `reasoning_processor.py`: Integrates with the reasoning system to add human-like trading reasoning to data.
*   `reasoning_system/`: Contains the sophisticated AI reasoning engine.
    *   `core/`: Core logic, orchestrators, and main components of the reasoning system.
    *   `engines/`: Different reasoning engines or algorithms.
    *   `generators/`: Modules for generating natural language reasoning narratives.
    *   `managers/`: Manages the lifecycle and interaction of reasoning components.
    *   `templates/`: Templates used for structuring reasoning outputs.
    *   `validators/`: Modules for validating the quality and coherence of generated reasoning.
*   `supervised_model/`: Modules dedicated to the supervised machine learning models.
    *   `generate_embeddings.py`: Handles the generation of numerical embeddings from text data.
    *   `model_pipeline.py`: Manages the training, evaluation, and saving of supervised models.

## 8. Root Level Files

*   `.gitignore`: Specifies files and directories that Git should ignore.
*   `data_processing_pipeline.log`: Log file for the main data processing pipeline.
*   `DEVELOPER_TASK_PROCESS_SUMMARY.md`: Summary of developer tasks.
*   `DEVELOPER_TASK_PROCESS.md`: Detailed developer task process.
*   `fyers_docs.txt`: Documentation related to the Fyers API.
*   `read-data.ipynb`: Jupyter notebook for initial data exploration and analysis.
*   `reasoning_processor.log`: Log file for the reasoning processor.
*   `requirements.txt`: Lists all Python dependencies required for the project.
